{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Virtual Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from vbranch.callbacks import classification_acc\n",
    "from vbranch.applications.fcn import FCN\n",
    "from vbranch.applications.cnn import CNN\n",
    "from vbranch.losses import softmax_cross_entropy_with_logits\n",
    "\n",
    "from vbranch.utils import TFSessionGrow, restore_sess\n",
    "from vbranch.utils.training import get_data, bag_samples, get_data_iterator\n",
    "from vbranch.utils.generic import get_path, save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 1\n",
    "ARCHITECTURE = 'cnn'\n",
    "DATASET = 'cifar10'\n",
    "NUM_CLASSES = 10\n",
    "NUM_FEATURES = None\n",
    "SAMPLES_PER_CLASS = None\n",
    "BAGGING_SAMPLES = 1.\n",
    "TRAIN_FRAC = 1.\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "STEPS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_data(DATASET, ARCHITECTURE, NUM_CLASSES,\n",
    "                                                NUM_FEATURES, SAMPLES_PER_CLASS, \n",
    "                                                train_frac=TRAIN_FRAC, preprocess=True)\n",
    "x_shape = (None,) + X_train.shape[1:]\n",
    "y_shape = (None, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.min(), X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path(n_branches, shared_frac):\n",
    "    return get_path(DATASET, ARCHITECTURE, 'sensitivity-2', vb=True, \n",
    "                    B=n_branches, S=shared_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_branches, shared_frac, name='model'):\n",
    "    inputs, labels, train_init_op, test_init_op = get_data_iterator(x_shape, y_shape, \n",
    "                                                                    batch_size=BATCH_SIZE, \n",
    "                                                                    n=n_branches, \n",
    "                                                                    share_xy=BAGGING_SAMPLES == 0)\n",
    "#     with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "#         if ARCHITECTURE == 'fcn':\n",
    "#             layer_spec = [(512, shared_frac), NUM_CLASSES]\n",
    "#         elif ARCHITECTURE == 'fcn2':\n",
    "#             layer_spec = [(512, shared_frac), 512, NUM_CLASSES]\n",
    "#         elif ARCHITECTURE == 'fcn3':\n",
    "#             layer_spec = [(512, shared_frac), 512, 512, NUM_CLASSES]\n",
    "#         elif ARCHITECTURE == 'fcn2A':\n",
    "#             layer_spec = [(512, shared_frac), (512, shared_frac), (NUM_CLASSES, shared_frac)]\n",
    "#         elif ARCHITECTURE == 'fcn3A':\n",
    "#             layer_spec = [(512, shared_frac), (512, shared_frac), \n",
    "#                           (512, shared_frac), (NUM_CLASSES, shared_frac)]\n",
    "#         else:\n",
    "#             raise ValueError('invalid model')\n",
    "            \n",
    "#         model = FCN(inputs, *layer_spec, name=name, shared_frac=1)\n",
    "\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "        if ARCHITECTURE == 'cnn':\n",
    "            layers = [32, 64, 128]\n",
    "        elif ARCHITECTURE == 'cnnx':\n",
    "            layers = [(32, shared_frac), (64, 1.0), (128, 1.0)]\n",
    "        else:\n",
    "            raise ValueError('invalid model')\n",
    "        model = CNN(inputs, NUM_CLASSES, *layers, name=name, shared_frac=shared_frac)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        model.compile(optimizer, softmax_cross_entropy_with_logits(), \n",
    "                      train_init_op, test_init_op, labels=labels, \n",
    "                      callbacks={'acc':classification_acc(n_branches)})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_branches, shared_frac, model_id=1):\n",
    "    dirpath = path(n_branches, shared_frac)\n",
    "    print(dirpath)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    model = build_model(n_branches, shared_frac)\n",
    "    model.summary()\n",
    "    \n",
    "    # Bagging\n",
    "    if BAGGING_SAMPLES > 0:\n",
    "        x_train_list, y_train_list = bag_samples(X_train, y_train, n_branches, \n",
    "                                                 max_samples=BAGGING_SAMPLES)\n",
    "\n",
    "    if n_branches == 1 or BAGGING_SAMPLES == 0:\n",
    "        train_dict = {'x:0': X_train, 'y:0': y_train, 'batch_size:0': BATCH_SIZE}\n",
    "    else:\n",
    "        train_dict = {'x:0': X_train, 'y:0': y_train}\n",
    "        for i in range(n_branches):\n",
    "            train_dict['vb{}_x:0'.format(i+1)] = x_train_list[i]\n",
    "            train_dict['vb{}_y:0'.format(i+1)] = y_train_list[i]\n",
    "        train_dict['batch_size:0'] = BATCH_SIZE\n",
    "\n",
    "    val_dict = {'x:0': X_test, 'y:0': y_test, 'batch_size:0': len(X_test)}\n",
    "\n",
    "    model_path = os.path.join('models', dirpath, 'model_{}'.format(model_id))\n",
    "    os.system('mkdir -p ' + model_path)\n",
    "    history = model.fit(EPOCHS, STEPS_PER_EPOCH, train_dict=train_dict,\n",
    "                        val_dict=val_dict, log_path=model_path)\n",
    "    save_results(history, dirpath, 'train_{}.csv'.format(model_id))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 01:09:49.422678 140268897367808 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/utils/training.py:134: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0803 01:09:49.476947 140268897367808 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/utils/training.py:156: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "W0803 01:09:49.482336 140268897367808 deprecation.py:323] From /home/gong/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
      "W0803 01:09:49.482869 140268897367808 deprecation.py:323] From /home/gong/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
      "W0803 01:09:49.483464 140268897367808 deprecation.py:323] From /home/gong/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
      "W0803 01:09:49.498260 140268897367808 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/vb_layers/core.py:63: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "W0803 01:09:49.498823 140268897367808 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/vb_layers/core.py:92: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0803 01:09:49.499916 140268897367808 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/layers/convolutional.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0803 01:09:49.500687 140268897367808 deprecation.py:506] From /home/gong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0803 01:09:49.604536 140268897367808 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/layers/pooling.py:22: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity-2/vb-cifar10-cnn/B2/S0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 01:09:49.894680 140268897367808 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/layers/core.py:83: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
      "\n",
      "W0803 01:09:49.990088 140268897367808 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/engine/training.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [] [None,32,32,32]  1792       input              \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_1         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [] [None,32,32,32]  0          bn_1_1             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [] [None,32,32,32]  18496      relu_1_1           \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_2         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [] [None,32,32,32]  0          bn_1_2             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [] [None,16,16,32]  0          relu_1_2           \n",
      "                                    [] [None,16,16,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [] [None,16,16,64]  36992      avg_pool2d_1       \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [] [None,16,16,64]  256        conv2d_2_1         \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [] [None,16,16,64]  0          bn_2_1             \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [] [None,16,16,64]  73856      relu_2_1           \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [] [None,16,16,64]  256        conv2d_2_2         \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [] [None,16,16,64]  0          bn_2_2             \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [] [None,8,8,64]    0          relu_2_2           \n",
      "                                    [] [None,8,8,64]                                  \n",
      "--------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [] [None,8,8,128]   147712     avg_pool2d_2       \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [] [None,8,8,128]   512        conv2d_3_1         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [] [None,8,8,128]   0          bn_3_1             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [] [None,8,8,128]   295168     relu_3_1           \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [] [None,8,8,128]   512        conv2d_3_2         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [] [None,8,8,128]   0          bn_3_2             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [] [None,128]       0          relu_3_2           \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [] [None,128]       33024      global_avg_pool2d  \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [] [None,128]       512        fc1                \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [] [None,128]       0          bn_fc1             \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]           2580       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 611924\n",
      "50000\n",
      "Epoch 1/50\n",
      "100/100 - 12s - loss_1: 1.6358 - loss_2: 1.6562 - acc_ensemble: 0.5050 - acc_1: 0.4880 - acc_2: 0.4900 - val_loss_1: 1.4387 - val_loss_2: 1.3969 - val_acc_ensemble: 0.5112 - val_acc_1: 0.4671 - val_acc_2: 0.4861\n",
      "Epoch 2/50\n",
      "100/100 - 8s - loss_1: 1.3120 - loss_2: 1.2746 - acc_ensemble: 0.5990 - acc_1: 0.5560 - acc_2: 0.5880 - val_loss_1: 1.2221 - val_loss_2: 1.1676 - val_acc_ensemble: 0.5983 - val_acc_1: 0.5555 - val_acc_2: 0.5766\n",
      "Epoch 3/50\n",
      "100/100 - 8s - loss_1: 1.1047 - loss_2: 1.1125 - acc_ensemble: 0.6630 - acc_1: 0.6520 - acc_2: 0.6130 - val_loss_1: 1.0414 - val_loss_2: 1.0668 - val_acc_ensemble: 0.6495 - val_acc_1: 0.6248 - val_acc_2: 0.6139\n",
      "Epoch 4/50\n",
      "100/100 - 8s - loss_1: 0.9920 - loss_2: 0.9860 - acc_ensemble: 0.6980 - acc_1: 0.6630 - acc_2: 0.6590 - val_loss_1: 0.9979 - val_loss_2: 0.9971 - val_acc_ensemble: 0.6794 - val_acc_1: 0.6433 - val_acc_2: 0.6410\n",
      "Epoch 5/50\n",
      "100/100 - 8s - loss_1: 0.9271 - loss_2: 0.9069 - acc_ensemble: 0.7130 - acc_1: 0.6960 - acc_2: 0.6860 - val_loss_1: 0.8928 - val_loss_2: 0.9104 - val_acc_ensemble: 0.7135 - val_acc_1: 0.6799 - val_acc_2: 0.6789\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 8s - loss_1: 0.8447 - loss_2: 0.8554 - acc_ensemble: 0.7480 - acc_1: 0.7230 - acc_2: 0.7130 - val_loss_1: 0.8366 - val_loss_2: 0.8494 - val_acc_ensemble: 0.7306 - val_acc_1: 0.7019 - val_acc_2: 0.6968\n",
      "Epoch 7/50\n",
      "100/100 - 8s - loss_1: 0.7780 - loss_2: 0.7940 - acc_ensemble: 0.7830 - acc_1: 0.7550 - acc_2: 0.7360 - val_loss_1: 0.7951 - val_loss_2: 0.7856 - val_acc_ensemble: 0.7465 - val_acc_1: 0.7180 - val_acc_2: 0.7246\n",
      "Epoch 8/50\n",
      "100/100 - 8s - loss_1: 0.7108 - loss_2: 0.7076 - acc_ensemble: 0.7840 - acc_1: 0.7640 - acc_2: 0.7530 - val_loss_1: 0.7536 - val_loss_2: 0.7676 - val_acc_ensemble: 0.7580 - val_acc_1: 0.7332 - val_acc_2: 0.7298\n",
      "Epoch 9/50\n",
      "100/100 - 8s - loss_1: 0.6815 - loss_2: 0.7061 - acc_ensemble: 0.8010 - acc_1: 0.7850 - acc_2: 0.7660 - val_loss_1: 0.7119 - val_loss_2: 0.7208 - val_acc_ensemble: 0.7775 - val_acc_1: 0.7507 - val_acc_2: 0.7456\n",
      "Epoch 10/50\n",
      "100/100 - 8s - loss_1: 0.6478 - loss_2: 0.6543 - acc_ensemble: 0.8280 - acc_1: 0.7950 - acc_2: 0.7810 - val_loss_1: 0.6910 - val_loss_2: 0.6955 - val_acc_ensemble: 0.7855 - val_acc_1: 0.7590 - val_acc_2: 0.7564\n",
      "Epoch 11/50\n",
      "100/100 - 8s - loss_1: 0.6086 - loss_2: 0.6141 - acc_ensemble: 0.8380 - acc_1: 0.8110 - acc_2: 0.8140 - val_loss_1: 0.6444 - val_loss_2: 0.6653 - val_acc_ensemble: 0.7959 - val_acc_1: 0.7750 - val_acc_2: 0.7673\n",
      "Epoch 12/50\n",
      "100/100 - 8s - loss_1: 0.5669 - loss_2: 0.5714 - acc_ensemble: 0.8430 - acc_1: 0.8220 - acc_2: 0.8130 - val_loss_1: 0.6512 - val_loss_2: 0.6670 - val_acc_ensemble: 0.7994 - val_acc_1: 0.7764 - val_acc_2: 0.7674\n",
      "Epoch 13/50\n",
      "100/100 - 8s - loss_1: 0.5458 - loss_2: 0.5624 - acc_ensemble: 0.8470 - acc_1: 0.8330 - acc_2: 0.8100 - val_loss_1: 0.6266 - val_loss_2: 0.6512 - val_acc_ensemble: 0.8074 - val_acc_1: 0.7821 - val_acc_2: 0.7735\n",
      "Epoch 14/50\n",
      "100/100 - 8s - loss_1: 0.5240 - loss_2: 0.5236 - acc_ensemble: 0.8730 - acc_1: 0.8420 - acc_2: 0.8330 - val_loss_1: 0.6047 - val_loss_2: 0.6235 - val_acc_ensemble: 0.8149 - val_acc_1: 0.7905 - val_acc_2: 0.7848\n",
      "Epoch 15/50\n",
      "100/100 - 8s - loss_1: 0.4979 - loss_2: 0.4776 - acc_ensemble: 0.8730 - acc_1: 0.8360 - acc_2: 0.8310 - val_loss_1: 0.6162 - val_loss_2: 0.6153 - val_acc_ensemble: 0.8187 - val_acc_1: 0.7863 - val_acc_2: 0.7822\n",
      "Epoch 16/50\n",
      "100/100 - 8s - loss_1: 0.4731 - loss_2: 0.4800 - acc_ensemble: 0.8730 - acc_1: 0.8490 - acc_2: 0.8400 - val_loss_1: 0.5940 - val_loss_2: 0.5883 - val_acc_ensemble: 0.8241 - val_acc_1: 0.7950 - val_acc_2: 0.7991\n",
      "Epoch 17/50\n",
      "100/100 - 8s - loss_1: 0.4370 - loss_2: 0.4695 - acc_ensemble: 0.8950 - acc_1: 0.8460 - acc_2: 0.8600 - val_loss_1: 0.6069 - val_loss_2: 0.6104 - val_acc_ensemble: 0.8171 - val_acc_1: 0.7919 - val_acc_2: 0.7881\n",
      "Epoch 18/50\n",
      "100/100 - 8s - loss_1: 0.4371 - loss_2: 0.4393 - acc_ensemble: 0.8910 - acc_1: 0.8600 - acc_2: 0.8780 - val_loss_1: 0.5838 - val_loss_2: 0.5683 - val_acc_ensemble: 0.8291 - val_acc_1: 0.7984 - val_acc_2: 0.8065\n",
      "Epoch 19/50\n",
      "100/100 - 8s - loss_1: 0.4099 - loss_2: 0.4171 - acc_ensemble: 0.9100 - acc_1: 0.8800 - acc_2: 0.8780 - val_loss_1: 0.5610 - val_loss_2: 0.5605 - val_acc_ensemble: 0.8349 - val_acc_1: 0.8094 - val_acc_2: 0.8057\n",
      "Epoch 20/50\n",
      "100/100 - 8s - loss_1: 0.3943 - loss_2: 0.4092 - acc_ensemble: 0.9080 - acc_1: 0.8800 - acc_2: 0.8730 - val_loss_1: 0.5496 - val_loss_2: 0.5683 - val_acc_ensemble: 0.8396 - val_acc_1: 0.8132 - val_acc_2: 0.8058\n",
      "Epoch 21/50\n",
      "100/100 - 8s - loss_1: 0.3818 - loss_2: 0.3978 - acc_ensemble: 0.9130 - acc_1: 0.8790 - acc_2: 0.8860 - val_loss_1: 0.5427 - val_loss_2: 0.5478 - val_acc_ensemble: 0.8423 - val_acc_1: 0.8140 - val_acc_2: 0.8148\n",
      "Epoch 22/50\n",
      "100/100 - 8s - loss_1: 0.3578 - loss_2: 0.3343 - acc_ensemble: 0.9170 - acc_1: 0.8630 - acc_2: 0.8820 - val_loss_1: 0.5627 - val_loss_2: 0.5416 - val_acc_ensemble: 0.8403 - val_acc_1: 0.8120 - val_acc_2: 0.8161\n",
      "Epoch 23/50\n",
      "100/100 - 8s - loss_1: 0.3467 - loss_2: 0.3645 - acc_ensemble: 0.9090 - acc_1: 0.8940 - acc_2: 0.8970 - val_loss_1: 0.5491 - val_loss_2: 0.5525 - val_acc_ensemble: 0.8404 - val_acc_1: 0.8144 - val_acc_2: 0.8128\n",
      "Epoch 24/50\n",
      "100/100 - 8s - loss_1: 0.3541 - loss_2: 0.3321 - acc_ensemble: 0.9160 - acc_1: 0.8730 - acc_2: 0.8950 - val_loss_1: 0.5481 - val_loss_2: 0.5506 - val_acc_ensemble: 0.8413 - val_acc_1: 0.8148 - val_acc_2: 0.8137\n",
      "Epoch 25/50\n",
      "100/100 - 8s - loss_1: 0.3421 - loss_2: 0.3167 - acc_ensemble: 0.9210 - acc_1: 0.8960 - acc_2: 0.8900 - val_loss_1: 0.5635 - val_loss_2: 0.5612 - val_acc_ensemble: 0.8437 - val_acc_1: 0.8128 - val_acc_2: 0.8201\n",
      "Epoch 26/50\n",
      "100/100 - 8s - loss_1: 0.3070 - loss_2: 0.3191 - acc_ensemble: 0.9290 - acc_1: 0.8900 - acc_2: 0.9040 - val_loss_1: 0.5289 - val_loss_2: 0.5413 - val_acc_ensemble: 0.8517 - val_acc_1: 0.8203 - val_acc_2: 0.8183\n",
      "Epoch 27/50\n",
      "100/100 - 8s - loss_1: 0.2961 - loss_2: 0.3104 - acc_ensemble: 0.9340 - acc_1: 0.9010 - acc_2: 0.9010 - val_loss_1: 0.5363 - val_loss_2: 0.5295 - val_acc_ensemble: 0.8517 - val_acc_1: 0.8186 - val_acc_2: 0.8226\n",
      "Epoch 28/50\n",
      "100/100 - 8s - loss_1: 0.2908 - loss_2: 0.3082 - acc_ensemble: 0.9460 - acc_1: 0.9150 - acc_2: 0.9160 - val_loss_1: 0.5294 - val_loss_2: 0.5285 - val_acc_ensemble: 0.8496 - val_acc_1: 0.8250 - val_acc_2: 0.8267\n",
      "Epoch 29/50\n",
      "100/100 - 8s - loss_1: 0.2867 - loss_2: 0.2565 - acc_ensemble: 0.9420 - acc_1: 0.9120 - acc_2: 0.9090 - val_loss_1: 0.5251 - val_loss_2: 0.5260 - val_acc_ensemble: 0.8542 - val_acc_1: 0.8259 - val_acc_2: 0.8296\n",
      "Epoch 30/50\n",
      "100/100 - 8s - loss_1: 0.2749 - loss_2: 0.2731 - acc_ensemble: 0.9500 - acc_1: 0.9130 - acc_2: 0.9310 - val_loss_1: 0.5188 - val_loss_2: 0.5216 - val_acc_ensemble: 0.8547 - val_acc_1: 0.8287 - val_acc_2: 0.8304\n",
      "Epoch 31/50\n",
      "100/100 - 8s - loss_1: 0.2627 - loss_2: 0.2479 - acc_ensemble: 0.9510 - acc_1: 0.9130 - acc_2: 0.9240 - val_loss_1: 0.5269 - val_loss_2: 0.5343 - val_acc_ensemble: 0.8548 - val_acc_1: 0.8293 - val_acc_2: 0.8323\n",
      "Epoch 32/50\n",
      "100/100 - 8s - loss_1: 0.2409 - loss_2: 0.2636 - acc_ensemble: 0.9560 - acc_1: 0.9310 - acc_2: 0.9150 - val_loss_1: 0.5252 - val_loss_2: 0.5410 - val_acc_ensemble: 0.8537 - val_acc_1: 0.8317 - val_acc_2: 0.8247\n",
      "Epoch 33/50\n",
      "100/100 - 8s - loss_1: 0.2365 - loss_2: 0.2163 - acc_ensemble: 0.9470 - acc_1: 0.9140 - acc_2: 0.9320 - val_loss_1: 0.5226 - val_loss_2: 0.5349 - val_acc_ensemble: 0.8573 - val_acc_1: 0.8316 - val_acc_2: 0.8298\n",
      "Epoch 34/50\n",
      "100/100 - 8s - loss_1: 0.2283 - loss_2: 0.2127 - acc_ensemble: 0.9450 - acc_1: 0.9270 - acc_2: 0.9310 - val_loss_1: 0.5395 - val_loss_2: 0.5227 - val_acc_ensemble: 0.8581 - val_acc_1: 0.8251 - val_acc_2: 0.8312\n",
      "Epoch 35/50\n",
      "100/100 - 8s - loss_1: 0.2074 - loss_2: 0.2456 - acc_ensemble: 0.9540 - acc_1: 0.9230 - acc_2: 0.9380 - val_loss_1: 0.5356 - val_loss_2: 0.5238 - val_acc_ensemble: 0.8600 - val_acc_1: 0.8292 - val_acc_2: 0.8349\n",
      "Epoch 36/50\n",
      "100/100 - 8s - loss_1: 0.2048 - loss_2: 0.1973 - acc_ensemble: 0.9590 - acc_1: 0.9380 - acc_2: 0.9360 - val_loss_1: 0.5387 - val_loss_2: 0.5378 - val_acc_ensemble: 0.8570 - val_acc_1: 0.8285 - val_acc_2: 0.8321\n",
      "Epoch 37/50\n",
      "100/100 - 8s - loss_1: 0.1911 - loss_2: 0.2141 - acc_ensemble: 0.9590 - acc_1: 0.9410 - acc_2: 0.9300 - val_loss_1: 0.5449 - val_loss_2: 0.5507 - val_acc_ensemble: 0.8591 - val_acc_1: 0.8300 - val_acc_2: 0.8339\n",
      "Epoch 38/50\n",
      "100/100 - 8s - loss_1: 0.1967 - loss_2: 0.1969 - acc_ensemble: 0.9610 - acc_1: 0.9360 - acc_2: 0.9290 - val_loss_1: 0.5466 - val_loss_2: 0.5530 - val_acc_ensemble: 0.8570 - val_acc_1: 0.8315 - val_acc_2: 0.8250\n",
      "Epoch 39/50\n",
      "100/100 - 8s - loss_1: 0.1902 - loss_2: 0.1930 - acc_ensemble: 0.9710 - acc_1: 0.9350 - acc_2: 0.9340 - val_loss_1: 0.5480 - val_loss_2: 0.5363 - val_acc_ensemble: 0.8612 - val_acc_1: 0.8295 - val_acc_2: 0.8350\n",
      "Epoch 40/50\n",
      "100/100 - 8s - loss_1: 0.1599 - loss_2: 0.1779 - acc_ensemble: 0.9710 - acc_1: 0.9340 - acc_2: 0.9470 - val_loss_1: 0.5643 - val_loss_2: 0.5541 - val_acc_ensemble: 0.8563 - val_acc_1: 0.8261 - val_acc_2: 0.8307\n",
      "Epoch 41/50\n",
      "100/100 - 8s - loss_1: 0.1668 - loss_2: 0.1739 - acc_ensemble: 0.9700 - acc_1: 0.9430 - acc_2: 0.9430 - val_loss_1: 0.5750 - val_loss_2: 0.5457 - val_acc_ensemble: 0.8594 - val_acc_1: 0.8247 - val_acc_2: 0.8377\n",
      "Epoch 42/50\n",
      "100/100 - 8s - loss_1: 0.1492 - loss_2: 0.1695 - acc_ensemble: 0.9740 - acc_1: 0.9430 - acc_2: 0.9480 - val_loss_1: 0.5594 - val_loss_2: 0.5498 - val_acc_ensemble: 0.8600 - val_acc_1: 0.8350 - val_acc_2: 0.8328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "100/100 - 8s - loss_1: 0.1710 - loss_2: 0.1452 - acc_ensemble: 0.9740 - acc_1: 0.9390 - acc_2: 0.9390 - val_loss_1: 0.5881 - val_loss_2: 0.5728 - val_acc_ensemble: 0.8601 - val_acc_1: 0.8220 - val_acc_2: 0.8294\n",
      "Epoch 44/50\n",
      "100/100 - 8s - loss_1: 0.1453 - loss_2: 0.1708 - acc_ensemble: 0.9710 - acc_1: 0.9430 - acc_2: 0.9430 - val_loss_1: 0.5652 - val_loss_2: 0.5486 - val_acc_ensemble: 0.8602 - val_acc_1: 0.8334 - val_acc_2: 0.8338\n",
      "Epoch 45/50\n",
      "100/100 - 8s - loss_1: 0.1224 - loss_2: 0.1520 - acc_ensemble: 0.9700 - acc_1: 0.9440 - acc_2: 0.9480 - val_loss_1: 0.5766 - val_loss_2: 0.5610 - val_acc_ensemble: 0.8642 - val_acc_1: 0.8368 - val_acc_2: 0.8348\n",
      "Epoch 46/50\n",
      "100/100 - 8s - loss_1: 0.1517 - loss_2: 0.1252 - acc_ensemble: 0.9760 - acc_1: 0.9570 - acc_2: 0.9600 - val_loss_1: 0.5693 - val_loss_2: 0.5533 - val_acc_ensemble: 0.8622 - val_acc_1: 0.8319 - val_acc_2: 0.8416\n",
      "Epoch 47/50\n",
      "100/100 - 8s - loss_1: 0.1405 - loss_2: 0.1246 - acc_ensemble: 0.9780 - acc_1: 0.9460 - acc_2: 0.9630 - val_loss_1: 0.5740 - val_loss_2: 0.5693 - val_acc_ensemble: 0.8662 - val_acc_1: 0.8343 - val_acc_2: 0.8391\n",
      "Epoch 48/50\n",
      "100/100 - 8s - loss_1: 0.1363 - loss_2: 0.1383 - acc_ensemble: 0.9840 - acc_1: 0.9600 - acc_2: 0.9570 - val_loss_1: 0.5972 - val_loss_2: 0.5719 - val_acc_ensemble: 0.8636 - val_acc_1: 0.8314 - val_acc_2: 0.8376\n",
      "Epoch 49/50\n",
      "100/100 - 8s - loss_1: 0.1375 - loss_2: 0.1529 - acc_ensemble: 0.9800 - acc_1: 0.9480 - acc_2: 0.9580 - val_loss_1: 0.5866 - val_loss_2: 0.5707 - val_acc_ensemble: 0.8663 - val_acc_1: 0.8312 - val_acc_2: 0.8389\n",
      "Epoch 50/50\n",
      "100/100 - 8s - loss_1: 0.1283 - loss_2: 0.1333 - acc_ensemble: 0.9850 - acc_1: 0.9650 - acc_2: 0.9610 - val_loss_1: 0.5740 - val_loss_2: 0.5809 - val_acc_ensemble: 0.8676 - val_acc_1: 0.8378 - val_acc_2: 0.8359\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.00\n",
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [] [None,32,32,32]  1792       input              \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_1         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [] [None,32,32,32]  0          bn_1_1             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [] [None,32,32,32]  18496      relu_1_1           \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_2         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [] [None,32,32,32]  0          bn_1_2             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [] [None,16,16,32]  0          relu_1_2           \n",
      "                                    [] [None,16,16,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [] [None,16,16,64]  36992      avg_pool2d_1       \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [] [None,16,16,64]  256        conv2d_2_1         \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [] [None,16,16,64]  0          bn_2_1             \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [] [None,16,16,64]  73856      relu_2_1           \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [] [None,16,16,64]  256        conv2d_2_2         \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [] [None,16,16,64]  0          bn_2_2             \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [] [None,8,8,64]    0          relu_2_2           \n",
      "                                    [] [None,8,8,64]                                  \n",
      "--------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [] [None,8,8,128]   147712     avg_pool2d_2       \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [] [None,8,8,128]   512        conv2d_3_1         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [] [None,8,8,128]   0          bn_3_1             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [] [None,8,8,128]   295168     relu_3_1           \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [] [None,8,8,128]   512        conv2d_3_2         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [] [None,8,8,128]   0          bn_3_2             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [] [None,128]       0          relu_3_2           \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [] [None,128]       33024      global_avg_pool2d  \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [] [None,128]       512        fc1                \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [] [None,128]       0          bn_fc1             \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]           2580       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 611924\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 9s - loss_1: 1.6568 - loss_2: 1.6231 - acc_ensemble: 0.5420 - acc_1: 0.4850 - acc_2: 0.5070 - val_loss_1: 1.4354 - val_loss_2: 1.3576 - val_acc_ensemble: 0.5152 - val_acc_1: 0.4688 - val_acc_2: 0.5047\n",
      "Epoch 2/50\n",
      "100/100 - 8s - loss_1: 1.3373 - loss_2: 1.2693 - acc_ensemble: 0.6270 - acc_1: 0.5860 - acc_2: 0.6120 - val_loss_1: 1.1800 - val_loss_2: 1.1276 - val_acc_ensemble: 0.6053 - val_acc_1: 0.5771 - val_acc_2: 0.5876\n",
      "Epoch 3/50\n",
      "100/100 - 8s - loss_1: 1.1344 - loss_2: 1.1075 - acc_ensemble: 0.6740 - acc_1: 0.6440 - acc_2: 0.6300 - val_loss_1: 1.0538 - val_loss_2: 1.0380 - val_acc_ensemble: 0.6578 - val_acc_1: 0.6204 - val_acc_2: 0.6325\n",
      "Epoch 4/50\n",
      "100/100 - 8s - loss_1: 1.0063 - loss_2: 0.9696 - acc_ensemble: 0.7040 - acc_1: 0.6780 - acc_2: 0.6560 - val_loss_1: 0.9968 - val_loss_2: 0.9500 - val_acc_ensemble: 0.6796 - val_acc_1: 0.6424 - val_acc_2: 0.6616\n",
      "Epoch 5/50\n",
      "100/100 - 8s - loss_1: 0.9194 - loss_2: 0.9051 - acc_ensemble: 0.7270 - acc_1: 0.6850 - acc_2: 0.6880 - val_loss_1: 0.8798 - val_loss_2: 0.8903 - val_acc_ensemble: 0.7141 - val_acc_1: 0.6900 - val_acc_2: 0.6860\n",
      "Epoch 6/50\n",
      "100/100 - 8s - loss_1: 0.8551 - loss_2: 0.8266 - acc_ensemble: 0.7660 - acc_1: 0.7310 - acc_2: 0.7280 - val_loss_1: 0.8393 - val_loss_2: 0.8297 - val_acc_ensemble: 0.7308 - val_acc_1: 0.6925 - val_acc_2: 0.7065\n",
      "Epoch 7/50\n",
      "100/100 - 8s - loss_1: 0.7581 - loss_2: 0.7602 - acc_ensemble: 0.7650 - acc_1: 0.7410 - acc_2: 0.7310 - val_loss_1: 0.7986 - val_loss_2: 0.7992 - val_acc_ensemble: 0.7468 - val_acc_1: 0.7142 - val_acc_2: 0.7207\n",
      "Epoch 8/50\n",
      "100/100 - 8s - loss_1: 0.7469 - loss_2: 0.7159 - acc_ensemble: 0.7930 - acc_1: 0.7630 - acc_2: 0.7650 - val_loss_1: 0.7697 - val_loss_2: 0.7329 - val_acc_ensemble: 0.7657 - val_acc_1: 0.7276 - val_acc_2: 0.7409\n",
      "Epoch 9/50\n",
      "100/100 - 8s - loss_1: 0.7081 - loss_2: 0.6544 - acc_ensemble: 0.8100 - acc_1: 0.7680 - acc_2: 0.7900 - val_loss_1: 0.7447 - val_loss_2: 0.7117 - val_acc_ensemble: 0.7687 - val_acc_1: 0.7338 - val_acc_2: 0.7459\n",
      "Epoch 10/50\n",
      "100/100 - 8s - loss_1: 0.6377 - loss_2: 0.6067 - acc_ensemble: 0.8270 - acc_1: 0.7910 - acc_2: 0.7830 - val_loss_1: 0.6801 - val_loss_2: 0.6947 - val_acc_ensemble: 0.7891 - val_acc_1: 0.7640 - val_acc_2: 0.7583\n",
      "Epoch 11/50\n",
      "100/100 - 8s - loss_1: 0.6244 - loss_2: 0.5932 - acc_ensemble: 0.8380 - acc_1: 0.8190 - acc_2: 0.8230 - val_loss_1: 0.6467 - val_loss_2: 0.6650 - val_acc_ensemble: 0.7976 - val_acc_1: 0.7752 - val_acc_2: 0.7692\n",
      "Epoch 12/50\n",
      "100/100 - 8s - loss_1: 0.5761 - loss_2: 0.5565 - acc_ensemble: 0.8560 - acc_1: 0.8200 - acc_2: 0.8290 - val_loss_1: 0.6527 - val_loss_2: 0.6495 - val_acc_ensemble: 0.8024 - val_acc_1: 0.7735 - val_acc_2: 0.7752\n",
      "Epoch 13/50\n",
      "100/100 - 8s - loss_1: 0.5367 - loss_2: 0.5304 - acc_ensemble: 0.8440 - acc_1: 0.8160 - acc_2: 0.8080 - val_loss_1: 0.6425 - val_loss_2: 0.6421 - val_acc_ensemble: 0.8074 - val_acc_1: 0.7823 - val_acc_2: 0.7804\n",
      "Epoch 14/50\n",
      "100/100 - 8s - loss_1: 0.5289 - loss_2: 0.5135 - acc_ensemble: 0.8710 - acc_1: 0.8370 - acc_2: 0.8400 - val_loss_1: 0.6118 - val_loss_2: 0.6049 - val_acc_ensemble: 0.8180 - val_acc_1: 0.7897 - val_acc_2: 0.7941\n",
      "Epoch 15/50\n",
      "100/100 - 8s - loss_1: 0.4965 - loss_2: 0.4737 - acc_ensemble: 0.8540 - acc_1: 0.8270 - acc_2: 0.8420 - val_loss_1: 0.6116 - val_loss_2: 0.5907 - val_acc_ensemble: 0.8198 - val_acc_1: 0.7899 - val_acc_2: 0.7928\n",
      "Epoch 16/50\n",
      "100/100 - 8s - loss_1: 0.4634 - loss_2: 0.4845 - acc_ensemble: 0.8860 - acc_1: 0.8550 - acc_2: 0.8520 - val_loss_1: 0.5834 - val_loss_2: 0.5813 - val_acc_ensemble: 0.8260 - val_acc_1: 0.8009 - val_acc_2: 0.7986\n",
      "Epoch 17/50\n",
      "100/100 - 8s - loss_1: 0.4518 - loss_2: 0.4385 - acc_ensemble: 0.9000 - acc_1: 0.8530 - acc_2: 0.8670 - val_loss_1: 0.5654 - val_loss_2: 0.5843 - val_acc_ensemble: 0.8331 - val_acc_1: 0.8067 - val_acc_2: 0.7971\n",
      "Epoch 18/50\n",
      "100/100 - 8s - loss_1: 0.4565 - loss_2: 0.4222 - acc_ensemble: 0.8930 - acc_1: 0.8600 - acc_2: 0.8520 - val_loss_1: 0.5791 - val_loss_2: 0.5788 - val_acc_ensemble: 0.8331 - val_acc_1: 0.8045 - val_acc_2: 0.8027\n",
      "Epoch 19/50\n",
      "100/100 - 8s - loss_1: 0.4158 - loss_2: 0.3826 - acc_ensemble: 0.8920 - acc_1: 0.8620 - acc_2: 0.8620 - val_loss_1: 0.5607 - val_loss_2: 0.5696 - val_acc_ensemble: 0.8356 - val_acc_1: 0.8093 - val_acc_2: 0.8038\n",
      "Epoch 20/50\n",
      "100/100 - 8s - loss_1: 0.4066 - loss_2: 0.3598 - acc_ensemble: 0.9090 - acc_1: 0.8680 - acc_2: 0.8810 - val_loss_1: 0.5731 - val_loss_2: 0.5633 - val_acc_ensemble: 0.8355 - val_acc_1: 0.8096 - val_acc_2: 0.8049\n",
      "Epoch 21/50\n",
      "100/100 - 8s - loss_1: 0.3662 - loss_2: 0.3787 - acc_ensemble: 0.9060 - acc_1: 0.8710 - acc_2: 0.8800 - val_loss_1: 0.5732 - val_loss_2: 0.5435 - val_acc_ensemble: 0.8430 - val_acc_1: 0.8088 - val_acc_2: 0.8164\n",
      "Epoch 22/50\n",
      "100/100 - 8s - loss_1: 0.3735 - loss_2: 0.3536 - acc_ensemble: 0.9060 - acc_1: 0.8850 - acc_2: 0.8740 - val_loss_1: 0.5447 - val_loss_2: 0.5748 - val_acc_ensemble: 0.8384 - val_acc_1: 0.8171 - val_acc_2: 0.8064\n",
      "Epoch 23/50\n",
      "100/100 - 8s - loss_1: 0.3442 - loss_2: 0.3210 - acc_ensemble: 0.9250 - acc_1: 0.8920 - acc_2: 0.8990 - val_loss_1: 0.5447 - val_loss_2: 0.5520 - val_acc_ensemble: 0.8450 - val_acc_1: 0.8177 - val_acc_2: 0.8141\n",
      "Epoch 24/50\n",
      "100/100 - 8s - loss_1: 0.3392 - loss_2: 0.3112 - acc_ensemble: 0.9240 - acc_1: 0.8830 - acc_2: 0.9130 - val_loss_1: 0.5461 - val_loss_2: 0.5278 - val_acc_ensemble: 0.8483 - val_acc_1: 0.8159 - val_acc_2: 0.8219\n",
      "Epoch 25/50\n",
      "100/100 - 8s - loss_1: 0.3256 - loss_2: 0.3045 - acc_ensemble: 0.9270 - acc_1: 0.8870 - acc_2: 0.8990 - val_loss_1: 0.5601 - val_loss_2: 0.5651 - val_acc_ensemble: 0.8452 - val_acc_1: 0.8169 - val_acc_2: 0.8117\n",
      "Epoch 26/50\n",
      "100/100 - 8s - loss_1: 0.3093 - loss_2: 0.2985 - acc_ensemble: 0.9350 - acc_1: 0.9050 - acc_2: 0.8910 - val_loss_1: 0.5419 - val_loss_2: 0.5579 - val_acc_ensemble: 0.8446 - val_acc_1: 0.8177 - val_acc_2: 0.8155\n",
      "Epoch 27/50\n",
      "100/100 - 8s - loss_1: 0.3088 - loss_2: 0.2768 - acc_ensemble: 0.9420 - acc_1: 0.8980 - acc_2: 0.9140 - val_loss_1: 0.5569 - val_loss_2: 0.5397 - val_acc_ensemble: 0.8494 - val_acc_1: 0.8162 - val_acc_2: 0.8236\n",
      "Epoch 28/50\n",
      "100/100 - 8s - loss_1: 0.2775 - loss_2: 0.2842 - acc_ensemble: 0.9270 - acc_1: 0.9100 - acc_2: 0.8950 - val_loss_1: 0.5384 - val_loss_2: 0.5428 - val_acc_ensemble: 0.8492 - val_acc_1: 0.8206 - val_acc_2: 0.8212\n",
      "Epoch 29/50\n",
      "100/100 - 8s - loss_1: 0.2650 - loss_2: 0.2587 - acc_ensemble: 0.9350 - acc_1: 0.9040 - acc_2: 0.9200 - val_loss_1: 0.5602 - val_loss_2: 0.5576 - val_acc_ensemble: 0.8476 - val_acc_1: 0.8201 - val_acc_2: 0.8190\n",
      "Epoch 30/50\n",
      "100/100 - 8s - loss_1: 0.2836 - loss_2: 0.2782 - acc_ensemble: 0.9510 - acc_1: 0.9220 - acc_2: 0.9280 - val_loss_1: 0.5351 - val_loss_2: 0.5259 - val_acc_ensemble: 0.8523 - val_acc_1: 0.8239 - val_acc_2: 0.8282\n",
      "Epoch 31/50\n",
      "100/100 - 8s - loss_1: 0.2524 - loss_2: 0.2653 - acc_ensemble: 0.9590 - acc_1: 0.9050 - acc_2: 0.9180 - val_loss_1: 0.5673 - val_loss_2: 0.5551 - val_acc_ensemble: 0.8556 - val_acc_1: 0.8222 - val_acc_2: 0.8187\n",
      "Epoch 32/50\n",
      "100/100 - 8s - loss_1: 0.2598 - loss_2: 0.2405 - acc_ensemble: 0.9590 - acc_1: 0.9130 - acc_2: 0.9290 - val_loss_1: 0.5494 - val_loss_2: 0.5376 - val_acc_ensemble: 0.8589 - val_acc_1: 0.8245 - val_acc_2: 0.8304\n",
      "Epoch 33/50\n",
      "100/100 - 8s - loss_1: 0.2370 - loss_2: 0.2341 - acc_ensemble: 0.9570 - acc_1: 0.9410 - acc_2: 0.9320 - val_loss_1: 0.5143 - val_loss_2: 0.5377 - val_acc_ensemble: 0.8529 - val_acc_1: 0.8313 - val_acc_2: 0.8274\n",
      "Epoch 34/50\n",
      "100/100 - 8s - loss_1: 0.2256 - loss_2: 0.2401 - acc_ensemble: 0.9560 - acc_1: 0.9260 - acc_2: 0.9270 - val_loss_1: 0.5207 - val_loss_2: 0.5455 - val_acc_ensemble: 0.8603 - val_acc_1: 0.8316 - val_acc_2: 0.8265\n",
      "Epoch 35/50\n",
      "100/100 - 8s - loss_1: 0.2177 - loss_2: 0.2326 - acc_ensemble: 0.9610 - acc_1: 0.9340 - acc_2: 0.9440 - val_loss_1: 0.5614 - val_loss_2: 0.5343 - val_acc_ensemble: 0.8570 - val_acc_1: 0.8241 - val_acc_2: 0.8312\n",
      "Epoch 36/50\n",
      "100/100 - 8s - loss_1: 0.2029 - loss_2: 0.2013 - acc_ensemble: 0.9700 - acc_1: 0.9360 - acc_2: 0.9380 - val_loss_1: 0.5304 - val_loss_2: 0.5312 - val_acc_ensemble: 0.8627 - val_acc_1: 0.8344 - val_acc_2: 0.8312\n",
      "Epoch 37/50\n",
      "100/100 - 8s - loss_1: 0.1824 - loss_2: 0.1859 - acc_ensemble: 0.9620 - acc_1: 0.9290 - acc_2: 0.9420 - val_loss_1: 0.5592 - val_loss_2: 0.5316 - val_acc_ensemble: 0.8631 - val_acc_1: 0.8277 - val_acc_2: 0.8335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 8s - loss_1: 0.2310 - loss_2: 0.1962 - acc_ensemble: 0.9580 - acc_1: 0.9460 - acc_2: 0.9420 - val_loss_1: 0.5382 - val_loss_2: 0.5332 - val_acc_ensemble: 0.8586 - val_acc_1: 0.8355 - val_acc_2: 0.8352\n",
      "Epoch 39/50\n",
      "100/100 - 8s - loss_1: 0.1718 - loss_2: 0.1710 - acc_ensemble: 0.9730 - acc_1: 0.9530 - acc_2: 0.9420 - val_loss_1: 0.5584 - val_loss_2: 0.5347 - val_acc_ensemble: 0.8624 - val_acc_1: 0.8325 - val_acc_2: 0.8355\n",
      "Epoch 40/50\n",
      "100/100 - 8s - loss_1: 0.1657 - loss_2: 0.1531 - acc_ensemble: 0.9750 - acc_1: 0.9510 - acc_2: 0.9470 - val_loss_1: 0.5402 - val_loss_2: 0.5446 - val_acc_ensemble: 0.8620 - val_acc_1: 0.8367 - val_acc_2: 0.8362\n",
      "Epoch 41/50\n",
      "100/100 - 8s - loss_1: 0.1405 - loss_2: 0.1555 - acc_ensemble: 0.9710 - acc_1: 0.9490 - acc_2: 0.9410 - val_loss_1: 0.5605 - val_loss_2: 0.5559 - val_acc_ensemble: 0.8616 - val_acc_1: 0.8345 - val_acc_2: 0.8309\n",
      "Epoch 42/50\n",
      "100/100 - 8s - loss_1: 0.1653 - loss_2: 0.1880 - acc_ensemble: 0.9710 - acc_1: 0.9470 - acc_2: 0.9500 - val_loss_1: 0.5439 - val_loss_2: 0.5541 - val_acc_ensemble: 0.8629 - val_acc_1: 0.8384 - val_acc_2: 0.8319\n",
      "Epoch 43/50\n",
      "100/100 - 8s - loss_1: 0.1339 - loss_2: 0.1746 - acc_ensemble: 0.9770 - acc_1: 0.9470 - acc_2: 0.9600 - val_loss_1: 0.5751 - val_loss_2: 0.5278 - val_acc_ensemble: 0.8625 - val_acc_1: 0.8313 - val_acc_2: 0.8375\n",
      "Epoch 44/50\n",
      "100/100 - 8s - loss_1: 0.1447 - loss_2: 0.1619 - acc_ensemble: 0.9700 - acc_1: 0.9490 - acc_2: 0.9490 - val_loss_1: 0.5940 - val_loss_2: 0.5549 - val_acc_ensemble: 0.8593 - val_acc_1: 0.8291 - val_acc_2: 0.8313\n",
      "Epoch 45/50\n",
      "100/100 - 8s - loss_1: 0.1535 - loss_2: 0.1448 - acc_ensemble: 0.9760 - acc_1: 0.9410 - acc_2: 0.9580 - val_loss_1: 0.5912 - val_loss_2: 0.5608 - val_acc_ensemble: 0.8651 - val_acc_1: 0.8292 - val_acc_2: 0.8353\n",
      "Epoch 46/50\n",
      "100/100 - 8s - loss_1: 0.1559 - loss_2: 0.1126 - acc_ensemble: 0.9810 - acc_1: 0.9630 - acc_2: 0.9570 - val_loss_1: 0.5673 - val_loss_2: 0.5629 - val_acc_ensemble: 0.8662 - val_acc_1: 0.8363 - val_acc_2: 0.8369\n",
      "Epoch 47/50\n",
      "100/100 - 8s - loss_1: 0.1315 - loss_2: 0.1174 - acc_ensemble: 0.9810 - acc_1: 0.9590 - acc_2: 0.9560 - val_loss_1: 0.5629 - val_loss_2: 0.5679 - val_acc_ensemble: 0.8714 - val_acc_1: 0.8375 - val_acc_2: 0.8385\n",
      "Epoch 48/50\n",
      "100/100 - 8s - loss_1: 0.1104 - loss_2: 0.1007 - acc_ensemble: 0.9840 - acc_1: 0.9590 - acc_2: 0.9540 - val_loss_1: 0.5658 - val_loss_2: 0.5777 - val_acc_ensemble: 0.8643 - val_acc_1: 0.8371 - val_acc_2: 0.8360\n",
      "Epoch 49/50\n",
      "100/100 - 8s - loss_1: 0.1144 - loss_2: 0.1131 - acc_ensemble: 0.9880 - acc_1: 0.9680 - acc_2: 0.9640 - val_loss_1: 0.5916 - val_loss_2: 0.5799 - val_acc_ensemble: 0.8663 - val_acc_1: 0.8355 - val_acc_2: 0.8387\n",
      "Epoch 50/50\n",
      "100/100 - 8s - loss_1: 0.1156 - loss_2: 0.1188 - acc_ensemble: 0.9790 - acc_1: 0.9480 - acc_2: 0.9540 - val_loss_1: 0.6080 - val_loss_2: 0.6003 - val_acc_ensemble: 0.8659 - val_acc_1: 0.8356 - val_acc_2: 0.8332\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.00\n",
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [] [None,32,32,32]  1792       input              \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_1         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [] [None,32,32,32]  0          bn_1_1             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [] [None,32,32,32]  18496      relu_1_1           \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_2         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [] [None,32,32,32]  0          bn_1_2             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [] [None,16,16,32]  0          relu_1_2           \n",
      "                                    [] [None,16,16,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [] [None,16,16,64]  36992      avg_pool2d_1       \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [] [None,16,16,64]  256        conv2d_2_1         \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [] [None,16,16,64]  0          bn_2_1             \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [] [None,16,16,64]  73856      relu_2_1           \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [] [None,16,16,64]  256        conv2d_2_2         \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [] [None,16,16,64]  0          bn_2_2             \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [] [None,8,8,64]    0          relu_2_2           \n",
      "                                    [] [None,8,8,64]                                  \n",
      "--------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [] [None,8,8,128]   147712     avg_pool2d_2       \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [] [None,8,8,128]   512        conv2d_3_1         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [] [None,8,8,128]   0          bn_3_1             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [] [None,8,8,128]   295168     relu_3_1           \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [] [None,8,8,128]   512        conv2d_3_2         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [] [None,8,8,128]   0          bn_3_2             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [] [None,128]       0          relu_3_2           \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [] [None,128]       33024      global_avg_pool2d  \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [] [None,128]       512        fc1                \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [] [None,128]       0          bn_fc1             \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]           2580       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 611924\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 9s - loss_1: 1.6720 - loss_2: 1.6556 - acc_ensemble: 0.5250 - acc_1: 0.4680 - acc_2: 0.4940 - val_loss_1: 1.4415 - val_loss_2: 1.3944 - val_acc_ensemble: 0.5156 - val_acc_1: 0.4611 - val_acc_2: 0.4960\n",
      "Epoch 2/50\n",
      "100/100 - 8s - loss_1: 1.3324 - loss_2: 1.2990 - acc_ensemble: 0.5780 - acc_1: 0.5440 - acc_2: 0.5700 - val_loss_1: 1.2200 - val_loss_2: 1.2052 - val_acc_ensemble: 0.5829 - val_acc_1: 0.5549 - val_acc_2: 0.5615\n",
      "Epoch 3/50\n",
      "100/100 - 8s - loss_1: 1.1267 - loss_2: 1.1358 - acc_ensemble: 0.6560 - acc_1: 0.6200 - acc_2: 0.6320 - val_loss_1: 1.0825 - val_loss_2: 1.0693 - val_acc_ensemble: 0.6470 - val_acc_1: 0.6065 - val_acc_2: 0.6117\n",
      "Epoch 4/50\n",
      "100/100 - 8s - loss_1: 1.0362 - loss_2: 0.9966 - acc_ensemble: 0.6910 - acc_1: 0.6620 - acc_2: 0.6430 - val_loss_1: 0.9932 - val_loss_2: 0.9995 - val_acc_ensemble: 0.6774 - val_acc_1: 0.6398 - val_acc_2: 0.6402\n",
      "Epoch 5/50\n",
      "100/100 - 8s - loss_1: 0.9216 - loss_2: 0.9160 - acc_ensemble: 0.7270 - acc_1: 0.6800 - acc_2: 0.6940 - val_loss_1: 0.9250 - val_loss_2: 0.8928 - val_acc_ensemble: 0.7054 - val_acc_1: 0.6691 - val_acc_2: 0.6821\n",
      "Epoch 6/50\n",
      "100/100 - 8s - loss_1: 0.8607 - loss_2: 0.8308 - acc_ensemble: 0.7490 - acc_1: 0.7240 - acc_2: 0.7140 - val_loss_1: 0.8546 - val_loss_2: 0.8738 - val_acc_ensemble: 0.7276 - val_acc_1: 0.6949 - val_acc_2: 0.6888\n",
      "Epoch 7/50\n",
      "100/100 - 8s - loss_1: 0.7929 - loss_2: 0.7739 - acc_ensemble: 0.7810 - acc_1: 0.7410 - acc_2: 0.7380 - val_loss_1: 0.8130 - val_loss_2: 0.7792 - val_acc_ensemble: 0.7441 - val_acc_1: 0.7119 - val_acc_2: 0.7265\n",
      "Epoch 8/50\n",
      "100/100 - 8s - loss_1: 0.7517 - loss_2: 0.7321 - acc_ensemble: 0.7980 - acc_1: 0.7520 - acc_2: 0.7770 - val_loss_1: 0.7687 - val_loss_2: 0.7451 - val_acc_ensemble: 0.7642 - val_acc_1: 0.7316 - val_acc_2: 0.7462\n",
      "Epoch 9/50\n",
      "100/100 - 8s - loss_1: 0.6900 - loss_2: 0.6611 - acc_ensemble: 0.8180 - acc_1: 0.7770 - acc_2: 0.7850 - val_loss_1: 0.7403 - val_loss_2: 0.7086 - val_acc_ensemble: 0.7744 - val_acc_1: 0.7371 - val_acc_2: 0.7550\n",
      "Epoch 10/50\n",
      "100/100 - 8s - loss_1: 0.6451 - loss_2: 0.6435 - acc_ensemble: 0.8320 - acc_1: 0.7790 - acc_2: 0.8090 - val_loss_1: 0.7169 - val_loss_2: 0.6700 - val_acc_ensemble: 0.7852 - val_acc_1: 0.7459 - val_acc_2: 0.7652\n",
      "Epoch 11/50\n",
      "100/100 - 8s - loss_1: 0.6343 - loss_2: 0.6096 - acc_ensemble: 0.8460 - acc_1: 0.8170 - acc_2: 0.7960 - val_loss_1: 0.6738 - val_loss_2: 0.6701 - val_acc_ensemble: 0.7965 - val_acc_1: 0.7644 - val_acc_2: 0.7708\n",
      "Epoch 12/50\n",
      "100/100 - 8s - loss_1: 0.6129 - loss_2: 0.5621 - acc_ensemble: 0.8340 - acc_1: 0.7980 - acc_2: 0.8100 - val_loss_1: 0.6594 - val_loss_2: 0.6385 - val_acc_ensemble: 0.8052 - val_acc_1: 0.7706 - val_acc_2: 0.7797\n",
      "Epoch 13/50\n",
      "100/100 - 8s - loss_1: 0.5708 - loss_2: 0.5495 - acc_ensemble: 0.8620 - acc_1: 0.8250 - acc_2: 0.8380 - val_loss_1: 0.6478 - val_loss_2: 0.6300 - val_acc_ensemble: 0.8093 - val_acc_1: 0.7736 - val_acc_2: 0.7847\n",
      "Epoch 14/50\n",
      "100/100 - 8s - loss_1: 0.5301 - loss_2: 0.5257 - acc_ensemble: 0.8650 - acc_1: 0.8320 - acc_2: 0.8340 - val_loss_1: 0.6229 - val_loss_2: 0.6023 - val_acc_ensemble: 0.8199 - val_acc_1: 0.7857 - val_acc_2: 0.7924\n",
      "Epoch 15/50\n",
      "100/100 - 8s - loss_1: 0.5063 - loss_2: 0.5070 - acc_ensemble: 0.8710 - acc_1: 0.8490 - acc_2: 0.8340 - val_loss_1: 0.6128 - val_loss_2: 0.5947 - val_acc_ensemble: 0.8186 - val_acc_1: 0.7855 - val_acc_2: 0.7944\n",
      "Epoch 16/50\n",
      "100/100 - 8s - loss_1: 0.5066 - loss_2: 0.4831 - acc_ensemble: 0.8700 - acc_1: 0.8290 - acc_2: 0.8650 - val_loss_1: 0.6077 - val_loss_2: 0.5686 - val_acc_ensemble: 0.8266 - val_acc_1: 0.7950 - val_acc_2: 0.8019\n",
      "Epoch 17/50\n",
      "100/100 - 8s - loss_1: 0.4616 - loss_2: 0.4481 - acc_ensemble: 0.8860 - acc_1: 0.8630 - acc_2: 0.8570 - val_loss_1: 0.5833 - val_loss_2: 0.5536 - val_acc_ensemble: 0.8314 - val_acc_1: 0.8012 - val_acc_2: 0.8114\n",
      "Epoch 18/50\n",
      "100/100 - 8s - loss_1: 0.4298 - loss_2: 0.4332 - acc_ensemble: 0.9030 - acc_1: 0.8690 - acc_2: 0.8640 - val_loss_1: 0.5615 - val_loss_2: 0.5652 - val_acc_ensemble: 0.8394 - val_acc_1: 0.8089 - val_acc_2: 0.8055\n",
      "Epoch 19/50\n",
      "100/100 - 8s - loss_1: 0.4206 - loss_2: 0.4104 - acc_ensemble: 0.8870 - acc_1: 0.8590 - acc_2: 0.8730 - val_loss_1: 0.6044 - val_loss_2: 0.5484 - val_acc_ensemble: 0.8357 - val_acc_1: 0.7920 - val_acc_2: 0.8124\n",
      "Epoch 20/50\n",
      "100/100 - 8s - loss_1: 0.4210 - loss_2: 0.3810 - acc_ensemble: 0.8990 - acc_1: 0.8730 - acc_2: 0.8640 - val_loss_1: 0.5535 - val_loss_2: 0.5497 - val_acc_ensemble: 0.8431 - val_acc_1: 0.8100 - val_acc_2: 0.8171\n",
      "Epoch 21/50\n",
      "100/100 - 8s - loss_1: 0.4052 - loss_2: 0.3754 - acc_ensemble: 0.8970 - acc_1: 0.8690 - acc_2: 0.8720 - val_loss_1: 0.5548 - val_loss_2: 0.5477 - val_acc_ensemble: 0.8405 - val_acc_1: 0.8111 - val_acc_2: 0.8151\n",
      "Epoch 22/50\n",
      "100/100 - 8s - loss_1: 0.3806 - loss_2: 0.3719 - acc_ensemble: 0.9160 - acc_1: 0.8630 - acc_2: 0.8920 - val_loss_1: 0.5655 - val_loss_2: 0.5326 - val_acc_ensemble: 0.8400 - val_acc_1: 0.8079 - val_acc_2: 0.8180\n",
      "Epoch 23/50\n",
      "100/100 - 8s - loss_1: 0.3859 - loss_2: 0.3520 - acc_ensemble: 0.9150 - acc_1: 0.8860 - acc_2: 0.8770 - val_loss_1: 0.5370 - val_loss_2: 0.5534 - val_acc_ensemble: 0.8436 - val_acc_1: 0.8189 - val_acc_2: 0.8134\n",
      "Epoch 24/50\n",
      "100/100 - 8s - loss_1: 0.3734 - loss_2: 0.3348 - acc_ensemble: 0.9270 - acc_1: 0.8950 - acc_2: 0.8910 - val_loss_1: 0.5170 - val_loss_2: 0.5272 - val_acc_ensemble: 0.8491 - val_acc_1: 0.8238 - val_acc_2: 0.8262\n",
      "Epoch 25/50\n",
      "100/100 - 8s - loss_1: 0.3198 - loss_2: 0.3126 - acc_ensemble: 0.9150 - acc_1: 0.8770 - acc_2: 0.8900 - val_loss_1: 0.5531 - val_loss_2: 0.5309 - val_acc_ensemble: 0.8484 - val_acc_1: 0.8124 - val_acc_2: 0.8235\n",
      "Epoch 26/50\n",
      "100/100 - 8s - loss_1: 0.3297 - loss_2: 0.2881 - acc_ensemble: 0.9240 - acc_1: 0.8870 - acc_2: 0.9010 - val_loss_1: 0.5516 - val_loss_2: 0.5434 - val_acc_ensemble: 0.8526 - val_acc_1: 0.8142 - val_acc_2: 0.8213\n",
      "Epoch 27/50\n",
      "100/100 - 8s - loss_1: 0.3231 - loss_2: 0.3087 - acc_ensemble: 0.9380 - acc_1: 0.9000 - acc_2: 0.9130 - val_loss_1: 0.5238 - val_loss_2: 0.5383 - val_acc_ensemble: 0.8518 - val_acc_1: 0.8255 - val_acc_2: 0.8222\n",
      "Epoch 28/50\n",
      "100/100 - 8s - loss_1: 0.3044 - loss_2: 0.3083 - acc_ensemble: 0.9400 - acc_1: 0.9160 - acc_2: 0.9110 - val_loss_1: 0.5175 - val_loss_2: 0.5229 - val_acc_ensemble: 0.8551 - val_acc_1: 0.8257 - val_acc_2: 0.8256\n",
      "Epoch 29/50\n",
      "100/100 - 8s - loss_1: 0.2815 - loss_2: 0.2728 - acc_ensemble: 0.9350 - acc_1: 0.9070 - acc_2: 0.9060 - val_loss_1: 0.5150 - val_loss_2: 0.5356 - val_acc_ensemble: 0.8541 - val_acc_1: 0.8288 - val_acc_2: 0.8267\n",
      "Epoch 30/50\n",
      "100/100 - 8s - loss_1: 0.2849 - loss_2: 0.2596 - acc_ensemble: 0.9360 - acc_1: 0.9050 - acc_2: 0.9110 - val_loss_1: 0.5268 - val_loss_2: 0.5382 - val_acc_ensemble: 0.8529 - val_acc_1: 0.8247 - val_acc_2: 0.8247\n",
      "Epoch 31/50\n",
      "100/100 - 8s - loss_1: 0.2565 - loss_2: 0.2414 - acc_ensemble: 0.9410 - acc_1: 0.9020 - acc_2: 0.9110 - val_loss_1: 0.5341 - val_loss_2: 0.5418 - val_acc_ensemble: 0.8539 - val_acc_1: 0.8288 - val_acc_2: 0.8260\n",
      "Epoch 32/50\n",
      "100/100 - 8s - loss_1: 0.2546 - loss_2: 0.2566 - acc_ensemble: 0.9460 - acc_1: 0.9130 - acc_2: 0.9180 - val_loss_1: 0.5315 - val_loss_2: 0.5466 - val_acc_ensemble: 0.8528 - val_acc_1: 0.8254 - val_acc_2: 0.8268\n",
      "Epoch 33/50\n",
      "100/100 - 8s - loss_1: 0.2610 - loss_2: 0.2112 - acc_ensemble: 0.9490 - acc_1: 0.9160 - acc_2: 0.9320 - val_loss_1: 0.5210 - val_loss_2: 0.5167 - val_acc_ensemble: 0.8600 - val_acc_1: 0.8290 - val_acc_2: 0.8373\n",
      "Epoch 34/50\n",
      "100/100 - 8s - loss_1: 0.2378 - loss_2: 0.2145 - acc_ensemble: 0.9520 - acc_1: 0.9290 - acc_2: 0.9170 - val_loss_1: 0.5130 - val_loss_2: 0.5337 - val_acc_ensemble: 0.8610 - val_acc_1: 0.8343 - val_acc_2: 0.8329\n",
      "Epoch 35/50\n",
      "100/100 - 8s - loss_1: 0.2186 - loss_2: 0.2141 - acc_ensemble: 0.9430 - acc_1: 0.9000 - acc_2: 0.9350 - val_loss_1: 0.5444 - val_loss_2: 0.5172 - val_acc_ensemble: 0.8614 - val_acc_1: 0.8303 - val_acc_2: 0.8395\n",
      "Epoch 36/50\n",
      "100/100 - 8s - loss_1: 0.2167 - loss_2: 0.1990 - acc_ensemble: 0.9620 - acc_1: 0.9330 - acc_2: 0.9340 - val_loss_1: 0.5168 - val_loss_2: 0.5365 - val_acc_ensemble: 0.8668 - val_acc_1: 0.8335 - val_acc_2: 0.8383\n",
      "Epoch 37/50\n",
      "100/100 - 8s - loss_1: 0.1954 - loss_2: 0.1872 - acc_ensemble: 0.9560 - acc_1: 0.9320 - acc_2: 0.9310 - val_loss_1: 0.5106 - val_loss_2: 0.5630 - val_acc_ensemble: 0.8623 - val_acc_1: 0.8413 - val_acc_2: 0.8306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 8s - loss_1: 0.2133 - loss_2: 0.1953 - acc_ensemble: 0.9670 - acc_1: 0.9300 - acc_2: 0.9440 - val_loss_1: 0.5312 - val_loss_2: 0.5413 - val_acc_ensemble: 0.8591 - val_acc_1: 0.8304 - val_acc_2: 0.8325\n",
      "Epoch 39/50\n",
      "100/100 - 8s - loss_1: 0.1947 - loss_2: 0.1780 - acc_ensemble: 0.9640 - acc_1: 0.9240 - acc_2: 0.9330 - val_loss_1: 0.5283 - val_loss_2: 0.5627 - val_acc_ensemble: 0.8616 - val_acc_1: 0.8352 - val_acc_2: 0.8300\n",
      "Epoch 40/50\n",
      "100/100 - 8s - loss_1: 0.1986 - loss_2: 0.1753 - acc_ensemble: 0.9710 - acc_1: 0.9390 - acc_2: 0.9290 - val_loss_1: 0.5314 - val_loss_2: 0.5694 - val_acc_ensemble: 0.8633 - val_acc_1: 0.8351 - val_acc_2: 0.8307\n",
      "Epoch 41/50\n",
      "100/100 - 8s - loss_1: 0.1612 - loss_2: 0.1546 - acc_ensemble: 0.9750 - acc_1: 0.9480 - acc_2: 0.9480 - val_loss_1: 0.5315 - val_loss_2: 0.5494 - val_acc_ensemble: 0.8619 - val_acc_1: 0.8350 - val_acc_2: 0.8350\n",
      "Epoch 42/50\n",
      "100/100 - 8s - loss_1: 0.1560 - loss_2: 0.1713 - acc_ensemble: 0.9720 - acc_1: 0.9570 - acc_2: 0.9420 - val_loss_1: 0.5374 - val_loss_2: 0.5648 - val_acc_ensemble: 0.8651 - val_acc_1: 0.8350 - val_acc_2: 0.8308\n",
      "Epoch 43/50\n",
      "100/100 - 8s - loss_1: 0.1524 - loss_2: 0.1547 - acc_ensemble: 0.9770 - acc_1: 0.9470 - acc_2: 0.9630 - val_loss_1: 0.5590 - val_loss_2: 0.5569 - val_acc_ensemble: 0.8657 - val_acc_1: 0.8354 - val_acc_2: 0.8348\n",
      "Epoch 44/50\n",
      "100/100 - 8s - loss_1: 0.1661 - loss_2: 0.1403 - acc_ensemble: 0.9790 - acc_1: 0.9380 - acc_2: 0.9550 - val_loss_1: 0.5590 - val_loss_2: 0.5436 - val_acc_ensemble: 0.8641 - val_acc_1: 0.8317 - val_acc_2: 0.8380\n",
      "Epoch 45/50\n",
      "100/100 - 8s - loss_1: 0.1560 - loss_2: 0.1550 - acc_ensemble: 0.9730 - acc_1: 0.9290 - acc_2: 0.9490 - val_loss_1: 0.5722 - val_loss_2: 0.5709 - val_acc_ensemble: 0.8614 - val_acc_1: 0.8351 - val_acc_2: 0.8321\n",
      "Epoch 46/50\n",
      "100/100 - 8s - loss_1: 0.1517 - loss_2: 0.1323 - acc_ensemble: 0.9800 - acc_1: 0.9400 - acc_2: 0.9520 - val_loss_1: 0.5761 - val_loss_2: 0.5867 - val_acc_ensemble: 0.8629 - val_acc_1: 0.8294 - val_acc_2: 0.8337\n",
      "Epoch 47/50\n",
      "100/100 - 8s - loss_1: 0.1419 - loss_2: 0.1297 - acc_ensemble: 0.9780 - acc_1: 0.9430 - acc_2: 0.9560 - val_loss_1: 0.5713 - val_loss_2: 0.5766 - val_acc_ensemble: 0.8641 - val_acc_1: 0.8350 - val_acc_2: 0.8356\n",
      "Epoch 48/50\n",
      "100/100 - 8s - loss_1: 0.1342 - loss_2: 0.1147 - acc_ensemble: 0.9850 - acc_1: 0.9520 - acc_2: 0.9560 - val_loss_1: 0.5459 - val_loss_2: 0.5989 - val_acc_ensemble: 0.8675 - val_acc_1: 0.8391 - val_acc_2: 0.8360\n",
      "Epoch 49/50\n",
      "100/100 - 8s - loss_1: 0.1429 - loss_2: 0.1253 - acc_ensemble: 0.9800 - acc_1: 0.9460 - acc_2: 0.9590 - val_loss_1: 0.5665 - val_loss_2: 0.5928 - val_acc_ensemble: 0.8651 - val_acc_1: 0.8386 - val_acc_2: 0.8390\n",
      "Epoch 50/50\n",
      "100/100 - 8s - loss_1: 0.1264 - loss_2: 0.1302 - acc_ensemble: 0.9840 - acc_1: 0.9520 - acc_2: 0.9640 - val_loss_1: 0.5850 - val_loss_2: 0.5942 - val_acc_ensemble: 0.8684 - val_acc_1: 0.8347 - val_acc_2: 0.8346\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.00\n",
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [] [None,32,32,32]  1792       input              \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_1         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [] [None,32,32,32]  0          bn_1_1             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [] [None,32,32,32]  18496      relu_1_1           \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_2         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [] [None,32,32,32]  0          bn_1_2             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [] [None,16,16,32]  0          relu_1_2           \n",
      "                                    [] [None,16,16,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [] [None,16,16,64]  36992      avg_pool2d_1       \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [] [None,16,16,64]  256        conv2d_2_1         \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [] [None,16,16,64]  0          bn_2_1             \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [] [None,16,16,64]  73856      relu_2_1           \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [] [None,16,16,64]  256        conv2d_2_2         \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [] [None,16,16,64]  0          bn_2_2             \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [] [None,8,8,64]    0          relu_2_2           \n",
      "                                    [] [None,8,8,64]                                  \n",
      "--------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [] [None,8,8,128]   147712     avg_pool2d_2       \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [] [None,8,8,128]   512        conv2d_3_1         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [] [None,8,8,128]   0          bn_3_1             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [] [None,8,8,128]   295168     relu_3_1           \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [] [None,8,8,128]   512        conv2d_3_2         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [] [None,8,8,128]   0          bn_3_2             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [] [None,128]       0          relu_3_2           \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [] [None,128]       33024      global_avg_pool2d  \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [] [None,128]       512        fc1                \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [] [None,128]       0          bn_fc1             \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]           2580       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 611924\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 9s - loss_1: 1.6562 - loss_2: 1.6376 - acc_ensemble: 0.5500 - acc_1: 0.5180 - acc_2: 0.5260 - val_loss_1: 1.3693 - val_loss_2: 1.3639 - val_acc_ensemble: 0.5307 - val_acc_1: 0.5015 - val_acc_2: 0.5003\n",
      "Epoch 2/50\n",
      "100/100 - 8s - loss_1: 1.2771 - loss_2: 1.2634 - acc_ensemble: 0.6070 - acc_1: 0.5670 - acc_2: 0.5770 - val_loss_1: 1.1764 - val_loss_2: 1.1632 - val_acc_ensemble: 0.6050 - val_acc_1: 0.5683 - val_acc_2: 0.5761\n",
      "Epoch 3/50\n",
      "100/100 - 8s - loss_1: 1.1097 - loss_2: 1.1110 - acc_ensemble: 0.6520 - acc_1: 0.6180 - acc_2: 0.6650 - val_loss_1: 1.0256 - val_loss_2: 1.0004 - val_acc_ensemble: 0.6607 - val_acc_1: 0.6263 - val_acc_2: 0.6435\n",
      "Epoch 4/50\n",
      "100/100 - 8s - loss_1: 0.9852 - loss_2: 1.0059 - acc_ensemble: 0.7070 - acc_1: 0.6780 - acc_2: 0.6760 - val_loss_1: 0.9861 - val_loss_2: 0.9898 - val_acc_ensemble: 0.6754 - val_acc_1: 0.6444 - val_acc_2: 0.6455\n",
      "Epoch 5/50\n",
      "100/100 - 8s - loss_1: 0.9195 - loss_2: 0.9234 - acc_ensemble: 0.7260 - acc_1: 0.7030 - acc_2: 0.6860 - val_loss_1: 0.9035 - val_loss_2: 0.9048 - val_acc_ensemble: 0.7094 - val_acc_1: 0.6842 - val_acc_2: 0.6760\n",
      "Epoch 6/50\n",
      "100/100 - 8s - loss_1: 0.8391 - loss_2: 0.8324 - acc_ensemble: 0.7600 - acc_1: 0.7290 - acc_2: 0.7300 - val_loss_1: 0.8434 - val_loss_2: 0.8304 - val_acc_ensemble: 0.7313 - val_acc_1: 0.7023 - val_acc_2: 0.7054\n",
      "Epoch 7/50\n",
      "100/100 - 8s - loss_1: 0.7739 - loss_2: 0.7768 - acc_ensemble: 0.7820 - acc_1: 0.7590 - acc_2: 0.7390 - val_loss_1: 0.7994 - val_loss_2: 0.7995 - val_acc_ensemble: 0.7440 - val_acc_1: 0.7147 - val_acc_2: 0.7159\n",
      "Epoch 8/50\n",
      "100/100 - 8s - loss_1: 0.7166 - loss_2: 0.7351 - acc_ensemble: 0.7950 - acc_1: 0.7690 - acc_2: 0.7480 - val_loss_1: 0.7551 - val_loss_2: 0.7425 - val_acc_ensemble: 0.7628 - val_acc_1: 0.7327 - val_acc_2: 0.7400\n",
      "Epoch 9/50\n",
      "100/100 - 8s - loss_1: 0.6694 - loss_2: 0.6821 - acc_ensemble: 0.8090 - acc_1: 0.7740 - acc_2: 0.7700 - val_loss_1: 0.7192 - val_loss_2: 0.7132 - val_acc_ensemble: 0.7780 - val_acc_1: 0.7487 - val_acc_2: 0.7483\n",
      "Epoch 10/50\n",
      "100/100 - 8s - loss_1: 0.6409 - loss_2: 0.6401 - acc_ensemble: 0.8290 - acc_1: 0.7970 - acc_2: 0.7910 - val_loss_1: 0.6747 - val_loss_2: 0.7005 - val_acc_ensemble: 0.7814 - val_acc_1: 0.7643 - val_acc_2: 0.7549\n",
      "Epoch 11/50\n",
      "100/100 - 8s - loss_1: 0.6001 - loss_2: 0.6041 - acc_ensemble: 0.8330 - acc_1: 0.8040 - acc_2: 0.7910 - val_loss_1: 0.7005 - val_loss_2: 0.6920 - val_acc_ensemble: 0.7882 - val_acc_1: 0.7541 - val_acc_2: 0.7565\n",
      "Epoch 12/50\n",
      "100/100 - 8s - loss_1: 0.5720 - loss_2: 0.5911 - acc_ensemble: 0.8440 - acc_1: 0.8300 - acc_2: 0.8090 - val_loss_1: 0.6325 - val_loss_2: 0.6512 - val_acc_ensemble: 0.7985 - val_acc_1: 0.7801 - val_acc_2: 0.7699\n",
      "Epoch 13/50\n",
      "100/100 - 8s - loss_1: 0.5423 - loss_2: 0.5434 - acc_ensemble: 0.8620 - acc_1: 0.8370 - acc_2: 0.8200 - val_loss_1: 0.6205 - val_loss_2: 0.6404 - val_acc_ensemble: 0.8093 - val_acc_1: 0.7851 - val_acc_2: 0.7764\n",
      "Epoch 14/50\n",
      "100/100 - 8s - loss_1: 0.5029 - loss_2: 0.5285 - acc_ensemble: 0.8620 - acc_1: 0.8350 - acc_2: 0.8250 - val_loss_1: 0.6136 - val_loss_2: 0.6217 - val_acc_ensemble: 0.8109 - val_acc_1: 0.7877 - val_acc_2: 0.7836\n",
      "Epoch 15/50\n",
      "100/100 - 8s - loss_1: 0.5034 - loss_2: 0.4989 - acc_ensemble: 0.8780 - acc_1: 0.8530 - acc_2: 0.8340 - val_loss_1: 0.5961 - val_loss_2: 0.6011 - val_acc_ensemble: 0.8221 - val_acc_1: 0.7956 - val_acc_2: 0.7915\n",
      "Epoch 16/50\n",
      "100/100 - 8s - loss_1: 0.4533 - loss_2: 0.4958 - acc_ensemble: 0.8810 - acc_1: 0.8530 - acc_2: 0.8480 - val_loss_1: 0.5902 - val_loss_2: 0.6138 - val_acc_ensemble: 0.8208 - val_acc_1: 0.7981 - val_acc_2: 0.7879\n",
      "Epoch 17/50\n",
      "100/100 - 8s - loss_1: 0.4388 - loss_2: 0.4710 - acc_ensemble: 0.8780 - acc_1: 0.8660 - acc_2: 0.8510 - val_loss_1: 0.5585 - val_loss_2: 0.5900 - val_acc_ensemble: 0.8283 - val_acc_1: 0.8105 - val_acc_2: 0.7984\n",
      "Epoch 18/50\n",
      "100/100 - 8s - loss_1: 0.4154 - loss_2: 0.4307 - acc_ensemble: 0.8940 - acc_1: 0.8560 - acc_2: 0.8600 - val_loss_1: 0.5708 - val_loss_2: 0.5692 - val_acc_ensemble: 0.8328 - val_acc_1: 0.8054 - val_acc_2: 0.8071\n",
      "Epoch 19/50\n",
      "100/100 - 8s - loss_1: 0.3989 - loss_2: 0.4120 - acc_ensemble: 0.8980 - acc_1: 0.8790 - acc_2: 0.8730 - val_loss_1: 0.5488 - val_loss_2: 0.5645 - val_acc_ensemble: 0.8367 - val_acc_1: 0.8095 - val_acc_2: 0.8071\n",
      "Epoch 20/50\n",
      "100/100 - 8s - loss_1: 0.3647 - loss_2: 0.4013 - acc_ensemble: 0.9070 - acc_1: 0.8860 - acc_2: 0.8760 - val_loss_1: 0.5381 - val_loss_2: 0.5442 - val_acc_ensemble: 0.8421 - val_acc_1: 0.8159 - val_acc_2: 0.8140\n",
      "Epoch 21/50\n",
      "100/100 - 8s - loss_1: 0.3824 - loss_2: 0.3871 - acc_ensemble: 0.9140 - acc_1: 0.8980 - acc_2: 0.8840 - val_loss_1: 0.5408 - val_loss_2: 0.5651 - val_acc_ensemble: 0.8388 - val_acc_1: 0.8168 - val_acc_2: 0.8066\n",
      "Epoch 22/50\n",
      "100/100 - 8s - loss_1: 0.3653 - loss_2: 0.3832 - acc_ensemble: 0.9130 - acc_1: 0.8940 - acc_2: 0.8730 - val_loss_1: 0.5485 - val_loss_2: 0.5536 - val_acc_ensemble: 0.8439 - val_acc_1: 0.8166 - val_acc_2: 0.8123\n",
      "Epoch 23/50\n",
      "100/100 - 8s - loss_1: 0.3570 - loss_2: 0.3793 - acc_ensemble: 0.9260 - acc_1: 0.9090 - acc_2: 0.8830 - val_loss_1: 0.5356 - val_loss_2: 0.5463 - val_acc_ensemble: 0.8440 - val_acc_1: 0.8221 - val_acc_2: 0.8139\n",
      "Epoch 24/50\n",
      "100/100 - 8s - loss_1: 0.3120 - loss_2: 0.3346 - acc_ensemble: 0.9100 - acc_1: 0.8890 - acc_2: 0.8880 - val_loss_1: 0.5640 - val_loss_2: 0.5576 - val_acc_ensemble: 0.8402 - val_acc_1: 0.8136 - val_acc_2: 0.8124\n",
      "Epoch 25/50\n",
      "100/100 - 8s - loss_1: 0.3285 - loss_2: 0.3418 - acc_ensemble: 0.9190 - acc_1: 0.9080 - acc_2: 0.8960 - val_loss_1: 0.5448 - val_loss_2: 0.5475 - val_acc_ensemble: 0.8430 - val_acc_1: 0.8193 - val_acc_2: 0.8129\n",
      "Epoch 26/50\n",
      "100/100 - 8s - loss_1: 0.2915 - loss_2: 0.3050 - acc_ensemble: 0.9300 - acc_1: 0.9070 - acc_2: 0.8840 - val_loss_1: 0.5378 - val_loss_2: 0.5500 - val_acc_ensemble: 0.8489 - val_acc_1: 0.8213 - val_acc_2: 0.8170\n",
      "Epoch 27/50\n",
      "100/100 - 8s - loss_1: 0.2800 - loss_2: 0.3069 - acc_ensemble: 0.9380 - acc_1: 0.9270 - acc_2: 0.8930 - val_loss_1: 0.5210 - val_loss_2: 0.5584 - val_acc_ensemble: 0.8481 - val_acc_1: 0.8296 - val_acc_2: 0.8149\n",
      "Epoch 28/50\n",
      "100/100 - 8s - loss_1: 0.2666 - loss_2: 0.3001 - acc_ensemble: 0.9490 - acc_1: 0.9150 - acc_2: 0.9140 - val_loss_1: 0.5332 - val_loss_2: 0.5310 - val_acc_ensemble: 0.8518 - val_acc_1: 0.8269 - val_acc_2: 0.8223\n",
      "Epoch 29/50\n",
      "100/100 - 8s - loss_1: 0.2744 - loss_2: 0.2879 - acc_ensemble: 0.9440 - acc_1: 0.9110 - acc_2: 0.9120 - val_loss_1: 0.5440 - val_loss_2: 0.5204 - val_acc_ensemble: 0.8529 - val_acc_1: 0.8215 - val_acc_2: 0.8269\n",
      "Epoch 30/50\n",
      "100/100 - 8s - loss_1: 0.2628 - loss_2: 0.2858 - acc_ensemble: 0.9510 - acc_1: 0.9160 - acc_2: 0.9120 - val_loss_1: 0.5317 - val_loss_2: 0.5436 - val_acc_ensemble: 0.8549 - val_acc_1: 0.8230 - val_acc_2: 0.8223\n",
      "Epoch 31/50\n",
      "100/100 - 8s - loss_1: 0.2618 - loss_2: 0.2734 - acc_ensemble: 0.9500 - acc_1: 0.9170 - acc_2: 0.9070 - val_loss_1: 0.5470 - val_loss_2: 0.5326 - val_acc_ensemble: 0.8548 - val_acc_1: 0.8241 - val_acc_2: 0.8294\n",
      "Epoch 32/50\n",
      "100/100 - 8s - loss_1: 0.2588 - loss_2: 0.2627 - acc_ensemble: 0.9490 - acc_1: 0.9280 - acc_2: 0.9120 - val_loss_1: 0.5206 - val_loss_2: 0.5598 - val_acc_ensemble: 0.8550 - val_acc_1: 0.8351 - val_acc_2: 0.8184\n",
      "Epoch 33/50\n",
      "100/100 - 8s - loss_1: 0.2355 - loss_2: 0.2343 - acc_ensemble: 0.9570 - acc_1: 0.9280 - acc_2: 0.9310 - val_loss_1: 0.5357 - val_loss_2: 0.5156 - val_acc_ensemble: 0.8570 - val_acc_1: 0.8290 - val_acc_2: 0.8300\n",
      "Epoch 34/50\n",
      "100/100 - 8s - loss_1: 0.2208 - loss_2: 0.2351 - acc_ensemble: 0.9630 - acc_1: 0.9300 - acc_2: 0.9270 - val_loss_1: 0.5543 - val_loss_2: 0.5213 - val_acc_ensemble: 0.8585 - val_acc_1: 0.8227 - val_acc_2: 0.8332\n",
      "Epoch 35/50\n",
      "100/100 - 8s - loss_1: 0.2112 - loss_2: 0.2177 - acc_ensemble: 0.9650 - acc_1: 0.9510 - acc_2: 0.9270 - val_loss_1: 0.5258 - val_loss_2: 0.5470 - val_acc_ensemble: 0.8587 - val_acc_1: 0.8344 - val_acc_2: 0.8277\n",
      "Epoch 36/50\n",
      "100/100 - 8s - loss_1: 0.1817 - loss_2: 0.1999 - acc_ensemble: 0.9600 - acc_1: 0.9420 - acc_2: 0.9280 - val_loss_1: 0.5190 - val_loss_2: 0.5474 - val_acc_ensemble: 0.8593 - val_acc_1: 0.8370 - val_acc_2: 0.8340\n",
      "Epoch 37/50\n",
      "100/100 - 8s - loss_1: 0.2019 - loss_2: 0.1805 - acc_ensemble: 0.9700 - acc_1: 0.9340 - acc_2: 0.9360 - val_loss_1: 0.5421 - val_loss_2: 0.5373 - val_acc_ensemble: 0.8549 - val_acc_1: 0.8308 - val_acc_2: 0.8310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 8s - loss_1: 0.1901 - loss_2: 0.1995 - acc_ensemble: 0.9710 - acc_1: 0.9480 - acc_2: 0.9250 - val_loss_1: 0.5338 - val_loss_2: 0.5494 - val_acc_ensemble: 0.8600 - val_acc_1: 0.8340 - val_acc_2: 0.8268\n",
      "Epoch 39/50\n",
      "100/100 - 8s - loss_1: 0.1655 - loss_2: 0.1702 - acc_ensemble: 0.9680 - acc_1: 0.9300 - acc_2: 0.9360 - val_loss_1: 0.5718 - val_loss_2: 0.5560 - val_acc_ensemble: 0.8592 - val_acc_1: 0.8206 - val_acc_2: 0.8347\n",
      "Epoch 40/50\n",
      "100/100 - 8s - loss_1: 0.1709 - loss_2: 0.1666 - acc_ensemble: 0.9710 - acc_1: 0.9450 - acc_2: 0.9340 - val_loss_1: 0.5361 - val_loss_2: 0.5675 - val_acc_ensemble: 0.8599 - val_acc_1: 0.8374 - val_acc_2: 0.8276\n",
      "Epoch 41/50\n",
      "100/100 - 8s - loss_1: 0.1672 - loss_2: 0.1886 - acc_ensemble: 0.9750 - acc_1: 0.9440 - acc_2: 0.9400 - val_loss_1: 0.5479 - val_loss_2: 0.5510 - val_acc_ensemble: 0.8626 - val_acc_1: 0.8337 - val_acc_2: 0.8295\n",
      "Epoch 42/50\n",
      "100/100 - 8s - loss_1: 0.1848 - loss_2: 0.1670 - acc_ensemble: 0.9770 - acc_1: 0.9430 - acc_2: 0.9430 - val_loss_1: 0.5352 - val_loss_2: 0.5797 - val_acc_ensemble: 0.8574 - val_acc_1: 0.8400 - val_acc_2: 0.8264\n",
      "Epoch 43/50\n",
      "100/100 - 8s - loss_1: 0.1638 - loss_2: 0.1659 - acc_ensemble: 0.9800 - acc_1: 0.9570 - acc_2: 0.9480 - val_loss_1: 0.5366 - val_loss_2: 0.5641 - val_acc_ensemble: 0.8637 - val_acc_1: 0.8367 - val_acc_2: 0.8283\n",
      "Epoch 44/50\n",
      "100/100 - 8s - loss_1: 0.1291 - loss_2: 0.1715 - acc_ensemble: 0.9760 - acc_1: 0.9550 - acc_2: 0.9430 - val_loss_1: 0.5493 - val_loss_2: 0.5519 - val_acc_ensemble: 0.8641 - val_acc_1: 0.8389 - val_acc_2: 0.8312\n",
      "Epoch 45/50\n",
      "100/100 - 8s - loss_1: 0.1485 - loss_2: 0.1622 - acc_ensemble: 0.9760 - acc_1: 0.9500 - acc_2: 0.9550 - val_loss_1: 0.5576 - val_loss_2: 0.5639 - val_acc_ensemble: 0.8661 - val_acc_1: 0.8363 - val_acc_2: 0.8348\n",
      "Epoch 46/50\n",
      "100/100 - 8s - loss_1: 0.1340 - loss_2: 0.1192 - acc_ensemble: 0.9770 - acc_1: 0.9660 - acc_2: 0.9440 - val_loss_1: 0.5700 - val_loss_2: 0.5648 - val_acc_ensemble: 0.8685 - val_acc_1: 0.8380 - val_acc_2: 0.8375\n",
      "Epoch 47/50\n",
      "100/100 - 8s - loss_1: 0.1224 - loss_2: 0.1227 - acc_ensemble: 0.9740 - acc_1: 0.9590 - acc_2: 0.9570 - val_loss_1: 0.5682 - val_loss_2: 0.5524 - val_acc_ensemble: 0.8652 - val_acc_1: 0.8403 - val_acc_2: 0.8384\n",
      "Epoch 48/50\n",
      "100/100 - 8s - loss_1: 0.1176 - loss_2: 0.1521 - acc_ensemble: 0.9790 - acc_1: 0.9540 - acc_2: 0.9530 - val_loss_1: 0.5823 - val_loss_2: 0.5702 - val_acc_ensemble: 0.8620 - val_acc_1: 0.8389 - val_acc_2: 0.8346\n",
      "Epoch 49/50\n",
      "100/100 - 8s - loss_1: 0.1121 - loss_2: 0.1203 - acc_ensemble: 0.9850 - acc_1: 0.9580 - acc_2: 0.9560 - val_loss_1: 0.6095 - val_loss_2: 0.5715 - val_acc_ensemble: 0.8651 - val_acc_1: 0.8339 - val_acc_2: 0.8349\n",
      "Epoch 50/50\n",
      "100/100 - 8s - loss_1: 0.1287 - loss_2: 0.1233 - acc_ensemble: 0.9850 - acc_1: 0.9570 - acc_2: 0.9650 - val_loss_1: 0.5840 - val_loss_2: 0.5631 - val_acc_ensemble: 0.8677 - val_acc_1: 0.8391 - val_acc_2: 0.8364\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.25\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,8] [None,32,32,24]   1568       input              \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,8] [None,32,32,24]   112        conv2d_1_1         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,8] [None,32,32,24]   0          bn_1_1             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,8] [None,32,32,24]   17976      relu_1_1           \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,8] [None,32,32,24]   112        conv2d_1_2         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,8] [None,32,32,24]   0          bn_1_2             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,8] [None,16,16,24]   0          relu_1_2           \n",
      "                                    [None,16,16,8] [None,16,16,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,16] [None,16,16,48]  35952      avg_pool2d_1       \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,16] [None,16,16,48]  224        conv2d_2_1         \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,16] [None,16,16,48]  0          bn_2_1             \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,16] [None,16,16,48]  71664      relu_2_1           \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,16] [None,16,16,48]  224        conv2d_2_2         \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,16] [None,16,16,48]  0          bn_2_2             \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,16] [None,8,8,48]      0          relu_2_2           \n",
      "                                    [None,8,8,16] [None,8,8,48]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,32] [None,8,8,96]      143328     avg_pool2d_2       \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,32] [None,8,8,96]      448        conv2d_3_1         \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,32] [None,8,8,96]      0          bn_3_1             \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,32] [None,8,8,96]      286176     relu_3_1           \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,32] [None,8,8,96]      448        conv2d_3_2         \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,32] [None,8,8,96]      0          bn_3_2             \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,32] [None,96]              0          relu_3_2           \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,32] [None,96]              32224      global_avg_pool2d  \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,32] [None,96]              448        fc1                \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,32] [None,96]              0          bn_fc1             \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        2534       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 593438\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 20s - loss_1: 1.6436 - loss_2: 1.6646 - acc_ensemble: 0.5320 - acc_1: 0.4950 - acc_2: 0.5290 - val_loss_1: 1.3825 - val_loss_2: 1.3539 - val_acc_ensemble: 0.5239 - val_acc_1: 0.4876 - val_acc_2: 0.5064\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.2567 - loss_2: 1.2920 - acc_ensemble: 0.6120 - acc_1: 0.5800 - acc_2: 0.5910 - val_loss_1: 1.1679 - val_loss_2: 1.1964 - val_acc_ensemble: 0.5998 - val_acc_1: 0.5718 - val_acc_2: 0.5677\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.0967 - loss_2: 1.1159 - acc_ensemble: 0.6730 - acc_1: 0.6410 - acc_2: 0.6570 - val_loss_1: 1.0416 - val_loss_2: 1.0404 - val_acc_ensemble: 0.6618 - val_acc_1: 0.6322 - val_acc_2: 0.6303\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 0.9726 - loss_2: 1.0032 - acc_ensemble: 0.6990 - acc_1: 0.6800 - acc_2: 0.6740 - val_loss_1: 0.9822 - val_loss_2: 0.9730 - val_acc_ensemble: 0.6801 - val_acc_1: 0.6558 - val_acc_2: 0.6558\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9076 - loss_2: 0.9126 - acc_ensemble: 0.7190 - acc_1: 0.6850 - acc_2: 0.6890 - val_loss_1: 0.9250 - val_loss_2: 0.9206 - val_acc_ensemble: 0.6989 - val_acc_1: 0.6697 - val_acc_2: 0.6706\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8157 - loss_2: 0.8356 - acc_ensemble: 0.7560 - acc_1: 0.7150 - acc_2: 0.7280 - val_loss_1: 0.8593 - val_loss_2: 0.8550 - val_acc_ensemble: 0.7240 - val_acc_1: 0.6980 - val_acc_2: 0.6978\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.7610 - loss_2: 0.7777 - acc_ensemble: 0.7780 - acc_1: 0.7620 - acc_2: 0.7480 - val_loss_1: 0.8001 - val_loss_2: 0.8150 - val_acc_ensemble: 0.7396 - val_acc_1: 0.7192 - val_acc_2: 0.7146\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7142 - loss_2: 0.7357 - acc_ensemble: 0.7930 - acc_1: 0.7650 - acc_2: 0.7640 - val_loss_1: 0.7728 - val_loss_2: 0.7574 - val_acc_ensemble: 0.7577 - val_acc_1: 0.7262 - val_acc_2: 0.7391\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.6740 - loss_2: 0.6836 - acc_ensemble: 0.8150 - acc_1: 0.7850 - acc_2: 0.7750 - val_loss_1: 0.7688 - val_loss_2: 0.7445 - val_acc_ensemble: 0.7607 - val_acc_1: 0.7296 - val_acc_2: 0.7384\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6236 - loss_2: 0.6475 - acc_ensemble: 0.8200 - acc_1: 0.7940 - acc_2: 0.7920 - val_loss_1: 0.7376 - val_loss_2: 0.7247 - val_acc_ensemble: 0.7784 - val_acc_1: 0.7439 - val_acc_2: 0.7476\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6170 - loss_2: 0.6135 - acc_ensemble: 0.8370 - acc_1: 0.7940 - acc_2: 0.8020 - val_loss_1: 0.7004 - val_loss_2: 0.7085 - val_acc_ensemble: 0.7834 - val_acc_1: 0.7590 - val_acc_2: 0.7536\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.5570 - loss_2: 0.5427 - acc_ensemble: 0.8420 - acc_1: 0.8100 - acc_2: 0.8110 - val_loss_1: 0.7095 - val_loss_2: 0.6984 - val_acc_ensemble: 0.7868 - val_acc_1: 0.7497 - val_acc_2: 0.7630\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5693 - loss_2: 0.5487 - acc_ensemble: 0.8660 - acc_1: 0.8350 - acc_2: 0.8070 - val_loss_1: 0.6645 - val_loss_2: 0.6872 - val_acc_ensemble: 0.7885 - val_acc_1: 0.7668 - val_acc_2: 0.7632\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5173 - loss_2: 0.5198 - acc_ensemble: 0.8790 - acc_1: 0.8320 - acc_2: 0.8440 - val_loss_1: 0.6769 - val_loss_2: 0.6656 - val_acc_ensemble: 0.7993 - val_acc_1: 0.7661 - val_acc_2: 0.7717\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.4808 - loss_2: 0.4681 - acc_ensemble: 0.8870 - acc_1: 0.8390 - acc_2: 0.8460 - val_loss_1: 0.6537 - val_loss_2: 0.6489 - val_acc_ensemble: 0.8083 - val_acc_1: 0.7764 - val_acc_2: 0.7778\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.4719 - loss_2: 0.4445 - acc_ensemble: 0.8930 - acc_1: 0.8560 - acc_2: 0.8400 - val_loss_1: 0.6566 - val_loss_2: 0.6381 - val_acc_ensemble: 0.8062 - val_acc_1: 0.7751 - val_acc_2: 0.7791\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4400 - loss_2: 0.4476 - acc_ensemble: 0.8950 - acc_1: 0.8680 - acc_2: 0.8700 - val_loss_1: 0.6441 - val_loss_2: 0.6474 - val_acc_ensemble: 0.8114 - val_acc_1: 0.7816 - val_acc_2: 0.7804\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4298 - loss_2: 0.4227 - acc_ensemble: 0.9010 - acc_1: 0.8740 - acc_2: 0.8740 - val_loss_1: 0.6453 - val_loss_2: 0.6345 - val_acc_ensemble: 0.8142 - val_acc_1: 0.7844 - val_acc_2: 0.7867\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.4044 - loss_2: 0.3950 - acc_ensemble: 0.9080 - acc_1: 0.8610 - acc_2: 0.8730 - val_loss_1: 0.6394 - val_loss_2: 0.6279 - val_acc_ensemble: 0.8200 - val_acc_1: 0.7872 - val_acc_2: 0.7871\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.3836 - loss_2: 0.3845 - acc_ensemble: 0.9170 - acc_1: 0.8910 - acc_2: 0.9070 - val_loss_1: 0.6238 - val_loss_2: 0.6031 - val_acc_ensemble: 0.8250 - val_acc_1: 0.7942 - val_acc_2: 0.8040\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3761 - loss_2: 0.3840 - acc_ensemble: 0.9110 - acc_1: 0.8830 - acc_2: 0.8910 - val_loss_1: 0.6423 - val_loss_2: 0.6311 - val_acc_ensemble: 0.8188 - val_acc_1: 0.7822 - val_acc_2: 0.7881\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3522 - loss_2: 0.3408 - acc_ensemble: 0.9300 - acc_1: 0.8970 - acc_2: 0.9020 - val_loss_1: 0.6126 - val_loss_2: 0.6039 - val_acc_ensemble: 0.8275 - val_acc_1: 0.8005 - val_acc_2: 0.7979\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3116 - loss_2: 0.3436 - acc_ensemble: 0.9320 - acc_1: 0.9010 - acc_2: 0.8960 - val_loss_1: 0.6289 - val_loss_2: 0.6235 - val_acc_ensemble: 0.8268 - val_acc_1: 0.7904 - val_acc_2: 0.7952\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3265 - loss_2: 0.2813 - acc_ensemble: 0.9310 - acc_1: 0.8850 - acc_2: 0.9120 - val_loss_1: 0.6533 - val_loss_2: 0.6221 - val_acc_ensemble: 0.8235 - val_acc_1: 0.7894 - val_acc_2: 0.7977\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3042 - loss_2: 0.3107 - acc_ensemble: 0.9400 - acc_1: 0.8960 - acc_2: 0.9080 - val_loss_1: 0.6188 - val_loss_2: 0.6209 - val_acc_ensemble: 0.8299 - val_acc_1: 0.7964 - val_acc_2: 0.7994\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.3024 - loss_2: 0.2972 - acc_ensemble: 0.9500 - acc_1: 0.8930 - acc_2: 0.9090 - val_loss_1: 0.6237 - val_loss_2: 0.6215 - val_acc_ensemble: 0.8308 - val_acc_1: 0.7970 - val_acc_2: 0.8020\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2508 - loss_2: 0.2489 - acc_ensemble: 0.9390 - acc_1: 0.8960 - acc_2: 0.9190 - val_loss_1: 0.6491 - val_loss_2: 0.6285 - val_acc_ensemble: 0.8271 - val_acc_1: 0.7912 - val_acc_2: 0.7977\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2611 - loss_2: 0.2551 - acc_ensemble: 0.9520 - acc_1: 0.9140 - acc_2: 0.9200 - val_loss_1: 0.6510 - val_loss_2: 0.6352 - val_acc_ensemble: 0.8321 - val_acc_1: 0.7923 - val_acc_2: 0.7964\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2460 - loss_2: 0.2543 - acc_ensemble: 0.9440 - acc_1: 0.9110 - acc_2: 0.9210 - val_loss_1: 0.6438 - val_loss_2: 0.6485 - val_acc_ensemble: 0.8313 - val_acc_1: 0.7955 - val_acc_2: 0.7965\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2309 - loss_2: 0.2418 - acc_ensemble: 0.9590 - acc_1: 0.9160 - acc_2: 0.9290 - val_loss_1: 0.6304 - val_loss_2: 0.6347 - val_acc_ensemble: 0.8369 - val_acc_1: 0.8022 - val_acc_2: 0.7992\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2294 - loss_2: 0.2125 - acc_ensemble: 0.9590 - acc_1: 0.9190 - acc_2: 0.9420 - val_loss_1: 0.6386 - val_loss_2: 0.6348 - val_acc_ensemble: 0.8393 - val_acc_1: 0.8096 - val_acc_2: 0.8018\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2051 - loss_2: 0.2062 - acc_ensemble: 0.9640 - acc_1: 0.9280 - acc_2: 0.9340 - val_loss_1: 0.6428 - val_loss_2: 0.6634 - val_acc_ensemble: 0.8367 - val_acc_1: 0.8025 - val_acc_2: 0.7966\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2174 - loss_2: 0.2000 - acc_ensemble: 0.9630 - acc_1: 0.9170 - acc_2: 0.9310 - val_loss_1: 0.6623 - val_loss_2: 0.6635 - val_acc_ensemble: 0.8358 - val_acc_1: 0.8005 - val_acc_2: 0.8015\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.1951 - loss_2: 0.1937 - acc_ensemble: 0.9720 - acc_1: 0.9320 - acc_2: 0.9530 - val_loss_1: 0.6417 - val_loss_2: 0.6345 - val_acc_ensemble: 0.8383 - val_acc_1: 0.8056 - val_acc_2: 0.8078\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.1734 - loss_2: 0.1992 - acc_ensemble: 0.9720 - acc_1: 0.9350 - acc_2: 0.9410 - val_loss_1: 0.6602 - val_loss_2: 0.6753 - val_acc_ensemble: 0.8373 - val_acc_1: 0.8063 - val_acc_2: 0.8055\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.1938 - loss_2: 0.2061 - acc_ensemble: 0.9750 - acc_1: 0.9390 - acc_2: 0.9500 - val_loss_1: 0.6459 - val_loss_2: 0.6631 - val_acc_ensemble: 0.8396 - val_acc_1: 0.8080 - val_acc_2: 0.8050\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.1748 - loss_2: 0.1801 - acc_ensemble: 0.9710 - acc_1: 0.9300 - acc_2: 0.9550 - val_loss_1: 0.6939 - val_loss_2: 0.6944 - val_acc_ensemble: 0.8348 - val_acc_1: 0.7950 - val_acc_2: 0.7965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1773 - loss_2: 0.1951 - acc_ensemble: 0.9690 - acc_1: 0.9390 - acc_2: 0.9490 - val_loss_1: 0.6469 - val_loss_2: 0.6729 - val_acc_ensemble: 0.8399 - val_acc_1: 0.8084 - val_acc_2: 0.8042\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1797 - loss_2: 0.1569 - acc_ensemble: 0.9790 - acc_1: 0.9420 - acc_2: 0.9660 - val_loss_1: 0.6883 - val_loss_2: 0.6673 - val_acc_ensemble: 0.8417 - val_acc_1: 0.7973 - val_acc_2: 0.8090\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1520 - loss_2: 0.1422 - acc_ensemble: 0.9830 - acc_1: 0.9580 - acc_2: 0.9690 - val_loss_1: 0.6877 - val_loss_2: 0.6702 - val_acc_ensemble: 0.8383 - val_acc_1: 0.7988 - val_acc_2: 0.8091\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1213 - loss_2: 0.1468 - acc_ensemble: 0.9860 - acc_1: 0.9610 - acc_2: 0.9640 - val_loss_1: 0.6981 - val_loss_2: 0.6900 - val_acc_ensemble: 0.8419 - val_acc_1: 0.8077 - val_acc_2: 0.8014\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1598 - loss_2: 0.1116 - acc_ensemble: 0.9860 - acc_1: 0.9490 - acc_2: 0.9610 - val_loss_1: 0.7310 - val_loss_2: 0.6971 - val_acc_ensemble: 0.8368 - val_acc_1: 0.7952 - val_acc_2: 0.8066\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1385 - loss_2: 0.1415 - acc_ensemble: 0.9850 - acc_1: 0.9470 - acc_2: 0.9540 - val_loss_1: 0.6875 - val_loss_2: 0.7107 - val_acc_ensemble: 0.8440 - val_acc_1: 0.8085 - val_acc_2: 0.8054\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1346 - loss_2: 0.1219 - acc_ensemble: 0.9890 - acc_1: 0.9590 - acc_2: 0.9690 - val_loss_1: 0.6962 - val_loss_2: 0.7130 - val_acc_ensemble: 0.8431 - val_acc_1: 0.8049 - val_acc_2: 0.8086\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1175 - loss_2: 0.0917 - acc_ensemble: 0.9860 - acc_1: 0.9580 - acc_2: 0.9670 - val_loss_1: 0.7086 - val_loss_2: 0.7182 - val_acc_ensemble: 0.8433 - val_acc_1: 0.8078 - val_acc_2: 0.8042\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1384 - loss_2: 0.1173 - acc_ensemble: 0.9830 - acc_1: 0.9570 - acc_2: 0.9630 - val_loss_1: 0.7134 - val_loss_2: 0.7337 - val_acc_ensemble: 0.8388 - val_acc_1: 0.8072 - val_acc_2: 0.8041\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.1085 - loss_2: 0.1161 - acc_ensemble: 0.9890 - acc_1: 0.9570 - acc_2: 0.9700 - val_loss_1: 0.7173 - val_loss_2: 0.7112 - val_acc_ensemble: 0.8433 - val_acc_1: 0.8078 - val_acc_2: 0.8142\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.1107 - loss_2: 0.1262 - acc_ensemble: 0.9890 - acc_1: 0.9600 - acc_2: 0.9640 - val_loss_1: 0.7020 - val_loss_2: 0.7405 - val_acc_ensemble: 0.8421 - val_acc_1: 0.8098 - val_acc_2: 0.8033\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.0987 - loss_2: 0.0968 - acc_ensemble: 0.9880 - acc_1: 0.9550 - acc_2: 0.9700 - val_loss_1: 0.7214 - val_loss_2: 0.7541 - val_acc_ensemble: 0.8409 - val_acc_1: 0.8095 - val_acc_2: 0.8053\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.1115 - loss_2: 0.1232 - acc_ensemble: 0.9920 - acc_1: 0.9580 - acc_2: 0.9710 - val_loss_1: 0.7674 - val_loss_2: 0.7380 - val_acc_ensemble: 0.8398 - val_acc_1: 0.8004 - val_acc_2: 0.8072\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.25\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,8] [None,32,32,24]   1568       input              \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,8] [None,32,32,24]   112        conv2d_1_1         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,8] [None,32,32,24]   0          bn_1_1             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,8] [None,32,32,24]   17976      relu_1_1           \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,8] [None,32,32,24]   112        conv2d_1_2         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,8] [None,32,32,24]   0          bn_1_2             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,8] [None,16,16,24]   0          relu_1_2           \n",
      "                                    [None,16,16,8] [None,16,16,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,16] [None,16,16,48]  35952      avg_pool2d_1       \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,16] [None,16,16,48]  224        conv2d_2_1         \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,16] [None,16,16,48]  0          bn_2_1             \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,16] [None,16,16,48]  71664      relu_2_1           \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,16] [None,16,16,48]  224        conv2d_2_2         \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,16] [None,16,16,48]  0          bn_2_2             \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,16] [None,8,8,48]      0          relu_2_2           \n",
      "                                    [None,8,8,16] [None,8,8,48]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,32] [None,8,8,96]      143328     avg_pool2d_2       \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,32] [None,8,8,96]      448        conv2d_3_1         \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,32] [None,8,8,96]      0          bn_3_1             \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,32] [None,8,8,96]      286176     relu_3_1           \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,32] [None,8,8,96]      448        conv2d_3_2         \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,32] [None,8,8,96]      0          bn_3_2             \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,32] [None,96]              0          relu_3_2           \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,32] [None,96]              32224      global_avg_pool2d  \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,32] [None,96]              448        fc1                \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,32] [None,96]              0          bn_fc1             \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        2534       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 593438\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.7017 - loss_2: 1.6612 - acc_ensemble: 0.5230 - acc_1: 0.4820 - acc_2: 0.4760 - val_loss_1: 1.4214 - val_loss_2: 1.4015 - val_acc_ensemble: 0.5166 - val_acc_1: 0.4845 - val_acc_2: 0.4829\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.2973 - loss_2: 1.3128 - acc_ensemble: 0.6220 - acc_1: 0.5620 - acc_2: 0.5780 - val_loss_1: 1.2662 - val_loss_2: 1.2141 - val_acc_ensemble: 0.5924 - val_acc_1: 0.5355 - val_acc_2: 0.5581\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.1497 - loss_2: 1.1364 - acc_ensemble: 0.6840 - acc_1: 0.6240 - acc_2: 0.6550 - val_loss_1: 1.1093 - val_loss_2: 1.0738 - val_acc_ensemble: 0.6402 - val_acc_1: 0.6071 - val_acc_2: 0.6145\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 1.0210 - loss_2: 1.0072 - acc_ensemble: 0.7030 - acc_1: 0.6620 - acc_2: 0.6830 - val_loss_1: 1.0124 - val_loss_2: 1.0145 - val_acc_ensemble: 0.6731 - val_acc_1: 0.6405 - val_acc_2: 0.6360\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9543 - loss_2: 0.9057 - acc_ensemble: 0.7350 - acc_1: 0.6870 - acc_2: 0.7030 - val_loss_1: 0.9480 - val_loss_2: 0.9266 - val_acc_ensemble: 0.7037 - val_acc_1: 0.6608 - val_acc_2: 0.6660\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8762 - loss_2: 0.8467 - acc_ensemble: 0.7570 - acc_1: 0.7120 - acc_2: 0.7400 - val_loss_1: 0.9049 - val_loss_2: 0.8895 - val_acc_ensemble: 0.7166 - val_acc_1: 0.6760 - val_acc_2: 0.6863\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.8137 - loss_2: 0.7818 - acc_ensemble: 0.7800 - acc_1: 0.7320 - acc_2: 0.7710 - val_loss_1: 0.8536 - val_loss_2: 0.8405 - val_acc_ensemble: 0.7269 - val_acc_1: 0.6980 - val_acc_2: 0.6981\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7602 - loss_2: 0.7345 - acc_ensemble: 0.7910 - acc_1: 0.7320 - acc_2: 0.7620 - val_loss_1: 0.8352 - val_loss_2: 0.8019 - val_acc_ensemble: 0.7419 - val_acc_1: 0.7060 - val_acc_2: 0.7161\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.7146 - loss_2: 0.6907 - acc_ensemble: 0.8010 - acc_1: 0.7460 - acc_2: 0.7720 - val_loss_1: 0.7859 - val_loss_2: 0.7913 - val_acc_ensemble: 0.7541 - val_acc_1: 0.7251 - val_acc_2: 0.7204\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6833 - loss_2: 0.6748 - acc_ensemble: 0.8220 - acc_1: 0.7730 - acc_2: 0.7720 - val_loss_1: 0.7604 - val_loss_2: 0.7839 - val_acc_ensemble: 0.7604 - val_acc_1: 0.7344 - val_acc_2: 0.7253\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6243 - loss_2: 0.6311 - acc_ensemble: 0.8290 - acc_1: 0.7870 - acc_2: 0.7940 - val_loss_1: 0.7583 - val_loss_2: 0.7342 - val_acc_ensemble: 0.7658 - val_acc_1: 0.7372 - val_acc_2: 0.7438\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.6192 - loss_2: 0.6090 - acc_ensemble: 0.8510 - acc_1: 0.8140 - acc_2: 0.8300 - val_loss_1: 0.7038 - val_loss_2: 0.7033 - val_acc_ensemble: 0.7835 - val_acc_1: 0.7607 - val_acc_2: 0.7527\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5890 - loss_2: 0.5566 - acc_ensemble: 0.8590 - acc_1: 0.8180 - acc_2: 0.8260 - val_loss_1: 0.6970 - val_loss_2: 0.7096 - val_acc_ensemble: 0.7855 - val_acc_1: 0.7575 - val_acc_2: 0.7527\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5453 - loss_2: 0.5222 - acc_ensemble: 0.8700 - acc_1: 0.8340 - acc_2: 0.8480 - val_loss_1: 0.6935 - val_loss_2: 0.6726 - val_acc_ensemble: 0.7929 - val_acc_1: 0.7640 - val_acc_2: 0.7670\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.5222 - loss_2: 0.5016 - acc_ensemble: 0.8660 - acc_1: 0.8150 - acc_2: 0.8410 - val_loss_1: 0.6928 - val_loss_2: 0.6830 - val_acc_ensemble: 0.7970 - val_acc_1: 0.7605 - val_acc_2: 0.7651\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.5012 - loss_2: 0.4731 - acc_ensemble: 0.8800 - acc_1: 0.8430 - acc_2: 0.8510 - val_loss_1: 0.6622 - val_loss_2: 0.6435 - val_acc_ensemble: 0.8055 - val_acc_1: 0.7749 - val_acc_2: 0.7764\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4702 - loss_2: 0.4664 - acc_ensemble: 0.8820 - acc_1: 0.8370 - acc_2: 0.8490 - val_loss_1: 0.6647 - val_loss_2: 0.6728 - val_acc_ensemble: 0.8050 - val_acc_1: 0.7727 - val_acc_2: 0.7727\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4618 - loss_2: 0.4468 - acc_ensemble: 0.8940 - acc_1: 0.8480 - acc_2: 0.8740 - val_loss_1: 0.6553 - val_loss_2: 0.6493 - val_acc_ensemble: 0.8089 - val_acc_1: 0.7773 - val_acc_2: 0.7762\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.4254 - loss_2: 0.3903 - acc_ensemble: 0.9060 - acc_1: 0.8560 - acc_2: 0.8670 - val_loss_1: 0.6477 - val_loss_2: 0.6581 - val_acc_ensemble: 0.8115 - val_acc_1: 0.7840 - val_acc_2: 0.7765\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.4121 - loss_2: 0.4171 - acc_ensemble: 0.8950 - acc_1: 0.8710 - acc_2: 0.8620 - val_loss_1: 0.6363 - val_loss_2: 0.6618 - val_acc_ensemble: 0.8136 - val_acc_1: 0.7845 - val_acc_2: 0.7777\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3926 - loss_2: 0.3621 - acc_ensemble: 0.9020 - acc_1: 0.8590 - acc_2: 0.8800 - val_loss_1: 0.6691 - val_loss_2: 0.6500 - val_acc_ensemble: 0.8153 - val_acc_1: 0.7775 - val_acc_2: 0.7879\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3786 - loss_2: 0.3703 - acc_ensemble: 0.9260 - acc_1: 0.8740 - acc_2: 0.8930 - val_loss_1: 0.6434 - val_loss_2: 0.6366 - val_acc_ensemble: 0.8241 - val_acc_1: 0.7885 - val_acc_2: 0.7881\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3774 - loss_2: 0.3646 - acc_ensemble: 0.9080 - acc_1: 0.8710 - acc_2: 0.8900 - val_loss_1: 0.6301 - val_loss_2: 0.6434 - val_acc_ensemble: 0.8202 - val_acc_1: 0.7862 - val_acc_2: 0.7849\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3298 - loss_2: 0.3232 - acc_ensemble: 0.9280 - acc_1: 0.8980 - acc_2: 0.9050 - val_loss_1: 0.6429 - val_loss_2: 0.6320 - val_acc_ensemble: 0.8216 - val_acc_1: 0.7855 - val_acc_2: 0.7900\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3160 - loss_2: 0.3419 - acc_ensemble: 0.9260 - acc_1: 0.8760 - acc_2: 0.9020 - val_loss_1: 0.6219 - val_loss_2: 0.6299 - val_acc_ensemble: 0.8211 - val_acc_1: 0.7975 - val_acc_2: 0.7933\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.2937 - loss_2: 0.3298 - acc_ensemble: 0.9330 - acc_1: 0.9040 - acc_2: 0.9070 - val_loss_1: 0.6099 - val_loss_2: 0.6226 - val_acc_ensemble: 0.8293 - val_acc_1: 0.8002 - val_acc_2: 0.7985\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2847 - loss_2: 0.2872 - acc_ensemble: 0.9410 - acc_1: 0.8950 - acc_2: 0.9100 - val_loss_1: 0.6599 - val_loss_2: 0.6241 - val_acc_ensemble: 0.8244 - val_acc_1: 0.7876 - val_acc_2: 0.8024\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2918 - loss_2: 0.2967 - acc_ensemble: 0.9360 - acc_1: 0.9000 - acc_2: 0.8940 - val_loss_1: 0.6460 - val_loss_2: 0.6589 - val_acc_ensemble: 0.8227 - val_acc_1: 0.7929 - val_acc_2: 0.7845\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2874 - loss_2: 0.2759 - acc_ensemble: 0.9530 - acc_1: 0.9050 - acc_2: 0.9230 - val_loss_1: 0.6429 - val_loss_2: 0.6463 - val_acc_ensemble: 0.8269 - val_acc_1: 0.7947 - val_acc_2: 0.7956\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2583 - loss_2: 0.2345 - acc_ensemble: 0.9560 - acc_1: 0.9200 - acc_2: 0.9200 - val_loss_1: 0.6446 - val_loss_2: 0.6507 - val_acc_ensemble: 0.8295 - val_acc_1: 0.8007 - val_acc_2: 0.7932\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2563 - loss_2: 0.2592 - acc_ensemble: 0.9540 - acc_1: 0.9190 - acc_2: 0.9310 - val_loss_1: 0.6474 - val_loss_2: 0.6456 - val_acc_ensemble: 0.8308 - val_acc_1: 0.7923 - val_acc_2: 0.7977\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2282 - loss_2: 0.2203 - acc_ensemble: 0.9550 - acc_1: 0.9230 - acc_2: 0.9300 - val_loss_1: 0.6653 - val_loss_2: 0.6387 - val_acc_ensemble: 0.8314 - val_acc_1: 0.7956 - val_acc_2: 0.8041\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2106 - loss_2: 0.2299 - acc_ensemble: 0.9590 - acc_1: 0.9330 - acc_2: 0.9160 - val_loss_1: 0.6783 - val_loss_2: 0.6792 - val_acc_ensemble: 0.8274 - val_acc_1: 0.7979 - val_acc_2: 0.7893\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.2139 - loss_2: 0.2120 - acc_ensemble: 0.9610 - acc_1: 0.9320 - acc_2: 0.9320 - val_loss_1: 0.6464 - val_loss_2: 0.6604 - val_acc_ensemble: 0.8334 - val_acc_1: 0.8034 - val_acc_2: 0.7996\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.1886 - loss_2: 0.2029 - acc_ensemble: 0.9620 - acc_1: 0.9280 - acc_2: 0.9300 - val_loss_1: 0.6826 - val_loss_2: 0.6535 - val_acc_ensemble: 0.8350 - val_acc_1: 0.7949 - val_acc_2: 0.8059\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.2166 - loss_2: 0.1985 - acc_ensemble: 0.9570 - acc_1: 0.9060 - acc_2: 0.9400 - val_loss_1: 0.7200 - val_loss_2: 0.6379 - val_acc_ensemble: 0.8295 - val_acc_1: 0.7873 - val_acc_2: 0.8074\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.2046 - loss_2: 0.1651 - acc_ensemble: 0.9630 - acc_1: 0.9240 - acc_2: 0.9320 - val_loss_1: 0.7005 - val_loss_2: 0.6748 - val_acc_ensemble: 0.8346 - val_acc_1: 0.7919 - val_acc_2: 0.7979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1848 - loss_2: 0.1670 - acc_ensemble: 0.9650 - acc_1: 0.9370 - acc_2: 0.9380 - val_loss_1: 0.6919 - val_loss_2: 0.6661 - val_acc_ensemble: 0.8329 - val_acc_1: 0.7982 - val_acc_2: 0.8065\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1723 - loss_2: 0.1730 - acc_ensemble: 0.9680 - acc_1: 0.9470 - acc_2: 0.9390 - val_loss_1: 0.6936 - val_loss_2: 0.6791 - val_acc_ensemble: 0.8334 - val_acc_1: 0.7997 - val_acc_2: 0.8016\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1631 - loss_2: 0.1560 - acc_ensemble: 0.9710 - acc_1: 0.9500 - acc_2: 0.9410 - val_loss_1: 0.6890 - val_loss_2: 0.6959 - val_acc_ensemble: 0.8330 - val_acc_1: 0.7982 - val_acc_2: 0.8026\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1512 - loss_2: 0.1437 - acc_ensemble: 0.9840 - acc_1: 0.9510 - acc_2: 0.9530 - val_loss_1: 0.7017 - val_loss_2: 0.6897 - val_acc_ensemble: 0.8354 - val_acc_1: 0.7999 - val_acc_2: 0.8026\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1321 - loss_2: 0.1431 - acc_ensemble: 0.9800 - acc_1: 0.9510 - acc_2: 0.9500 - val_loss_1: 0.7182 - val_loss_2: 0.7270 - val_acc_ensemble: 0.8308 - val_acc_1: 0.8009 - val_acc_2: 0.7940\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1501 - loss_2: 0.1588 - acc_ensemble: 0.9730 - acc_1: 0.9490 - acc_2: 0.9370 - val_loss_1: 0.7297 - val_loss_2: 0.7394 - val_acc_ensemble: 0.8338 - val_acc_1: 0.7990 - val_acc_2: 0.7958\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1313 - loss_2: 0.1590 - acc_ensemble: 0.9740 - acc_1: 0.9460 - acc_2: 0.9420 - val_loss_1: 0.7178 - val_loss_2: 0.7141 - val_acc_ensemble: 0.8356 - val_acc_1: 0.8002 - val_acc_2: 0.8001\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1517 - loss_2: 0.1274 - acc_ensemble: 0.9840 - acc_1: 0.9550 - acc_2: 0.9490 - val_loss_1: 0.7207 - val_loss_2: 0.7143 - val_acc_ensemble: 0.8358 - val_acc_1: 0.7999 - val_acc_2: 0.8051\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1291 - loss_2: 0.1374 - acc_ensemble: 0.9840 - acc_1: 0.9550 - acc_2: 0.9620 - val_loss_1: 0.7027 - val_loss_2: 0.7176 - val_acc_ensemble: 0.8408 - val_acc_1: 0.8063 - val_acc_2: 0.8015\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.1187 - loss_2: 0.1027 - acc_ensemble: 0.9790 - acc_1: 0.9500 - acc_2: 0.9600 - val_loss_1: 0.7433 - val_loss_2: 0.7393 - val_acc_ensemble: 0.8367 - val_acc_1: 0.8033 - val_acc_2: 0.8040\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.1167 - loss_2: 0.1352 - acc_ensemble: 0.9830 - acc_1: 0.9580 - acc_2: 0.9480 - val_loss_1: 0.7406 - val_loss_2: 0.7525 - val_acc_ensemble: 0.8373 - val_acc_1: 0.7992 - val_acc_2: 0.7973\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.0977 - loss_2: 0.1047 - acc_ensemble: 0.9850 - acc_1: 0.9660 - acc_2: 0.9600 - val_loss_1: 0.7493 - val_loss_2: 0.7141 - val_acc_ensemble: 0.8391 - val_acc_1: 0.8015 - val_acc_2: 0.8085\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.0979 - loss_2: 0.0785 - acc_ensemble: 0.9890 - acc_1: 0.9390 - acc_2: 0.9630 - val_loss_1: 0.7886 - val_loss_2: 0.7380 - val_acc_ensemble: 0.8411 - val_acc_1: 0.7974 - val_acc_2: 0.8100\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.25\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,8] [None,32,32,24]   1568       input              \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,8] [None,32,32,24]   112        conv2d_1_1         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,8] [None,32,32,24]   0          bn_1_1             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,8] [None,32,32,24]   17976      relu_1_1           \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,8] [None,32,32,24]   112        conv2d_1_2         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,8] [None,32,32,24]   0          bn_1_2             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,8] [None,16,16,24]   0          relu_1_2           \n",
      "                                    [None,16,16,8] [None,16,16,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,16] [None,16,16,48]  35952      avg_pool2d_1       \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,16] [None,16,16,48]  224        conv2d_2_1         \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,16] [None,16,16,48]  0          bn_2_1             \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,16] [None,16,16,48]  71664      relu_2_1           \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,16] [None,16,16,48]  224        conv2d_2_2         \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,16] [None,16,16,48]  0          bn_2_2             \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,16] [None,8,8,48]      0          relu_2_2           \n",
      "                                    [None,8,8,16] [None,8,8,48]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,32] [None,8,8,96]      143328     avg_pool2d_2       \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,32] [None,8,8,96]      448        conv2d_3_1         \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,32] [None,8,8,96]      0          bn_3_1             \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,32] [None,8,8,96]      286176     relu_3_1           \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,32] [None,8,8,96]      448        conv2d_3_2         \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,32] [None,8,8,96]      0          bn_3_2             \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,32] [None,96]              0          relu_3_2           \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,32] [None,96]              32224      global_avg_pool2d  \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,32] [None,96]              448        fc1                \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,32] [None,96]              0          bn_fc1             \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        2534       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 593438\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.7072 - loss_2: 1.7008 - acc_ensemble: 0.5170 - acc_1: 0.4830 - acc_2: 0.4890 - val_loss_1: 1.4139 - val_loss_2: 1.4183 - val_acc_ensemble: 0.5088 - val_acc_1: 0.4807 - val_acc_2: 0.4768\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.3332 - loss_2: 1.2998 - acc_ensemble: 0.6060 - acc_1: 0.5720 - acc_2: 0.5810 - val_loss_1: 1.2102 - val_loss_2: 1.2169 - val_acc_ensemble: 0.5901 - val_acc_1: 0.5673 - val_acc_2: 0.5593\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.1297 - loss_2: 1.1378 - acc_ensemble: 0.6490 - acc_1: 0.6100 - acc_2: 0.6310 - val_loss_1: 1.1095 - val_loss_2: 1.0885 - val_acc_ensemble: 0.6308 - val_acc_1: 0.5945 - val_acc_2: 0.6106\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 1.0205 - loss_2: 1.0276 - acc_ensemble: 0.7030 - acc_1: 0.6490 - acc_2: 0.6890 - val_loss_1: 1.0113 - val_loss_2: 0.9787 - val_acc_ensemble: 0.6723 - val_acc_1: 0.6347 - val_acc_2: 0.6480\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9201 - loss_2: 0.9327 - acc_ensemble: 0.7220 - acc_1: 0.6790 - acc_2: 0.7110 - val_loss_1: 0.9540 - val_loss_2: 0.9291 - val_acc_ensemble: 0.6874 - val_acc_1: 0.6573 - val_acc_2: 0.6692\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8599 - loss_2: 0.8581 - acc_ensemble: 0.7610 - acc_1: 0.7090 - acc_2: 0.7380 - val_loss_1: 0.8987 - val_loss_2: 0.8930 - val_acc_ensemble: 0.7150 - val_acc_1: 0.6820 - val_acc_2: 0.6817\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.8031 - loss_2: 0.8221 - acc_ensemble: 0.7700 - acc_1: 0.7360 - acc_2: 0.7460 - val_loss_1: 0.8225 - val_loss_2: 0.8121 - val_acc_ensemble: 0.7331 - val_acc_1: 0.7088 - val_acc_2: 0.7120\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7352 - loss_2: 0.7497 - acc_ensemble: 0.7890 - acc_1: 0.7450 - acc_2: 0.7530 - val_loss_1: 0.8045 - val_loss_2: 0.8096 - val_acc_ensemble: 0.7455 - val_acc_1: 0.7163 - val_acc_2: 0.7164\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.7178 - loss_2: 0.7210 - acc_ensemble: 0.7980 - acc_1: 0.7600 - acc_2: 0.7810 - val_loss_1: 0.7829 - val_loss_2: 0.7626 - val_acc_ensemble: 0.7613 - val_acc_1: 0.7248 - val_acc_2: 0.7335\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6587 - loss_2: 0.6651 - acc_ensemble: 0.8210 - acc_1: 0.7830 - acc_2: 0.7750 - val_loss_1: 0.7335 - val_loss_2: 0.7582 - val_acc_ensemble: 0.7715 - val_acc_1: 0.7414 - val_acc_2: 0.7356\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6539 - loss_2: 0.6309 - acc_ensemble: 0.8330 - acc_1: 0.8030 - acc_2: 0.8090 - val_loss_1: 0.7205 - val_loss_2: 0.7190 - val_acc_ensemble: 0.7806 - val_acc_1: 0.7484 - val_acc_2: 0.7495\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.6081 - loss_2: 0.5857 - acc_ensemble: 0.8540 - acc_1: 0.8280 - acc_2: 0.8140 - val_loss_1: 0.6991 - val_loss_2: 0.7060 - val_acc_ensemble: 0.7835 - val_acc_1: 0.7520 - val_acc_2: 0.7553\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5799 - loss_2: 0.5698 - acc_ensemble: 0.8530 - acc_1: 0.8150 - acc_2: 0.8080 - val_loss_1: 0.6917 - val_loss_2: 0.7158 - val_acc_ensemble: 0.7890 - val_acc_1: 0.7644 - val_acc_2: 0.7501\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5426 - loss_2: 0.5239 - acc_ensemble: 0.8620 - acc_1: 0.8180 - acc_2: 0.8350 - val_loss_1: 0.6963 - val_loss_2: 0.6734 - val_acc_ensemble: 0.7951 - val_acc_1: 0.7626 - val_acc_2: 0.7692\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.4952 - loss_2: 0.4942 - acc_ensemble: 0.8710 - acc_1: 0.8370 - acc_2: 0.8370 - val_loss_1: 0.6842 - val_loss_2: 0.6816 - val_acc_ensemble: 0.7961 - val_acc_1: 0.7656 - val_acc_2: 0.7687\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.4961 - loss_2: 0.4997 - acc_ensemble: 0.8790 - acc_1: 0.8480 - acc_2: 0.8370 - val_loss_1: 0.6562 - val_loss_2: 0.6542 - val_acc_ensemble: 0.8044 - val_acc_1: 0.7744 - val_acc_2: 0.7764\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4481 - loss_2: 0.4751 - acc_ensemble: 0.8780 - acc_1: 0.8490 - acc_2: 0.8560 - val_loss_1: 0.6748 - val_loss_2: 0.6315 - val_acc_ensemble: 0.8076 - val_acc_1: 0.7726 - val_acc_2: 0.7840\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4485 - loss_2: 0.4451 - acc_ensemble: 0.8880 - acc_1: 0.8660 - acc_2: 0.8560 - val_loss_1: 0.6366 - val_loss_2: 0.6465 - val_acc_ensemble: 0.8160 - val_acc_1: 0.7877 - val_acc_2: 0.7805\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.4209 - loss_2: 0.4001 - acc_ensemble: 0.8910 - acc_1: 0.8530 - acc_2: 0.8540 - val_loss_1: 0.6420 - val_loss_2: 0.6347 - val_acc_ensemble: 0.8187 - val_acc_1: 0.7795 - val_acc_2: 0.7851\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.4024 - loss_2: 0.4058 - acc_ensemble: 0.9010 - acc_1: 0.8790 - acc_2: 0.8670 - val_loss_1: 0.6347 - val_loss_2: 0.6427 - val_acc_ensemble: 0.8204 - val_acc_1: 0.7896 - val_acc_2: 0.7851\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.4026 - loss_2: 0.3728 - acc_ensemble: 0.9080 - acc_1: 0.8790 - acc_2: 0.8840 - val_loss_1: 0.6285 - val_loss_2: 0.6338 - val_acc_ensemble: 0.8222 - val_acc_1: 0.7931 - val_acc_2: 0.7893\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3785 - loss_2: 0.3809 - acc_ensemble: 0.8940 - acc_1: 0.8660 - acc_2: 0.8640 - val_loss_1: 0.6473 - val_loss_2: 0.6418 - val_acc_ensemble: 0.8180 - val_acc_1: 0.7844 - val_acc_2: 0.7882\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3498 - loss_2: 0.3546 - acc_ensemble: 0.9140 - acc_1: 0.8970 - acc_2: 0.8960 - val_loss_1: 0.6124 - val_loss_2: 0.6275 - val_acc_ensemble: 0.8248 - val_acc_1: 0.7960 - val_acc_2: 0.7937\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3412 - loss_2: 0.3431 - acc_ensemble: 0.9320 - acc_1: 0.9070 - acc_2: 0.8980 - val_loss_1: 0.6222 - val_loss_2: 0.6349 - val_acc_ensemble: 0.8238 - val_acc_1: 0.7929 - val_acc_2: 0.7888\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3078 - loss_2: 0.3093 - acc_ensemble: 0.9350 - acc_1: 0.8990 - acc_2: 0.8850 - val_loss_1: 0.6412 - val_loss_2: 0.6427 - val_acc_ensemble: 0.8260 - val_acc_1: 0.7889 - val_acc_2: 0.7933\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.3258 - loss_2: 0.3163 - acc_ensemble: 0.9320 - acc_1: 0.9010 - acc_2: 0.9030 - val_loss_1: 0.6280 - val_loss_2: 0.6393 - val_acc_ensemble: 0.8322 - val_acc_1: 0.7970 - val_acc_2: 0.7930\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.3025 - loss_2: 0.2761 - acc_ensemble: 0.9410 - acc_1: 0.8970 - acc_2: 0.9030 - val_loss_1: 0.6360 - val_loss_2: 0.6316 - val_acc_ensemble: 0.8314 - val_acc_1: 0.7958 - val_acc_2: 0.7967\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2862 - loss_2: 0.2884 - acc_ensemble: 0.9310 - acc_1: 0.9080 - acc_2: 0.8970 - val_loss_1: 0.6541 - val_loss_2: 0.6631 - val_acc_ensemble: 0.8265 - val_acc_1: 0.7971 - val_acc_2: 0.7898\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2695 - loss_2: 0.2868 - acc_ensemble: 0.9460 - acc_1: 0.9150 - acc_2: 0.9300 - val_loss_1: 0.6361 - val_loss_2: 0.6255 - val_acc_ensemble: 0.8315 - val_acc_1: 0.7994 - val_acc_2: 0.7980\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2553 - loss_2: 0.2399 - acc_ensemble: 0.9570 - acc_1: 0.9290 - acc_2: 0.9200 - val_loss_1: 0.6198 - val_loss_2: 0.6489 - val_acc_ensemble: 0.8351 - val_acc_1: 0.8060 - val_acc_2: 0.7974\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2012 - loss_2: 0.2335 - acc_ensemble: 0.9570 - acc_1: 0.9220 - acc_2: 0.9230 - val_loss_1: 0.6552 - val_loss_2: 0.6399 - val_acc_ensemble: 0.8353 - val_acc_1: 0.8042 - val_acc_2: 0.8025\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2536 - loss_2: 0.2240 - acc_ensemble: 0.9560 - acc_1: 0.9280 - acc_2: 0.9230 - val_loss_1: 0.6530 - val_loss_2: 0.6655 - val_acc_ensemble: 0.8310 - val_acc_1: 0.7974 - val_acc_2: 0.7953\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2473 - loss_2: 0.2091 - acc_ensemble: 0.9630 - acc_1: 0.9260 - acc_2: 0.9200 - val_loss_1: 0.6219 - val_loss_2: 0.6663 - val_acc_ensemble: 0.8363 - val_acc_1: 0.8059 - val_acc_2: 0.8014\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.2043 - loss_2: 0.2054 - acc_ensemble: 0.9610 - acc_1: 0.9430 - acc_2: 0.9290 - val_loss_1: 0.6472 - val_loss_2: 0.6810 - val_acc_ensemble: 0.8339 - val_acc_1: 0.8021 - val_acc_2: 0.7950\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.2197 - loss_2: 0.1955 - acc_ensemble: 0.9630 - acc_1: 0.9220 - acc_2: 0.9360 - val_loss_1: 0.6638 - val_loss_2: 0.6730 - val_acc_ensemble: 0.8369 - val_acc_1: 0.8000 - val_acc_2: 0.8012\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.1845 - loss_2: 0.1875 - acc_ensemble: 0.9650 - acc_1: 0.9410 - acc_2: 0.9350 - val_loss_1: 0.6506 - val_loss_2: 0.6534 - val_acc_ensemble: 0.8437 - val_acc_1: 0.8068 - val_acc_2: 0.8025\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.1686 - loss_2: 0.1866 - acc_ensemble: 0.9690 - acc_1: 0.9420 - acc_2: 0.9340 - val_loss_1: 0.6739 - val_loss_2: 0.6533 - val_acc_ensemble: 0.8380 - val_acc_1: 0.8012 - val_acc_2: 0.8075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1677 - loss_2: 0.1624 - acc_ensemble: 0.9670 - acc_1: 0.9370 - acc_2: 0.9430 - val_loss_1: 0.6815 - val_loss_2: 0.6788 - val_acc_ensemble: 0.8365 - val_acc_1: 0.8025 - val_acc_2: 0.8045\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1833 - loss_2: 0.1710 - acc_ensemble: 0.9700 - acc_1: 0.9470 - acc_2: 0.9430 - val_loss_1: 0.6892 - val_loss_2: 0.6729 - val_acc_ensemble: 0.8364 - val_acc_1: 0.8037 - val_acc_2: 0.8040\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1550 - loss_2: 0.1695 - acc_ensemble: 0.9780 - acc_1: 0.9510 - acc_2: 0.9400 - val_loss_1: 0.6804 - val_loss_2: 0.6967 - val_acc_ensemble: 0.8374 - val_acc_1: 0.8094 - val_acc_2: 0.7998\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1741 - loss_2: 0.1614 - acc_ensemble: 0.9700 - acc_1: 0.9360 - acc_2: 0.9340 - val_loss_1: 0.7058 - val_loss_2: 0.6854 - val_acc_ensemble: 0.8384 - val_acc_1: 0.8001 - val_acc_2: 0.8021\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1771 - loss_2: 0.1455 - acc_ensemble: 0.9750 - acc_1: 0.9490 - acc_2: 0.9460 - val_loss_1: 0.6865 - val_loss_2: 0.6885 - val_acc_ensemble: 0.8404 - val_acc_1: 0.8025 - val_acc_2: 0.8043\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1393 - loss_2: 0.1385 - acc_ensemble: 0.9820 - acc_1: 0.9620 - acc_2: 0.9540 - val_loss_1: 0.6952 - val_loss_2: 0.6959 - val_acc_ensemble: 0.8407 - val_acc_1: 0.8039 - val_acc_2: 0.8066\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1172 - loss_2: 0.1323 - acc_ensemble: 0.9800 - acc_1: 0.9610 - acc_2: 0.9470 - val_loss_1: 0.6951 - val_loss_2: 0.7089 - val_acc_ensemble: 0.8376 - val_acc_1: 0.8071 - val_acc_2: 0.8047\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1173 - loss_2: 0.1356 - acc_ensemble: 0.9830 - acc_1: 0.9690 - acc_2: 0.9590 - val_loss_1: 0.6989 - val_loss_2: 0.7054 - val_acc_ensemble: 0.8405 - val_acc_1: 0.8097 - val_acc_2: 0.8079\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1034 - loss_2: 0.1265 - acc_ensemble: 0.9870 - acc_1: 0.9680 - acc_2: 0.9600 - val_loss_1: 0.7270 - val_loss_2: 0.7249 - val_acc_ensemble: 0.8417 - val_acc_1: 0.8051 - val_acc_2: 0.8064\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.1183 - loss_2: 0.1306 - acc_ensemble: 0.9780 - acc_1: 0.9540 - acc_2: 0.9480 - val_loss_1: 0.7544 - val_loss_2: 0.7164 - val_acc_ensemble: 0.8401 - val_acc_1: 0.8057 - val_acc_2: 0.8079\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.1329 - loss_2: 0.1454 - acc_ensemble: 0.9810 - acc_1: 0.9550 - acc_2: 0.9600 - val_loss_1: 0.7378 - val_loss_2: 0.7255 - val_acc_ensemble: 0.8394 - val_acc_1: 0.8036 - val_acc_2: 0.8064\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.1401 - loss_2: 0.1110 - acc_ensemble: 0.9810 - acc_1: 0.9550 - acc_2: 0.9660 - val_loss_1: 0.7508 - val_loss_2: 0.7148 - val_acc_ensemble: 0.8434 - val_acc_1: 0.8019 - val_acc_2: 0.8086\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.1196 - loss_2: 0.1041 - acc_ensemble: 0.9850 - acc_1: 0.9660 - acc_2: 0.9510 - val_loss_1: 0.7506 - val_loss_2: 0.7486 - val_acc_ensemble: 0.8410 - val_acc_1: 0.8025 - val_acc_2: 0.8049\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.25\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,8] [None,32,32,24]   1568       input              \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,8] [None,32,32,24]   112        conv2d_1_1         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,8] [None,32,32,24]   0          bn_1_1             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,8] [None,32,32,24]   17976      relu_1_1           \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,8] [None,32,32,24]   112        conv2d_1_2         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,8] [None,32,32,24]   0          bn_1_2             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,8] [None,16,16,24]   0          relu_1_2           \n",
      "                                    [None,16,16,8] [None,16,16,24]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,16] [None,16,16,48]  35952      avg_pool2d_1       \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,16] [None,16,16,48]  224        conv2d_2_1         \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,16] [None,16,16,48]  0          bn_2_1             \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,16] [None,16,16,48]  71664      relu_2_1           \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,16] [None,16,16,48]  224        conv2d_2_2         \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,16] [None,16,16,48]  0          bn_2_2             \n",
      "                                    [None,16,16,16] [None,16,16,48]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,16] [None,8,8,48]      0          relu_2_2           \n",
      "                                    [None,8,8,16] [None,8,8,48]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,32] [None,8,8,96]      143328     avg_pool2d_2       \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,32] [None,8,8,96]      448        conv2d_3_1         \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,32] [None,8,8,96]      0          bn_3_1             \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,32] [None,8,8,96]      286176     relu_3_1           \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,32] [None,8,8,96]      448        conv2d_3_2         \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,32] [None,8,8,96]      0          bn_3_2             \n",
      "                                    [None,8,8,32] [None,8,8,96]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,32] [None,96]              0          relu_3_2           \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,32] [None,96]              32224      global_avg_pool2d  \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,32] [None,96]              448        fc1                \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,32] [None,96]              0          bn_fc1             \n",
      "                                    [None,32] [None,96]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        2534       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 593438\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.6671 - loss_2: 1.6946 - acc_ensemble: 0.5360 - acc_1: 0.5060 - acc_2: 0.5050 - val_loss_1: 1.3614 - val_loss_2: 1.4166 - val_acc_ensemble: 0.5098 - val_acc_1: 0.4941 - val_acc_2: 0.4768\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.2500 - loss_2: 1.3221 - acc_ensemble: 0.6290 - acc_1: 0.5930 - acc_2: 0.5780 - val_loss_1: 1.1713 - val_loss_2: 1.1916 - val_acc_ensemble: 0.6115 - val_acc_1: 0.5745 - val_acc_2: 0.5775\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.0886 - loss_2: 1.1472 - acc_ensemble: 0.6540 - acc_1: 0.6350 - acc_2: 0.6110 - val_loss_1: 1.0562 - val_loss_2: 1.1228 - val_acc_ensemble: 0.6544 - val_acc_1: 0.6255 - val_acc_2: 0.5926\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 0.9894 - loss_2: 1.0144 - acc_ensemble: 0.7130 - acc_1: 0.6430 - acc_2: 0.6930 - val_loss_1: 1.0130 - val_loss_2: 0.9806 - val_acc_ensemble: 0.6798 - val_acc_1: 0.6392 - val_acc_2: 0.6509\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9145 - loss_2: 0.9367 - acc_ensemble: 0.7360 - acc_1: 0.6800 - acc_2: 0.7050 - val_loss_1: 0.9526 - val_loss_2: 0.9332 - val_acc_ensemble: 0.6977 - val_acc_1: 0.6658 - val_acc_2: 0.6681\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8332 - loss_2: 0.8768 - acc_ensemble: 0.7660 - acc_1: 0.7230 - acc_2: 0.7220 - val_loss_1: 0.8505 - val_loss_2: 0.8796 - val_acc_ensemble: 0.7235 - val_acc_1: 0.6982 - val_acc_2: 0.6847\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.7776 - loss_2: 0.7881 - acc_ensemble: 0.7730 - acc_1: 0.7340 - acc_2: 0.7210 - val_loss_1: 0.8223 - val_loss_2: 0.8599 - val_acc_ensemble: 0.7347 - val_acc_1: 0.7089 - val_acc_2: 0.6991\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7187 - loss_2: 0.7659 - acc_ensemble: 0.7830 - acc_1: 0.7360 - acc_2: 0.7700 - val_loss_1: 0.8023 - val_loss_2: 0.8018 - val_acc_ensemble: 0.7503 - val_acc_1: 0.7187 - val_acc_2: 0.7184\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.6967 - loss_2: 0.6870 - acc_ensemble: 0.8080 - acc_1: 0.7830 - acc_2: 0.7590 - val_loss_1: 0.7492 - val_loss_2: 0.7811 - val_acc_ensemble: 0.7563 - val_acc_1: 0.7357 - val_acc_2: 0.7262\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6511 - loss_2: 0.6548 - acc_ensemble: 0.8200 - acc_1: 0.7850 - acc_2: 0.7760 - val_loss_1: 0.7385 - val_loss_2: 0.7711 - val_acc_ensemble: 0.7653 - val_acc_1: 0.7392 - val_acc_2: 0.7259\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6128 - loss_2: 0.6359 - acc_ensemble: 0.8330 - acc_1: 0.8120 - acc_2: 0.7900 - val_loss_1: 0.7183 - val_loss_2: 0.7541 - val_acc_ensemble: 0.7722 - val_acc_1: 0.7440 - val_acc_2: 0.7345\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.5770 - loss_2: 0.5882 - acc_ensemble: 0.8270 - acc_1: 0.8040 - acc_2: 0.7970 - val_loss_1: 0.7138 - val_loss_2: 0.7066 - val_acc_ensemble: 0.7823 - val_acc_1: 0.7492 - val_acc_2: 0.7521\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5523 - loss_2: 0.5644 - acc_ensemble: 0.8420 - acc_1: 0.8160 - acc_2: 0.8070 - val_loss_1: 0.7070 - val_loss_2: 0.7236 - val_acc_ensemble: 0.7845 - val_acc_1: 0.7547 - val_acc_2: 0.7476\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5077 - loss_2: 0.5435 - acc_ensemble: 0.8600 - acc_1: 0.8310 - acc_2: 0.8250 - val_loss_1: 0.6742 - val_loss_2: 0.6835 - val_acc_ensemble: 0.7934 - val_acc_1: 0.7684 - val_acc_2: 0.7621\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.4925 - loss_2: 0.5156 - acc_ensemble: 0.8610 - acc_1: 0.8200 - acc_2: 0.8290 - val_loss_1: 0.6741 - val_loss_2: 0.6706 - val_acc_ensemble: 0.8022 - val_acc_1: 0.7650 - val_acc_2: 0.7692\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.4827 - loss_2: 0.4799 - acc_ensemble: 0.8730 - acc_1: 0.8390 - acc_2: 0.8120 - val_loss_1: 0.6779 - val_loss_2: 0.6753 - val_acc_ensemble: 0.7980 - val_acc_1: 0.7649 - val_acc_2: 0.7619\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4510 - loss_2: 0.4507 - acc_ensemble: 0.8740 - acc_1: 0.8350 - acc_2: 0.8400 - val_loss_1: 0.6526 - val_loss_2: 0.6482 - val_acc_ensemble: 0.8063 - val_acc_1: 0.7725 - val_acc_2: 0.7802\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4249 - loss_2: 0.4366 - acc_ensemble: 0.8960 - acc_1: 0.8610 - acc_2: 0.8610 - val_loss_1: 0.6362 - val_loss_2: 0.6547 - val_acc_ensemble: 0.8082 - val_acc_1: 0.7846 - val_acc_2: 0.7735\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.4018 - loss_2: 0.4389 - acc_ensemble: 0.9030 - acc_1: 0.8730 - acc_2: 0.8600 - val_loss_1: 0.6163 - val_loss_2: 0.6386 - val_acc_ensemble: 0.8159 - val_acc_1: 0.7895 - val_acc_2: 0.7853\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.4044 - loss_2: 0.3953 - acc_ensemble: 0.9090 - acc_1: 0.8720 - acc_2: 0.8690 - val_loss_1: 0.6247 - val_loss_2: 0.6391 - val_acc_ensemble: 0.8143 - val_acc_1: 0.7856 - val_acc_2: 0.7827\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3681 - loss_2: 0.3690 - acc_ensemble: 0.9190 - acc_1: 0.8830 - acc_2: 0.8780 - val_loss_1: 0.6288 - val_loss_2: 0.6347 - val_acc_ensemble: 0.8204 - val_acc_1: 0.7902 - val_acc_2: 0.7886\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3299 - loss_2: 0.3517 - acc_ensemble: 0.9190 - acc_1: 0.8960 - acc_2: 0.8650 - val_loss_1: 0.6253 - val_loss_2: 0.6361 - val_acc_ensemble: 0.8237 - val_acc_1: 0.7920 - val_acc_2: 0.7881\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3548 - loss_2: 0.3616 - acc_ensemble: 0.9160 - acc_1: 0.8790 - acc_2: 0.8900 - val_loss_1: 0.6176 - val_loss_2: 0.6397 - val_acc_ensemble: 0.8226 - val_acc_1: 0.7933 - val_acc_2: 0.7835\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3421 - loss_2: 0.2984 - acc_ensemble: 0.9140 - acc_1: 0.8870 - acc_2: 0.8820 - val_loss_1: 0.6557 - val_loss_2: 0.6448 - val_acc_ensemble: 0.8274 - val_acc_1: 0.7866 - val_acc_2: 0.7928\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3263 - loss_2: 0.3286 - acc_ensemble: 0.9290 - acc_1: 0.8940 - acc_2: 0.9030 - val_loss_1: 0.6252 - val_loss_2: 0.6517 - val_acc_ensemble: 0.8254 - val_acc_1: 0.7936 - val_acc_2: 0.7866\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.3157 - loss_2: 0.3034 - acc_ensemble: 0.9310 - acc_1: 0.8990 - acc_2: 0.8970 - val_loss_1: 0.6169 - val_loss_2: 0.6363 - val_acc_ensemble: 0.8290 - val_acc_1: 0.7981 - val_acc_2: 0.7959\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2913 - loss_2: 0.3011 - acc_ensemble: 0.9310 - acc_1: 0.9050 - acc_2: 0.8900 - val_loss_1: 0.6022 - val_loss_2: 0.6505 - val_acc_ensemble: 0.8305 - val_acc_1: 0.8062 - val_acc_2: 0.7933\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2516 - loss_2: 0.2748 - acc_ensemble: 0.9310 - acc_1: 0.9090 - acc_2: 0.8880 - val_loss_1: 0.6099 - val_loss_2: 0.6375 - val_acc_ensemble: 0.8315 - val_acc_1: 0.8022 - val_acc_2: 0.7934\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2819 - loss_2: 0.2630 - acc_ensemble: 0.9410 - acc_1: 0.9080 - acc_2: 0.8970 - val_loss_1: 0.6147 - val_loss_2: 0.6592 - val_acc_ensemble: 0.8310 - val_acc_1: 0.8012 - val_acc_2: 0.7931\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2656 - loss_2: 0.2564 - acc_ensemble: 0.9420 - acc_1: 0.9120 - acc_2: 0.9090 - val_loss_1: 0.6178 - val_loss_2: 0.6388 - val_acc_ensemble: 0.8330 - val_acc_1: 0.8035 - val_acc_2: 0.7998\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2321 - loss_2: 0.2514 - acc_ensemble: 0.9510 - acc_1: 0.9220 - acc_2: 0.9240 - val_loss_1: 0.6378 - val_loss_2: 0.6540 - val_acc_ensemble: 0.8322 - val_acc_1: 0.7987 - val_acc_2: 0.7951\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2282 - loss_2: 0.2236 - acc_ensemble: 0.9530 - acc_1: 0.9180 - acc_2: 0.9200 - val_loss_1: 0.6177 - val_loss_2: 0.6563 - val_acc_ensemble: 0.8372 - val_acc_1: 0.8098 - val_acc_2: 0.7964\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2135 - loss_2: 0.2206 - acc_ensemble: 0.9590 - acc_1: 0.9220 - acc_2: 0.9320 - val_loss_1: 0.6370 - val_loss_2: 0.6665 - val_acc_ensemble: 0.8323 - val_acc_1: 0.8061 - val_acc_2: 0.7924\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.2064 - loss_2: 0.2148 - acc_ensemble: 0.9620 - acc_1: 0.9430 - acc_2: 0.9230 - val_loss_1: 0.6342 - val_loss_2: 0.6668 - val_acc_ensemble: 0.8347 - val_acc_1: 0.8102 - val_acc_2: 0.7971\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.2112 - loss_2: 0.1787 - acc_ensemble: 0.9700 - acc_1: 0.9320 - acc_2: 0.9390 - val_loss_1: 0.6382 - val_loss_2: 0.6711 - val_acc_ensemble: 0.8323 - val_acc_1: 0.8096 - val_acc_2: 0.7993\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.2078 - loss_2: 0.1908 - acc_ensemble: 0.9620 - acc_1: 0.9460 - acc_2: 0.9330 - val_loss_1: 0.6270 - val_loss_2: 0.6745 - val_acc_ensemble: 0.8394 - val_acc_1: 0.8058 - val_acc_2: 0.8000\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.1732 - loss_2: 0.1730 - acc_ensemble: 0.9560 - acc_1: 0.9290 - acc_2: 0.9190 - val_loss_1: 0.6785 - val_loss_2: 0.6710 - val_acc_ensemble: 0.8395 - val_acc_1: 0.8006 - val_acc_2: 0.8051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1765 - loss_2: 0.1567 - acc_ensemble: 0.9740 - acc_1: 0.9470 - acc_2: 0.9370 - val_loss_1: 0.6418 - val_loss_2: 0.6882 - val_acc_ensemble: 0.8402 - val_acc_1: 0.8126 - val_acc_2: 0.8017\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1547 - loss_2: 0.1807 - acc_ensemble: 0.9680 - acc_1: 0.9470 - acc_2: 0.9300 - val_loss_1: 0.6557 - val_loss_2: 0.6958 - val_acc_ensemble: 0.8372 - val_acc_1: 0.8085 - val_acc_2: 0.8012\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1647 - loss_2: 0.1616 - acc_ensemble: 0.9830 - acc_1: 0.9450 - acc_2: 0.9470 - val_loss_1: 0.6703 - val_loss_2: 0.6945 - val_acc_ensemble: 0.8383 - val_acc_1: 0.8105 - val_acc_2: 0.8014\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1458 - loss_2: 0.1550 - acc_ensemble: 0.9710 - acc_1: 0.9460 - acc_2: 0.9470 - val_loss_1: 0.6668 - val_loss_2: 0.7043 - val_acc_ensemble: 0.8386 - val_acc_1: 0.8085 - val_acc_2: 0.7995\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1490 - loss_2: 0.1467 - acc_ensemble: 0.9770 - acc_1: 0.9460 - acc_2: 0.9520 - val_loss_1: 0.6692 - val_loss_2: 0.6858 - val_acc_ensemble: 0.8414 - val_acc_1: 0.8129 - val_acc_2: 0.8058\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1407 - loss_2: 0.1155 - acc_ensemble: 0.9740 - acc_1: 0.9500 - acc_2: 0.9560 - val_loss_1: 0.6771 - val_loss_2: 0.6991 - val_acc_ensemble: 0.8370 - val_acc_1: 0.8070 - val_acc_2: 0.8063\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1359 - loss_2: 0.1230 - acc_ensemble: 0.9810 - acc_1: 0.9470 - acc_2: 0.9600 - val_loss_1: 0.6942 - val_loss_2: 0.7098 - val_acc_ensemble: 0.8393 - val_acc_1: 0.8084 - val_acc_2: 0.8052\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1218 - loss_2: 0.1258 - acc_ensemble: 0.9820 - acc_1: 0.9640 - acc_2: 0.9560 - val_loss_1: 0.7156 - val_loss_2: 0.7140 - val_acc_ensemble: 0.8410 - val_acc_1: 0.8071 - val_acc_2: 0.8073\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1224 - loss_2: 0.1267 - acc_ensemble: 0.9810 - acc_1: 0.9470 - acc_2: 0.9630 - val_loss_1: 0.6903 - val_loss_2: 0.7173 - val_acc_ensemble: 0.8433 - val_acc_1: 0.8101 - val_acc_2: 0.8059\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.1085 - loss_2: 0.0969 - acc_ensemble: 0.9850 - acc_1: 0.9610 - acc_2: 0.9670 - val_loss_1: 0.6975 - val_loss_2: 0.7519 - val_acc_ensemble: 0.8416 - val_acc_1: 0.8104 - val_acc_2: 0.8018\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.1298 - loss_2: 0.1127 - acc_ensemble: 0.9850 - acc_1: 0.9650 - acc_2: 0.9650 - val_loss_1: 0.7307 - val_loss_2: 0.7503 - val_acc_ensemble: 0.8322 - val_acc_1: 0.8020 - val_acc_2: 0.8021\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.1348 - loss_2: 0.1261 - acc_ensemble: 0.9840 - acc_1: 0.9630 - acc_2: 0.9580 - val_loss_1: 0.7170 - val_loss_2: 0.7490 - val_acc_ensemble: 0.8402 - val_acc_1: 0.8074 - val_acc_2: 0.8026\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.0964 - loss_2: 0.1114 - acc_ensemble: 0.9830 - acc_1: 0.9680 - acc_2: 0.9550 - val_loss_1: 0.7014 - val_loss_2: 0.7394 - val_acc_ensemble: 0.8454 - val_acc_1: 0.8163 - val_acc_2: 0.8086\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.50\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,16] [None,32,32,16]  1344       input              \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_1         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_1             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,16] [None,32,32,16]  16240      relu_1_1           \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_2         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_2             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,16] [None,16,16,16]  0          relu_1_2           \n",
      "                                    [None,16,16,16] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,32] [None,16,16,32]  32480      avg_pool2d_1       \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,32] [None,16,16,32]  192        conv2d_2_1         \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,32] [None,16,16,32]  0          bn_2_1             \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,32] [None,16,16,32]  64736      relu_2_1           \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,32] [None,16,16,32]  192        conv2d_2_2         \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,32] [None,16,16,32]  0          bn_2_2             \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,32] [None,8,8,32]      0          relu_2_2           \n",
      "                                    [None,8,8,32] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,64] [None,8,8,64]      129472     avg_pool2d_2       \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,64] [None,8,8,64]      384        conv2d_3_1         \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,64] [None,8,8,64]      0          bn_3_1             \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,64] [None,8,8,64]      258496     relu_3_1           \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,64] [None,8,8,64]      384        conv2d_3_2         \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,64] [None,8,8,64]      0          bn_3_2             \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,64] [None,64]              0          relu_3_2           \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,64] [None,64]              29120      global_avg_pool2d  \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,64] [None,64]              384        fc1                \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,64] [None,64]              0          bn_fc1             \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        2275       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 535891\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 17s - loss_1: 1.7071 - loss_2: 1.6415 - acc_ensemble: 0.5340 - acc_1: 0.4870 - acc_2: 0.5260 - val_loss_1: 1.4338 - val_loss_2: 1.3615 - val_acc_ensemble: 0.5148 - val_acc_1: 0.4690 - val_acc_2: 0.4982\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.3259 - loss_2: 1.2802 - acc_ensemble: 0.6130 - acc_1: 0.5750 - acc_2: 0.5720 - val_loss_1: 1.2113 - val_loss_2: 1.2256 - val_acc_ensemble: 0.5960 - val_acc_1: 0.5675 - val_acc_2: 0.5567\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.1307 - loss_2: 1.1054 - acc_ensemble: 0.6850 - acc_1: 0.6530 - acc_2: 0.6430 - val_loss_1: 1.0495 - val_loss_2: 1.0794 - val_acc_ensemble: 0.6502 - val_acc_1: 0.6262 - val_acc_2: 0.6144\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 1.0174 - loss_2: 1.0104 - acc_ensemble: 0.7100 - acc_1: 0.6820 - acc_2: 0.6870 - val_loss_1: 0.9908 - val_loss_2: 0.9694 - val_acc_ensemble: 0.6705 - val_acc_1: 0.6428 - val_acc_2: 0.6527\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9241 - loss_2: 0.9182 - acc_ensemble: 0.7250 - acc_1: 0.6780 - acc_2: 0.7090 - val_loss_1: 0.9089 - val_loss_2: 0.9229 - val_acc_ensemble: 0.7032 - val_acc_1: 0.6772 - val_acc_2: 0.6728\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8471 - loss_2: 0.8677 - acc_ensemble: 0.7400 - acc_1: 0.7150 - acc_2: 0.7210 - val_loss_1: 0.8787 - val_loss_2: 0.9044 - val_acc_ensemble: 0.7120 - val_acc_1: 0.6869 - val_acc_2: 0.6793\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.7956 - loss_2: 0.8104 - acc_ensemble: 0.7590 - acc_1: 0.7260 - acc_2: 0.7300 - val_loss_1: 0.8259 - val_loss_2: 0.8351 - val_acc_ensemble: 0.7333 - val_acc_1: 0.7094 - val_acc_2: 0.7014\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7440 - loss_2: 0.7436 - acc_ensemble: 0.8080 - acc_1: 0.7520 - acc_2: 0.7720 - val_loss_1: 0.8037 - val_loss_2: 0.7876 - val_acc_ensemble: 0.7441 - val_acc_1: 0.7181 - val_acc_2: 0.7162\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.7262 - loss_2: 0.7104 - acc_ensemble: 0.7890 - acc_1: 0.7630 - acc_2: 0.7560 - val_loss_1: 0.7825 - val_loss_2: 0.7776 - val_acc_ensemble: 0.7493 - val_acc_1: 0.7204 - val_acc_2: 0.7245\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6721 - loss_2: 0.6705 - acc_ensemble: 0.8060 - acc_1: 0.7590 - acc_2: 0.7870 - val_loss_1: 0.7505 - val_loss_2: 0.7300 - val_acc_ensemble: 0.7672 - val_acc_1: 0.7354 - val_acc_2: 0.7455\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6219 - loss_2: 0.6176 - acc_ensemble: 0.8180 - acc_1: 0.7730 - acc_2: 0.7860 - val_loss_1: 0.7210 - val_loss_2: 0.7409 - val_acc_ensemble: 0.7679 - val_acc_1: 0.7474 - val_acc_2: 0.7377\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.6038 - loss_2: 0.5930 - acc_ensemble: 0.8290 - acc_1: 0.7900 - acc_2: 0.7970 - val_loss_1: 0.7339 - val_loss_2: 0.7071 - val_acc_ensemble: 0.7799 - val_acc_1: 0.7434 - val_acc_2: 0.7525\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5621 - loss_2: 0.5701 - acc_ensemble: 0.8380 - acc_1: 0.8060 - acc_2: 0.8100 - val_loss_1: 0.7202 - val_loss_2: 0.6840 - val_acc_ensemble: 0.7820 - val_acc_1: 0.7476 - val_acc_2: 0.7635\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5452 - loss_2: 0.5288 - acc_ensemble: 0.8590 - acc_1: 0.8280 - acc_2: 0.8240 - val_loss_1: 0.6627 - val_loss_2: 0.6859 - val_acc_ensemble: 0.7936 - val_acc_1: 0.7686 - val_acc_2: 0.7620\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.5276 - loss_2: 0.5189 - acc_ensemble: 0.8600 - acc_1: 0.8200 - acc_2: 0.8310 - val_loss_1: 0.6724 - val_loss_2: 0.6751 - val_acc_ensemble: 0.7938 - val_acc_1: 0.7669 - val_acc_2: 0.7689\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.5183 - loss_2: 0.4870 - acc_ensemble: 0.8700 - acc_1: 0.8400 - acc_2: 0.8380 - val_loss_1: 0.6355 - val_loss_2: 0.6613 - val_acc_ensemble: 0.8048 - val_acc_1: 0.7823 - val_acc_2: 0.7733\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4797 - loss_2: 0.4615 - acc_ensemble: 0.8820 - acc_1: 0.8500 - acc_2: 0.8510 - val_loss_1: 0.6475 - val_loss_2: 0.6768 - val_acc_ensemble: 0.8033 - val_acc_1: 0.7745 - val_acc_2: 0.7673\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4273 - loss_2: 0.4565 - acc_ensemble: 0.8710 - acc_1: 0.8300 - acc_2: 0.8470 - val_loss_1: 0.6488 - val_loss_2: 0.6591 - val_acc_ensemble: 0.8074 - val_acc_1: 0.7767 - val_acc_2: 0.7711\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.4337 - loss_2: 0.4460 - acc_ensemble: 0.8950 - acc_1: 0.8470 - acc_2: 0.8720 - val_loss_1: 0.6440 - val_loss_2: 0.6502 - val_acc_ensemble: 0.8135 - val_acc_1: 0.7836 - val_acc_2: 0.7825\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.4185 - loss_2: 0.3907 - acc_ensemble: 0.9020 - acc_1: 0.8590 - acc_2: 0.8820 - val_loss_1: 0.6384 - val_loss_2: 0.6482 - val_acc_ensemble: 0.8145 - val_acc_1: 0.7822 - val_acc_2: 0.7859\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.4039 - loss_2: 0.3450 - acc_ensemble: 0.9110 - acc_1: 0.8650 - acc_2: 0.8750 - val_loss_1: 0.6372 - val_loss_2: 0.6416 - val_acc_ensemble: 0.8172 - val_acc_1: 0.7864 - val_acc_2: 0.7831\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3892 - loss_2: 0.3679 - acc_ensemble: 0.9050 - acc_1: 0.8770 - acc_2: 0.8820 - val_loss_1: 0.6549 - val_loss_2: 0.6671 - val_acc_ensemble: 0.8121 - val_acc_1: 0.7826 - val_acc_2: 0.7825\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3519 - loss_2: 0.3788 - acc_ensemble: 0.9190 - acc_1: 0.8850 - acc_2: 0.8930 - val_loss_1: 0.6396 - val_loss_2: 0.6372 - val_acc_ensemble: 0.8190 - val_acc_1: 0.7853 - val_acc_2: 0.7864\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3383 - loss_2: 0.3413 - acc_ensemble: 0.9250 - acc_1: 0.8780 - acc_2: 0.9020 - val_loss_1: 0.6322 - val_loss_2: 0.6181 - val_acc_ensemble: 0.8246 - val_acc_1: 0.7926 - val_acc_2: 0.7928\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3176 - loss_2: 0.3392 - acc_ensemble: 0.9110 - acc_1: 0.8920 - acc_2: 0.8830 - val_loss_1: 0.6395 - val_loss_2: 0.6497 - val_acc_ensemble: 0.8216 - val_acc_1: 0.7927 - val_acc_2: 0.7866\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.3226 - loss_2: 0.2985 - acc_ensemble: 0.9350 - acc_1: 0.9050 - acc_2: 0.9090 - val_loss_1: 0.6341 - val_loss_2: 0.6270 - val_acc_ensemble: 0.8236 - val_acc_1: 0.7917 - val_acc_2: 0.7949\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.3083 - loss_2: 0.3007 - acc_ensemble: 0.9450 - acc_1: 0.8970 - acc_2: 0.9090 - val_loss_1: 0.6501 - val_loss_2: 0.6472 - val_acc_ensemble: 0.8216 - val_acc_1: 0.7881 - val_acc_2: 0.7899\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2610 - loss_2: 0.2993 - acc_ensemble: 0.9400 - acc_1: 0.8970 - acc_2: 0.9090 - val_loss_1: 0.6499 - val_loss_2: 0.6471 - val_acc_ensemble: 0.8259 - val_acc_1: 0.7908 - val_acc_2: 0.7963\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2765 - loss_2: 0.2802 - acc_ensemble: 0.9440 - acc_1: 0.9100 - acc_2: 0.9170 - val_loss_1: 0.6375 - val_loss_2: 0.6375 - val_acc_ensemble: 0.8257 - val_acc_1: 0.8000 - val_acc_2: 0.7978\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.3067 - loss_2: 0.2719 - acc_ensemble: 0.9560 - acc_1: 0.9090 - acc_2: 0.9380 - val_loss_1: 0.6433 - val_loss_2: 0.6345 - val_acc_ensemble: 0.8284 - val_acc_1: 0.7957 - val_acc_2: 0.7989\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2488 - loss_2: 0.2275 - acc_ensemble: 0.9500 - acc_1: 0.9150 - acc_2: 0.9110 - val_loss_1: 0.6497 - val_loss_2: 0.6584 - val_acc_ensemble: 0.8276 - val_acc_1: 0.7913 - val_acc_2: 0.7963\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2432 - loss_2: 0.2289 - acc_ensemble: 0.9610 - acc_1: 0.9130 - acc_2: 0.9330 - val_loss_1: 0.6549 - val_loss_2: 0.6490 - val_acc_ensemble: 0.8328 - val_acc_1: 0.7980 - val_acc_2: 0.8006\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2228 - loss_2: 0.2065 - acc_ensemble: 0.9560 - acc_1: 0.9230 - acc_2: 0.9300 - val_loss_1: 0.6721 - val_loss_2: 0.6667 - val_acc_ensemble: 0.8275 - val_acc_1: 0.7884 - val_acc_2: 0.7974\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.2027 - loss_2: 0.2119 - acc_ensemble: 0.9560 - acc_1: 0.9170 - acc_2: 0.9280 - val_loss_1: 0.6924 - val_loss_2: 0.6681 - val_acc_ensemble: 0.8275 - val_acc_1: 0.7925 - val_acc_2: 0.7976\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.2374 - loss_2: 0.2144 - acc_ensemble: 0.9650 - acc_1: 0.9320 - acc_2: 0.9310 - val_loss_1: 0.6556 - val_loss_2: 0.6626 - val_acc_ensemble: 0.8325 - val_acc_1: 0.7994 - val_acc_2: 0.7993\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.1901 - loss_2: 0.1860 - acc_ensemble: 0.9710 - acc_1: 0.9460 - acc_2: 0.9360 - val_loss_1: 0.6732 - val_loss_2: 0.6604 - val_acc_ensemble: 0.8344 - val_acc_1: 0.8022 - val_acc_2: 0.8023\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.2080 - loss_2: 0.1754 - acc_ensemble: 0.9680 - acc_1: 0.9290 - acc_2: 0.9410 - val_loss_1: 0.6702 - val_loss_2: 0.6832 - val_acc_ensemble: 0.8305 - val_acc_1: 0.7972 - val_acc_2: 0.7966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1749 - loss_2: 0.1801 - acc_ensemble: 0.9760 - acc_1: 0.9400 - acc_2: 0.9450 - val_loss_1: 0.6713 - val_loss_2: 0.6774 - val_acc_ensemble: 0.8377 - val_acc_1: 0.8029 - val_acc_2: 0.8046\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1816 - loss_2: 0.1770 - acc_ensemble: 0.9760 - acc_1: 0.9360 - acc_2: 0.9530 - val_loss_1: 0.6870 - val_loss_2: 0.6845 - val_acc_ensemble: 0.8339 - val_acc_1: 0.7964 - val_acc_2: 0.8002\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1855 - loss_2: 0.1670 - acc_ensemble: 0.9780 - acc_1: 0.9400 - acc_2: 0.9530 - val_loss_1: 0.6831 - val_loss_2: 0.6789 - val_acc_ensemble: 0.8369 - val_acc_1: 0.8043 - val_acc_2: 0.8046\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1552 - loss_2: 0.1632 - acc_ensemble: 0.9730 - acc_1: 0.9450 - acc_2: 0.9430 - val_loss_1: 0.7183 - val_loss_2: 0.7127 - val_acc_ensemble: 0.8296 - val_acc_1: 0.7933 - val_acc_2: 0.7954\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1474 - loss_2: 0.1555 - acc_ensemble: 0.9830 - acc_1: 0.9480 - acc_2: 0.9470 - val_loss_1: 0.7076 - val_loss_2: 0.6982 - val_acc_ensemble: 0.8334 - val_acc_1: 0.8009 - val_acc_2: 0.8016\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1379 - loss_2: 0.1422 - acc_ensemble: 0.9810 - acc_1: 0.9540 - acc_2: 0.9620 - val_loss_1: 0.6975 - val_loss_2: 0.7002 - val_acc_ensemble: 0.8366 - val_acc_1: 0.8030 - val_acc_2: 0.8072\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1234 - loss_2: 0.1145 - acc_ensemble: 0.9850 - acc_1: 0.9590 - acc_2: 0.9470 - val_loss_1: 0.7176 - val_loss_2: 0.7347 - val_acc_ensemble: 0.8367 - val_acc_1: 0.8015 - val_acc_2: 0.7968\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1403 - loss_2: 0.1274 - acc_ensemble: 0.9730 - acc_1: 0.9450 - acc_2: 0.9420 - val_loss_1: 0.7413 - val_loss_2: 0.7629 - val_acc_ensemble: 0.8333 - val_acc_1: 0.7974 - val_acc_2: 0.7954\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1348 - loss_2: 0.1249 - acc_ensemble: 0.9810 - acc_1: 0.9470 - acc_2: 0.9550 - val_loss_1: 0.7243 - val_loss_2: 0.7386 - val_acc_ensemble: 0.8379 - val_acc_1: 0.8030 - val_acc_2: 0.7977\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.1322 - loss_2: 0.1353 - acc_ensemble: 0.9840 - acc_1: 0.9520 - acc_2: 0.9530 - val_loss_1: 0.7358 - val_loss_2: 0.7524 - val_acc_ensemble: 0.8340 - val_acc_1: 0.7967 - val_acc_2: 0.8010\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.1317 - loss_2: 0.1315 - acc_ensemble: 0.9880 - acc_1: 0.9650 - acc_2: 0.9520 - val_loss_1: 0.7520 - val_loss_2: 0.7372 - val_acc_ensemble: 0.8354 - val_acc_1: 0.8019 - val_acc_2: 0.8053\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.1232 - loss_2: 0.1139 - acc_ensemble: 0.9820 - acc_1: 0.9470 - acc_2: 0.9630 - val_loss_1: 0.7562 - val_loss_2: 0.7524 - val_acc_ensemble: 0.8331 - val_acc_1: 0.8018 - val_acc_2: 0.8032\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.1227 - loss_2: 0.1091 - acc_ensemble: 0.9850 - acc_1: 0.9410 - acc_2: 0.9610 - val_loss_1: 0.7877 - val_loss_2: 0.7583 - val_acc_ensemble: 0.8361 - val_acc_1: 0.7963 - val_acc_2: 0.8029\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.50\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,16] [None,32,32,16]  1344       input              \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_1         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_1             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,16] [None,32,32,16]  16240      relu_1_1           \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_2         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_2             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,16] [None,16,16,16]  0          relu_1_2           \n",
      "                                    [None,16,16,16] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,32] [None,16,16,32]  32480      avg_pool2d_1       \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,32] [None,16,16,32]  192        conv2d_2_1         \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,32] [None,16,16,32]  0          bn_2_1             \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,32] [None,16,16,32]  64736      relu_2_1           \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,32] [None,16,16,32]  192        conv2d_2_2         \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,32] [None,16,16,32]  0          bn_2_2             \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,32] [None,8,8,32]      0          relu_2_2           \n",
      "                                    [None,8,8,32] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,64] [None,8,8,64]      129472     avg_pool2d_2       \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,64] [None,8,8,64]      384        conv2d_3_1         \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,64] [None,8,8,64]      0          bn_3_1             \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,64] [None,8,8,64]      258496     relu_3_1           \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,64] [None,8,8,64]      384        conv2d_3_2         \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,64] [None,8,8,64]      0          bn_3_2             \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,64] [None,64]              0          relu_3_2           \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,64] [None,64]              29120      global_avg_pool2d  \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,64] [None,64]              384        fc1                \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,64] [None,64]              0          bn_fc1             \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        2275       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 535891\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.6534 - loss_2: 1.6840 - acc_ensemble: 0.5200 - acc_1: 0.5080 - acc_2: 0.4850 - val_loss_1: 1.4098 - val_loss_2: 1.4476 - val_acc_ensemble: 0.5040 - val_acc_1: 0.4827 - val_acc_2: 0.4707\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.3076 - loss_2: 1.2995 - acc_ensemble: 0.6250 - acc_1: 0.6050 - acc_2: 0.5970 - val_loss_1: 1.1968 - val_loss_2: 1.2341 - val_acc_ensemble: 0.5924 - val_acc_1: 0.5659 - val_acc_2: 0.5569\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.1271 - loss_2: 1.1508 - acc_ensemble: 0.6600 - acc_1: 0.6320 - acc_2: 0.6330 - val_loss_1: 1.0859 - val_loss_2: 1.1192 - val_acc_ensemble: 0.6359 - val_acc_1: 0.6123 - val_acc_2: 0.5990\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 1.0018 - loss_2: 1.0298 - acc_ensemble: 0.7070 - acc_1: 0.6820 - acc_2: 0.6510 - val_loss_1: 1.0090 - val_loss_2: 1.0403 - val_acc_ensemble: 0.6667 - val_acc_1: 0.6420 - val_acc_2: 0.6298\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9406 - loss_2: 0.9276 - acc_ensemble: 0.7350 - acc_1: 0.6960 - acc_2: 0.7050 - val_loss_1: 0.9301 - val_loss_2: 0.9558 - val_acc_ensemble: 0.6925 - val_acc_1: 0.6710 - val_acc_2: 0.6577\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8684 - loss_2: 0.8844 - acc_ensemble: 0.7380 - acc_1: 0.6950 - acc_2: 0.7060 - val_loss_1: 0.9005 - val_loss_2: 0.8996 - val_acc_ensemble: 0.7133 - val_acc_1: 0.6811 - val_acc_2: 0.6818\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.8139 - loss_2: 0.8125 - acc_ensemble: 0.7670 - acc_1: 0.7570 - acc_2: 0.7380 - val_loss_1: 0.8321 - val_loss_2: 0.8646 - val_acc_ensemble: 0.7275 - val_acc_1: 0.7047 - val_acc_2: 0.6938\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7569 - loss_2: 0.7380 - acc_ensemble: 0.7810 - acc_1: 0.7520 - acc_2: 0.7510 - val_loss_1: 0.8017 - val_loss_2: 0.8305 - val_acc_ensemble: 0.7425 - val_acc_1: 0.7218 - val_acc_2: 0.7071\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.7117 - loss_2: 0.7155 - acc_ensemble: 0.8060 - acc_1: 0.7670 - acc_2: 0.7760 - val_loss_1: 0.7851 - val_loss_2: 0.7964 - val_acc_ensemble: 0.7487 - val_acc_1: 0.7264 - val_acc_2: 0.7189\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6840 - loss_2: 0.6826 - acc_ensemble: 0.8100 - acc_1: 0.7810 - acc_2: 0.7560 - val_loss_1: 0.7581 - val_loss_2: 0.7968 - val_acc_ensemble: 0.7540 - val_acc_1: 0.7351 - val_acc_2: 0.7187\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6394 - loss_2: 0.6380 - acc_ensemble: 0.8210 - acc_1: 0.7780 - acc_2: 0.7860 - val_loss_1: 0.7474 - val_loss_2: 0.7507 - val_acc_ensemble: 0.7666 - val_acc_1: 0.7414 - val_acc_2: 0.7416\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.6098 - loss_2: 0.6062 - acc_ensemble: 0.8500 - acc_1: 0.8150 - acc_2: 0.7970 - val_loss_1: 0.7153 - val_loss_2: 0.7330 - val_acc_ensemble: 0.7787 - val_acc_1: 0.7510 - val_acc_2: 0.7439\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5640 - loss_2: 0.5846 - acc_ensemble: 0.8370 - acc_1: 0.8070 - acc_2: 0.8160 - val_loss_1: 0.6826 - val_loss_2: 0.7101 - val_acc_ensemble: 0.7855 - val_acc_1: 0.7646 - val_acc_2: 0.7570\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5367 - loss_2: 0.5587 - acc_ensemble: 0.8520 - acc_1: 0.8400 - acc_2: 0.8190 - val_loss_1: 0.6811 - val_loss_2: 0.7047 - val_acc_ensemble: 0.7875 - val_acc_1: 0.7654 - val_acc_2: 0.7550\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.5154 - loss_2: 0.5257 - acc_ensemble: 0.8580 - acc_1: 0.8320 - acc_2: 0.8130 - val_loss_1: 0.6767 - val_loss_2: 0.7108 - val_acc_ensemble: 0.7914 - val_acc_1: 0.7704 - val_acc_2: 0.7484\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.5082 - loss_2: 0.5103 - acc_ensemble: 0.8720 - acc_1: 0.8500 - acc_2: 0.8270 - val_loss_1: 0.6570 - val_loss_2: 0.6755 - val_acc_ensemble: 0.8001 - val_acc_1: 0.7748 - val_acc_2: 0.7657\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4596 - loss_2: 0.4789 - acc_ensemble: 0.8860 - acc_1: 0.8510 - acc_2: 0.8520 - val_loss_1: 0.6587 - val_loss_2: 0.6828 - val_acc_ensemble: 0.7999 - val_acc_1: 0.7753 - val_acc_2: 0.7663\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4774 - loss_2: 0.4838 - acc_ensemble: 0.8910 - acc_1: 0.8620 - acc_2: 0.8520 - val_loss_1: 0.6618 - val_loss_2: 0.6584 - val_acc_ensemble: 0.8085 - val_acc_1: 0.7720 - val_acc_2: 0.7757\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.4229 - loss_2: 0.4500 - acc_ensemble: 0.8950 - acc_1: 0.8550 - acc_2: 0.8580 - val_loss_1: 0.6513 - val_loss_2: 0.6481 - val_acc_ensemble: 0.8092 - val_acc_1: 0.7793 - val_acc_2: 0.7796\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.4036 - loss_2: 0.4400 - acc_ensemble: 0.8950 - acc_1: 0.8770 - acc_2: 0.8580 - val_loss_1: 0.6502 - val_loss_2: 0.6522 - val_acc_ensemble: 0.8075 - val_acc_1: 0.7824 - val_acc_2: 0.7772\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3938 - loss_2: 0.4107 - acc_ensemble: 0.9020 - acc_1: 0.8810 - acc_2: 0.8750 - val_loss_1: 0.6508 - val_loss_2: 0.6417 - val_acc_ensemble: 0.8125 - val_acc_1: 0.7865 - val_acc_2: 0.7805\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3554 - loss_2: 0.3770 - acc_ensemble: 0.9200 - acc_1: 0.8810 - acc_2: 0.8860 - val_loss_1: 0.6449 - val_loss_2: 0.6270 - val_acc_ensemble: 0.8148 - val_acc_1: 0.7876 - val_acc_2: 0.7823\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3595 - loss_2: 0.3501 - acc_ensemble: 0.9230 - acc_1: 0.8860 - acc_2: 0.8900 - val_loss_1: 0.6742 - val_loss_2: 0.6413 - val_acc_ensemble: 0.8177 - val_acc_1: 0.7754 - val_acc_2: 0.7825\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3399 - loss_2: 0.3528 - acc_ensemble: 0.9180 - acc_1: 0.8870 - acc_2: 0.8900 - val_loss_1: 0.6545 - val_loss_2: 0.6382 - val_acc_ensemble: 0.8170 - val_acc_1: 0.7858 - val_acc_2: 0.7838\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3093 - loss_2: 0.3448 - acc_ensemble: 0.9310 - acc_1: 0.9110 - acc_2: 0.8850 - val_loss_1: 0.6197 - val_loss_2: 0.6688 - val_acc_ensemble: 0.8209 - val_acc_1: 0.7975 - val_acc_2: 0.7800\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.3027 - loss_2: 0.3271 - acc_ensemble: 0.9390 - acc_1: 0.9140 - acc_2: 0.8960 - val_loss_1: 0.6481 - val_loss_2: 0.6350 - val_acc_ensemble: 0.8208 - val_acc_1: 0.7883 - val_acc_2: 0.7885\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2907 - loss_2: 0.2970 - acc_ensemble: 0.9290 - acc_1: 0.8990 - acc_2: 0.8890 - val_loss_1: 0.6584 - val_loss_2: 0.6453 - val_acc_ensemble: 0.8221 - val_acc_1: 0.7913 - val_acc_2: 0.7885\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2988 - loss_2: 0.2920 - acc_ensemble: 0.9330 - acc_1: 0.9000 - acc_2: 0.9050 - val_loss_1: 0.6547 - val_loss_2: 0.6356 - val_acc_ensemble: 0.8240 - val_acc_1: 0.7902 - val_acc_2: 0.7969\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2399 - loss_2: 0.2951 - acc_ensemble: 0.9340 - acc_1: 0.9020 - acc_2: 0.9190 - val_loss_1: 0.6790 - val_loss_2: 0.6391 - val_acc_ensemble: 0.8250 - val_acc_1: 0.7864 - val_acc_2: 0.7939\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2739 - loss_2: 0.2552 - acc_ensemble: 0.9460 - acc_1: 0.9100 - acc_2: 0.9190 - val_loss_1: 0.6472 - val_loss_2: 0.6545 - val_acc_ensemble: 0.8291 - val_acc_1: 0.7980 - val_acc_2: 0.7913\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2539 - loss_2: 0.2395 - acc_ensemble: 0.9590 - acc_1: 0.9200 - acc_2: 0.9290 - val_loss_1: 0.6622 - val_loss_2: 0.6405 - val_acc_ensemble: 0.8311 - val_acc_1: 0.7943 - val_acc_2: 0.7929\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2130 - loss_2: 0.2505 - acc_ensemble: 0.9620 - acc_1: 0.9280 - acc_2: 0.9230 - val_loss_1: 0.6616 - val_loss_2: 0.6573 - val_acc_ensemble: 0.8275 - val_acc_1: 0.7995 - val_acc_2: 0.7969\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2065 - loss_2: 0.2420 - acc_ensemble: 0.9610 - acc_1: 0.9260 - acc_2: 0.9200 - val_loss_1: 0.6751 - val_loss_2: 0.6496 - val_acc_ensemble: 0.8302 - val_acc_1: 0.7961 - val_acc_2: 0.7947\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.2171 - loss_2: 0.2468 - acc_ensemble: 0.9550 - acc_1: 0.9200 - acc_2: 0.9100 - val_loss_1: 0.6788 - val_loss_2: 0.6742 - val_acc_ensemble: 0.8284 - val_acc_1: 0.7932 - val_acc_2: 0.7912\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.2104 - loss_2: 0.2361 - acc_ensemble: 0.9580 - acc_1: 0.9240 - acc_2: 0.9280 - val_loss_1: 0.6706 - val_loss_2: 0.6405 - val_acc_ensemble: 0.8332 - val_acc_1: 0.8000 - val_acc_2: 0.7971\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.2109 - loss_2: 0.2468 - acc_ensemble: 0.9630 - acc_1: 0.9430 - acc_2: 0.9190 - val_loss_1: 0.6549 - val_loss_2: 0.6687 - val_acc_ensemble: 0.8332 - val_acc_1: 0.7995 - val_acc_2: 0.7915\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.1669 - loss_2: 0.1954 - acc_ensemble: 0.9640 - acc_1: 0.9470 - acc_2: 0.9400 - val_loss_1: 0.6744 - val_loss_2: 0.6477 - val_acc_ensemble: 0.8365 - val_acc_1: 0.8021 - val_acc_2: 0.8025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1868 - loss_2: 0.1766 - acc_ensemble: 0.9630 - acc_1: 0.9400 - acc_2: 0.9410 - val_loss_1: 0.6976 - val_loss_2: 0.6813 - val_acc_ensemble: 0.8307 - val_acc_1: 0.7964 - val_acc_2: 0.7911\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1610 - loss_2: 0.1757 - acc_ensemble: 0.9740 - acc_1: 0.9480 - acc_2: 0.9450 - val_loss_1: 0.6965 - val_loss_2: 0.6731 - val_acc_ensemble: 0.8334 - val_acc_1: 0.7970 - val_acc_2: 0.7960\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1685 - loss_2: 0.1698 - acc_ensemble: 0.9790 - acc_1: 0.9450 - acc_2: 0.9410 - val_loss_1: 0.7359 - val_loss_2: 0.6850 - val_acc_ensemble: 0.8273 - val_acc_1: 0.7920 - val_acc_2: 0.7972\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1451 - loss_2: 0.1664 - acc_ensemble: 0.9740 - acc_1: 0.9440 - acc_2: 0.9450 - val_loss_1: 0.7105 - val_loss_2: 0.7167 - val_acc_ensemble: 0.8296 - val_acc_1: 0.8008 - val_acc_2: 0.7926\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1594 - loss_2: 0.1699 - acc_ensemble: 0.9830 - acc_1: 0.9500 - acc_2: 0.9350 - val_loss_1: 0.7209 - val_loss_2: 0.6943 - val_acc_ensemble: 0.8347 - val_acc_1: 0.8008 - val_acc_2: 0.7983\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1391 - loss_2: 0.1536 - acc_ensemble: 0.9840 - acc_1: 0.9540 - acc_2: 0.9470 - val_loss_1: 0.7030 - val_loss_2: 0.7215 - val_acc_ensemble: 0.8380 - val_acc_1: 0.8056 - val_acc_2: 0.7935\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1221 - loss_2: 0.1446 - acc_ensemble: 0.9780 - acc_1: 0.9640 - acc_2: 0.9560 - val_loss_1: 0.7271 - val_loss_2: 0.7181 - val_acc_ensemble: 0.8364 - val_acc_1: 0.7993 - val_acc_2: 0.7937\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1254 - loss_2: 0.1521 - acc_ensemble: 0.9740 - acc_1: 0.9540 - acc_2: 0.9550 - val_loss_1: 0.7368 - val_loss_2: 0.6986 - val_acc_ensemble: 0.8365 - val_acc_1: 0.8023 - val_acc_2: 0.7972\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1236 - loss_2: 0.1372 - acc_ensemble: 0.9780 - acc_1: 0.9500 - acc_2: 0.9460 - val_loss_1: 0.7594 - val_loss_2: 0.7428 - val_acc_ensemble: 0.8326 - val_acc_1: 0.7972 - val_acc_2: 0.7958\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.0964 - loss_2: 0.1259 - acc_ensemble: 0.9850 - acc_1: 0.9750 - acc_2: 0.9530 - val_loss_1: 0.7427 - val_loss_2: 0.7231 - val_acc_ensemble: 0.8369 - val_acc_1: 0.8053 - val_acc_2: 0.7996\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.1214 - loss_2: 0.1189 - acc_ensemble: 0.9860 - acc_1: 0.9680 - acc_2: 0.9550 - val_loss_1: 0.7445 - val_loss_2: 0.7333 - val_acc_ensemble: 0.8352 - val_acc_1: 0.7990 - val_acc_2: 0.7999\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.1141 - loss_2: 0.1256 - acc_ensemble: 0.9840 - acc_1: 0.9550 - acc_2: 0.9640 - val_loss_1: 0.7732 - val_loss_2: 0.7507 - val_acc_ensemble: 0.8336 - val_acc_1: 0.8014 - val_acc_2: 0.7975\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.1280 - loss_2: 0.1019 - acc_ensemble: 0.9820 - acc_1: 0.9500 - acc_2: 0.9650 - val_loss_1: 0.8010 - val_loss_2: 0.7577 - val_acc_ensemble: 0.8333 - val_acc_1: 0.7983 - val_acc_2: 0.7990\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.50\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,16] [None,32,32,16]  1344       input              \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_1         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_1             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,16] [None,32,32,16]  16240      relu_1_1           \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_2         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_2             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,16] [None,16,16,16]  0          relu_1_2           \n",
      "                                    [None,16,16,16] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,32] [None,16,16,32]  32480      avg_pool2d_1       \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,32] [None,16,16,32]  192        conv2d_2_1         \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,32] [None,16,16,32]  0          bn_2_1             \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,32] [None,16,16,32]  64736      relu_2_1           \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,32] [None,16,16,32]  192        conv2d_2_2         \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,32] [None,16,16,32]  0          bn_2_2             \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,32] [None,8,8,32]      0          relu_2_2           \n",
      "                                    [None,8,8,32] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,64] [None,8,8,64]      129472     avg_pool2d_2       \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,64] [None,8,8,64]      384        conv2d_3_1         \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,64] [None,8,8,64]      0          bn_3_1             \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,64] [None,8,8,64]      258496     relu_3_1           \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,64] [None,8,8,64]      384        conv2d_3_2         \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,64] [None,8,8,64]      0          bn_3_2             \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,64] [None,64]              0          relu_3_2           \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,64] [None,64]              29120      global_avg_pool2d  \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,64] [None,64]              384        fc1                \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,64] [None,64]              0          bn_fc1             \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        2275       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 535891\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.6494 - loss_2: 1.7368 - acc_ensemble: 0.5270 - acc_1: 0.4950 - acc_2: 0.5020 - val_loss_1: 1.4033 - val_loss_2: 1.4244 - val_acc_ensemble: 0.5117 - val_acc_1: 0.4839 - val_acc_2: 0.4774\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.2717 - loss_2: 1.3268 - acc_ensemble: 0.6070 - acc_1: 0.5820 - acc_2: 0.5830 - val_loss_1: 1.1908 - val_loss_2: 1.2278 - val_acc_ensemble: 0.5928 - val_acc_1: 0.5663 - val_acc_2: 0.5575\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.1157 - loss_2: 1.1284 - acc_ensemble: 0.6390 - acc_1: 0.6270 - acc_2: 0.5940 - val_loss_1: 1.0867 - val_loss_2: 1.1349 - val_acc_ensemble: 0.6266 - val_acc_1: 0.6112 - val_acc_2: 0.5865\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 0.9890 - loss_2: 1.0184 - acc_ensemble: 0.7020 - acc_1: 0.6650 - acc_2: 0.6460 - val_loss_1: 0.9893 - val_loss_2: 1.0167 - val_acc_ensemble: 0.6734 - val_acc_1: 0.6449 - val_acc_2: 0.6355\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9088 - loss_2: 0.9463 - acc_ensemble: 0.7370 - acc_1: 0.7050 - acc_2: 0.6980 - val_loss_1: 0.9070 - val_loss_2: 0.9320 - val_acc_ensemble: 0.7002 - val_acc_1: 0.6775 - val_acc_2: 0.6658\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8317 - loss_2: 0.8637 - acc_ensemble: 0.7620 - acc_1: 0.7380 - acc_2: 0.7220 - val_loss_1: 0.8444 - val_loss_2: 0.8828 - val_acc_ensemble: 0.7235 - val_acc_1: 0.6993 - val_acc_2: 0.6876\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.7639 - loss_2: 0.8245 - acc_ensemble: 0.7720 - acc_1: 0.7580 - acc_2: 0.7190 - val_loss_1: 0.8139 - val_loss_2: 0.8514 - val_acc_ensemble: 0.7390 - val_acc_1: 0.7150 - val_acc_2: 0.7006\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7294 - loss_2: 0.7474 - acc_ensemble: 0.7860 - acc_1: 0.7720 - acc_2: 0.7220 - val_loss_1: 0.7898 - val_loss_2: 0.8287 - val_acc_ensemble: 0.7457 - val_acc_1: 0.7198 - val_acc_2: 0.7059\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.6999 - loss_2: 0.7103 - acc_ensemble: 0.8080 - acc_1: 0.7610 - acc_2: 0.7490 - val_loss_1: 0.7624 - val_loss_2: 0.7745 - val_acc_ensemble: 0.7601 - val_acc_1: 0.7343 - val_acc_2: 0.7286\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6478 - loss_2: 0.6570 - acc_ensemble: 0.8150 - acc_1: 0.7880 - acc_2: 0.7770 - val_loss_1: 0.7517 - val_loss_2: 0.7639 - val_acc_ensemble: 0.7638 - val_acc_1: 0.7357 - val_acc_2: 0.7356\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6196 - loss_2: 0.6009 - acc_ensemble: 0.8270 - acc_1: 0.7980 - acc_2: 0.7820 - val_loss_1: 0.7262 - val_loss_2: 0.7271 - val_acc_ensemble: 0.7778 - val_acc_1: 0.7474 - val_acc_2: 0.7496\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.5845 - loss_2: 0.5972 - acc_ensemble: 0.8450 - acc_1: 0.8100 - acc_2: 0.7940 - val_loss_1: 0.7222 - val_loss_2: 0.7319 - val_acc_ensemble: 0.7763 - val_acc_1: 0.7493 - val_acc_2: 0.7461\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5462 - loss_2: 0.5666 - acc_ensemble: 0.8650 - acc_1: 0.8410 - acc_2: 0.8180 - val_loss_1: 0.6892 - val_loss_2: 0.7066 - val_acc_ensemble: 0.7843 - val_acc_1: 0.7619 - val_acc_2: 0.7548\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5122 - loss_2: 0.5203 - acc_ensemble: 0.8580 - acc_1: 0.8240 - acc_2: 0.8130 - val_loss_1: 0.7004 - val_loss_2: 0.6911 - val_acc_ensemble: 0.7889 - val_acc_1: 0.7556 - val_acc_2: 0.7604\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.4935 - loss_2: 0.4993 - acc_ensemble: 0.8820 - acc_1: 0.8390 - acc_2: 0.8230 - val_loss_1: 0.6849 - val_loss_2: 0.6887 - val_acc_ensemble: 0.8018 - val_acc_1: 0.7638 - val_acc_2: 0.7660\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.4638 - loss_2: 0.5053 - acc_ensemble: 0.8760 - acc_1: 0.8470 - acc_2: 0.8490 - val_loss_1: 0.6758 - val_loss_2: 0.6923 - val_acc_ensemble: 0.7971 - val_acc_1: 0.7700 - val_acc_2: 0.7624\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4481 - loss_2: 0.4857 - acc_ensemble: 0.8800 - acc_1: 0.8500 - acc_2: 0.8400 - val_loss_1: 0.6557 - val_loss_2: 0.6740 - val_acc_ensemble: 0.8015 - val_acc_1: 0.7782 - val_acc_2: 0.7688\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4216 - loss_2: 0.4493 - acc_ensemble: 0.8970 - acc_1: 0.8670 - acc_2: 0.8510 - val_loss_1: 0.6476 - val_loss_2: 0.6605 - val_acc_ensemble: 0.8070 - val_acc_1: 0.7772 - val_acc_2: 0.7752\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.4242 - loss_2: 0.4225 - acc_ensemble: 0.8970 - acc_1: 0.8750 - acc_2: 0.8620 - val_loss_1: 0.6427 - val_loss_2: 0.6806 - val_acc_ensemble: 0.8085 - val_acc_1: 0.7823 - val_acc_2: 0.7733\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.4014 - loss_2: 0.4046 - acc_ensemble: 0.9080 - acc_1: 0.8900 - acc_2: 0.8700 - val_loss_1: 0.6362 - val_loss_2: 0.6456 - val_acc_ensemble: 0.8155 - val_acc_1: 0.7858 - val_acc_2: 0.7830\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3626 - loss_2: 0.3784 - acc_ensemble: 0.9140 - acc_1: 0.8810 - acc_2: 0.8780 - val_loss_1: 0.6567 - val_loss_2: 0.6439 - val_acc_ensemble: 0.8152 - val_acc_1: 0.7774 - val_acc_2: 0.7862\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3782 - loss_2: 0.3578 - acc_ensemble: 0.9220 - acc_1: 0.8740 - acc_2: 0.8770 - val_loss_1: 0.6551 - val_loss_2: 0.6592 - val_acc_ensemble: 0.8167 - val_acc_1: 0.7794 - val_acc_2: 0.7827\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3212 - loss_2: 0.3472 - acc_ensemble: 0.9150 - acc_1: 0.9050 - acc_2: 0.8830 - val_loss_1: 0.6586 - val_loss_2: 0.6619 - val_acc_ensemble: 0.8136 - val_acc_1: 0.7859 - val_acc_2: 0.7796\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3408 - loss_2: 0.3427 - acc_ensemble: 0.9190 - acc_1: 0.8800 - acc_2: 0.8790 - val_loss_1: 0.6487 - val_loss_2: 0.6595 - val_acc_ensemble: 0.8185 - val_acc_1: 0.7830 - val_acc_2: 0.7832\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3291 - loss_2: 0.3209 - acc_ensemble: 0.9250 - acc_1: 0.8770 - acc_2: 0.8910 - val_loss_1: 0.6719 - val_loss_2: 0.6484 - val_acc_ensemble: 0.8208 - val_acc_1: 0.7803 - val_acc_2: 0.7919\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.2697 - loss_2: 0.3214 - acc_ensemble: 0.9290 - acc_1: 0.9050 - acc_2: 0.8980 - val_loss_1: 0.6409 - val_loss_2: 0.6422 - val_acc_ensemble: 0.8238 - val_acc_1: 0.7953 - val_acc_2: 0.7945\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2737 - loss_2: 0.3127 - acc_ensemble: 0.9350 - acc_1: 0.8980 - acc_2: 0.8920 - val_loss_1: 0.6617 - val_loss_2: 0.6634 - val_acc_ensemble: 0.8190 - val_acc_1: 0.7847 - val_acc_2: 0.7914\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2742 - loss_2: 0.2712 - acc_ensemble: 0.9490 - acc_1: 0.9150 - acc_2: 0.9070 - val_loss_1: 0.6768 - val_loss_2: 0.6670 - val_acc_ensemble: 0.8190 - val_acc_1: 0.7866 - val_acc_2: 0.7912\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2685 - loss_2: 0.2641 - acc_ensemble: 0.9370 - acc_1: 0.9040 - acc_2: 0.9020 - val_loss_1: 0.6790 - val_loss_2: 0.6515 - val_acc_ensemble: 0.8269 - val_acc_1: 0.7867 - val_acc_2: 0.7961\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2540 - loss_2: 0.2603 - acc_ensemble: 0.9550 - acc_1: 0.9270 - acc_2: 0.9230 - val_loss_1: 0.6450 - val_loss_2: 0.6615 - val_acc_ensemble: 0.8268 - val_acc_1: 0.7975 - val_acc_2: 0.7959\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2461 - loss_2: 0.2423 - acc_ensemble: 0.9450 - acc_1: 0.9230 - acc_2: 0.8960 - val_loss_1: 0.6861 - val_loss_2: 0.6867 - val_acc_ensemble: 0.8225 - val_acc_1: 0.7886 - val_acc_2: 0.7860\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2659 - loss_2: 0.2334 - acc_ensemble: 0.9570 - acc_1: 0.9330 - acc_2: 0.9100 - val_loss_1: 0.6580 - val_loss_2: 0.6775 - val_acc_ensemble: 0.8255 - val_acc_1: 0.7952 - val_acc_2: 0.7933\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2252 - loss_2: 0.2247 - acc_ensemble: 0.9650 - acc_1: 0.9340 - acc_2: 0.9270 - val_loss_1: 0.6657 - val_loss_2: 0.6544 - val_acc_ensemble: 0.8319 - val_acc_1: 0.7932 - val_acc_2: 0.8008\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.1989 - loss_2: 0.2181 - acc_ensemble: 0.9640 - acc_1: 0.9310 - acc_2: 0.9250 - val_loss_1: 0.6796 - val_loss_2: 0.6540 - val_acc_ensemble: 0.8289 - val_acc_1: 0.7907 - val_acc_2: 0.8007\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.1916 - loss_2: 0.1840 - acc_ensemble: 0.9640 - acc_1: 0.9350 - acc_2: 0.9350 - val_loss_1: 0.6923 - val_loss_2: 0.6721 - val_acc_ensemble: 0.8298 - val_acc_1: 0.7922 - val_acc_2: 0.7987\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.1771 - loss_2: 0.1831 - acc_ensemble: 0.9620 - acc_1: 0.9300 - acc_2: 0.9270 - val_loss_1: 0.7023 - val_loss_2: 0.6984 - val_acc_ensemble: 0.8245 - val_acc_1: 0.7898 - val_acc_2: 0.7906\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.1686 - loss_2: 0.1997 - acc_ensemble: 0.9690 - acc_1: 0.9410 - acc_2: 0.9370 - val_loss_1: 0.7012 - val_loss_2: 0.6920 - val_acc_ensemble: 0.8316 - val_acc_1: 0.7948 - val_acc_2: 0.7983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.2026 - loss_2: 0.1823 - acc_ensemble: 0.9660 - acc_1: 0.9450 - acc_2: 0.9450 - val_loss_1: 0.7018 - val_loss_2: 0.6833 - val_acc_ensemble: 0.8288 - val_acc_1: 0.7961 - val_acc_2: 0.7996\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1389 - loss_2: 0.1622 - acc_ensemble: 0.9650 - acc_1: 0.9430 - acc_2: 0.9300 - val_loss_1: 0.7303 - val_loss_2: 0.7068 - val_acc_ensemble: 0.8288 - val_acc_1: 0.7914 - val_acc_2: 0.7940\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1307 - loss_2: 0.1653 - acc_ensemble: 0.9770 - acc_1: 0.9600 - acc_2: 0.9370 - val_loss_1: 0.7100 - val_loss_2: 0.7234 - val_acc_ensemble: 0.8316 - val_acc_1: 0.7953 - val_acc_2: 0.7964\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1446 - loss_2: 0.1559 - acc_ensemble: 0.9730 - acc_1: 0.9530 - acc_2: 0.9370 - val_loss_1: 0.7254 - val_loss_2: 0.7152 - val_acc_ensemble: 0.8308 - val_acc_1: 0.7969 - val_acc_2: 0.7966\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1500 - loss_2: 0.1706 - acc_ensemble: 0.9700 - acc_1: 0.9580 - acc_2: 0.9350 - val_loss_1: 0.7552 - val_loss_2: 0.7550 - val_acc_ensemble: 0.8248 - val_acc_1: 0.7925 - val_acc_2: 0.7860\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1405 - loss_2: 0.1380 - acc_ensemble: 0.9810 - acc_1: 0.9430 - acc_2: 0.9460 - val_loss_1: 0.7468 - val_loss_2: 0.7358 - val_acc_ensemble: 0.8327 - val_acc_1: 0.7917 - val_acc_2: 0.8007\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1265 - loss_2: 0.1552 - acc_ensemble: 0.9850 - acc_1: 0.9710 - acc_2: 0.9440 - val_loss_1: 0.7624 - val_loss_2: 0.7194 - val_acc_ensemble: 0.8349 - val_acc_1: 0.7980 - val_acc_2: 0.7971\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1305 - loss_2: 0.1470 - acc_ensemble: 0.9820 - acc_1: 0.9500 - acc_2: 0.9540 - val_loss_1: 0.7462 - val_loss_2: 0.7486 - val_acc_ensemble: 0.8313 - val_acc_1: 0.8007 - val_acc_2: 0.7981\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1168 - loss_2: 0.1229 - acc_ensemble: 0.9830 - acc_1: 0.9580 - acc_2: 0.9540 - val_loss_1: 0.7564 - val_loss_2: 0.7479 - val_acc_ensemble: 0.8330 - val_acc_1: 0.8006 - val_acc_2: 0.7978\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.1308 - loss_2: 0.1183 - acc_ensemble: 0.9820 - acc_1: 0.9580 - acc_2: 0.9570 - val_loss_1: 0.7549 - val_loss_2: 0.7303 - val_acc_ensemble: 0.8325 - val_acc_1: 0.7957 - val_acc_2: 0.7999\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.1348 - loss_2: 0.1076 - acc_ensemble: 0.9840 - acc_1: 0.9690 - acc_2: 0.9570 - val_loss_1: 0.7769 - val_loss_2: 0.7687 - val_acc_ensemble: 0.8304 - val_acc_1: 0.7929 - val_acc_2: 0.7971\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.1049 - loss_2: 0.1234 - acc_ensemble: 0.9830 - acc_1: 0.9660 - acc_2: 0.9530 - val_loss_1: 0.7823 - val_loss_2: 0.7755 - val_acc_ensemble: 0.8329 - val_acc_1: 0.7958 - val_acc_2: 0.7947\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.1060 - loss_2: 0.1154 - acc_ensemble: 0.9880 - acc_1: 0.9650 - acc_2: 0.9610 - val_loss_1: 0.7760 - val_loss_2: 0.7697 - val_acc_ensemble: 0.8340 - val_acc_1: 0.7971 - val_acc_2: 0.7991\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.50\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,16] [None,32,32,16]  1344       input              \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_1         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_1             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,16] [None,32,32,16]  16240      relu_1_1           \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_2         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_2             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,16] [None,16,16,16]  0          relu_1_2           \n",
      "                                    [None,16,16,16] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,32] [None,16,16,32]  32480      avg_pool2d_1       \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,32] [None,16,16,32]  192        conv2d_2_1         \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,32] [None,16,16,32]  0          bn_2_1             \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,32] [None,16,16,32]  64736      relu_2_1           \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,32] [None,16,16,32]  192        conv2d_2_2         \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,32] [None,16,16,32]  0          bn_2_2             \n",
      "                                    [None,16,16,32] [None,16,16,32]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,32] [None,8,8,32]      0          relu_2_2           \n",
      "                                    [None,8,8,32] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,64] [None,8,8,64]      129472     avg_pool2d_2       \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,64] [None,8,8,64]      384        conv2d_3_1         \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,64] [None,8,8,64]      0          bn_3_1             \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,64] [None,8,8,64]      258496     relu_3_1           \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,64] [None,8,8,64]      384        conv2d_3_2         \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,64] [None,8,8,64]      0          bn_3_2             \n",
      "                                    [None,8,8,64] [None,8,8,64]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,64] [None,64]              0          relu_3_2           \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,64] [None,64]              29120      global_avg_pool2d  \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,64] [None,64]              384        fc1                \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,64] [None,64]              0          bn_fc1             \n",
      "                                    [None,64] [None,64]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        2275       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 535891\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.6423 - loss_2: 1.6859 - acc_ensemble: 0.5180 - acc_1: 0.4990 - acc_2: 0.4750 - val_loss_1: 1.3899 - val_loss_2: 1.4302 - val_acc_ensemble: 0.5092 - val_acc_1: 0.4874 - val_acc_2: 0.4735\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.2999 - loss_2: 1.3326 - acc_ensemble: 0.6020 - acc_1: 0.5570 - acc_2: 0.5610 - val_loss_1: 1.2025 - val_loss_2: 1.2289 - val_acc_ensemble: 0.5965 - val_acc_1: 0.5612 - val_acc_2: 0.5600\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.1059 - loss_2: 1.1706 - acc_ensemble: 0.6670 - acc_1: 0.6190 - acc_2: 0.6340 - val_loss_1: 1.1002 - val_loss_2: 1.1273 - val_acc_ensemble: 0.6251 - val_acc_1: 0.6043 - val_acc_2: 0.5942\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 1.0241 - loss_2: 1.0343 - acc_ensemble: 0.6830 - acc_1: 0.6570 - acc_2: 0.6410 - val_loss_1: 1.0004 - val_loss_2: 1.0013 - val_acc_ensemble: 0.6723 - val_acc_1: 0.6411 - val_acc_2: 0.6469\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9306 - loss_2: 0.9494 - acc_ensemble: 0.7240 - acc_1: 0.6970 - acc_2: 0.6820 - val_loss_1: 0.9108 - val_loss_2: 0.9375 - val_acc_ensemble: 0.6989 - val_acc_1: 0.6765 - val_acc_2: 0.6712\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8419 - loss_2: 0.8779 - acc_ensemble: 0.7190 - acc_1: 0.7040 - acc_2: 0.6930 - val_loss_1: 0.8647 - val_loss_2: 0.9084 - val_acc_ensemble: 0.7148 - val_acc_1: 0.6914 - val_acc_2: 0.6794\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.8111 - loss_2: 0.8096 - acc_ensemble: 0.7650 - acc_1: 0.7490 - acc_2: 0.7260 - val_loss_1: 0.8325 - val_loss_2: 0.8499 - val_acc_ensemble: 0.7309 - val_acc_1: 0.7055 - val_acc_2: 0.7018\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7443 - loss_2: 0.7508 - acc_ensemble: 0.7690 - acc_1: 0.7340 - acc_2: 0.7470 - val_loss_1: 0.7985 - val_loss_2: 0.8158 - val_acc_ensemble: 0.7498 - val_acc_1: 0.7180 - val_acc_2: 0.7130\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.7078 - loss_2: 0.7107 - acc_ensemble: 0.7960 - acc_1: 0.7670 - acc_2: 0.7620 - val_loss_1: 0.7475 - val_loss_2: 0.7781 - val_acc_ensemble: 0.7630 - val_acc_1: 0.7368 - val_acc_2: 0.7281\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6583 - loss_2: 0.6893 - acc_ensemble: 0.8190 - acc_1: 0.7940 - acc_2: 0.7820 - val_loss_1: 0.7529 - val_loss_2: 0.7586 - val_acc_ensemble: 0.7650 - val_acc_1: 0.7330 - val_acc_2: 0.7304\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6223 - loss_2: 0.6535 - acc_ensemble: 0.8210 - acc_1: 0.7940 - acc_2: 0.7800 - val_loss_1: 0.7107 - val_loss_2: 0.7438 - val_acc_ensemble: 0.7749 - val_acc_1: 0.7465 - val_acc_2: 0.7377\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.5694 - loss_2: 0.5865 - acc_ensemble: 0.8470 - acc_1: 0.8160 - acc_2: 0.8040 - val_loss_1: 0.6915 - val_loss_2: 0.6983 - val_acc_ensemble: 0.7881 - val_acc_1: 0.7613 - val_acc_2: 0.7554\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5714 - loss_2: 0.5738 - acc_ensemble: 0.8420 - acc_1: 0.8140 - acc_2: 0.7970 - val_loss_1: 0.6944 - val_loss_2: 0.7100 - val_acc_ensemble: 0.7856 - val_acc_1: 0.7607 - val_acc_2: 0.7547\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5303 - loss_2: 0.5437 - acc_ensemble: 0.8390 - acc_1: 0.8240 - acc_2: 0.8070 - val_loss_1: 0.6695 - val_loss_2: 0.6938 - val_acc_ensemble: 0.7853 - val_acc_1: 0.7664 - val_acc_2: 0.7584\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.5126 - loss_2: 0.5279 - acc_ensemble: 0.8670 - acc_1: 0.8390 - acc_2: 0.8340 - val_loss_1: 0.6542 - val_loss_2: 0.6641 - val_acc_ensemble: 0.7999 - val_acc_1: 0.7719 - val_acc_2: 0.7690\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.4660 - loss_2: 0.4867 - acc_ensemble: 0.8600 - acc_1: 0.7950 - acc_2: 0.8430 - val_loss_1: 0.6882 - val_loss_2: 0.6657 - val_acc_ensemble: 0.7972 - val_acc_1: 0.7667 - val_acc_2: 0.7696\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4503 - loss_2: 0.4656 - acc_ensemble: 0.8850 - acc_1: 0.8530 - acc_2: 0.8380 - val_loss_1: 0.6438 - val_loss_2: 0.6686 - val_acc_ensemble: 0.8032 - val_acc_1: 0.7845 - val_acc_2: 0.7712\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4097 - loss_2: 0.4209 - acc_ensemble: 0.8850 - acc_1: 0.8500 - acc_2: 0.8540 - val_loss_1: 0.6426 - val_loss_2: 0.6654 - val_acc_ensemble: 0.8076 - val_acc_1: 0.7773 - val_acc_2: 0.7737\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.3962 - loss_2: 0.4346 - acc_ensemble: 0.9060 - acc_1: 0.8680 - acc_2: 0.8560 - val_loss_1: 0.6299 - val_loss_2: 0.6550 - val_acc_ensemble: 0.8145 - val_acc_1: 0.7879 - val_acc_2: 0.7756\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.3892 - loss_2: 0.4298 - acc_ensemble: 0.9120 - acc_1: 0.8600 - acc_2: 0.8710 - val_loss_1: 0.6294 - val_loss_2: 0.6334 - val_acc_ensemble: 0.8167 - val_acc_1: 0.7858 - val_acc_2: 0.7852\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3777 - loss_2: 0.3686 - acc_ensemble: 0.8980 - acc_1: 0.8740 - acc_2: 0.8670 - val_loss_1: 0.6268 - val_loss_2: 0.6443 - val_acc_ensemble: 0.8216 - val_acc_1: 0.7945 - val_acc_2: 0.7841\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3720 - loss_2: 0.3704 - acc_ensemble: 0.9180 - acc_1: 0.8750 - acc_2: 0.8730 - val_loss_1: 0.6211 - val_loss_2: 0.6481 - val_acc_ensemble: 0.8217 - val_acc_1: 0.7956 - val_acc_2: 0.7866\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3619 - loss_2: 0.3569 - acc_ensemble: 0.9190 - acc_1: 0.8820 - acc_2: 0.8780 - val_loss_1: 0.6358 - val_loss_2: 0.6444 - val_acc_ensemble: 0.8216 - val_acc_1: 0.7883 - val_acc_2: 0.7899\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3272 - loss_2: 0.3571 - acc_ensemble: 0.9240 - acc_1: 0.8910 - acc_2: 0.8880 - val_loss_1: 0.6330 - val_loss_2: 0.6443 - val_acc_ensemble: 0.8227 - val_acc_1: 0.7923 - val_acc_2: 0.7880\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3238 - loss_2: 0.3207 - acc_ensemble: 0.9290 - acc_1: 0.8920 - acc_2: 0.8850 - val_loss_1: 0.6294 - val_loss_2: 0.6327 - val_acc_ensemble: 0.8244 - val_acc_1: 0.7931 - val_acc_2: 0.7897\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.3109 - loss_2: 0.2998 - acc_ensemble: 0.9270 - acc_1: 0.8890 - acc_2: 0.9000 - val_loss_1: 0.6469 - val_loss_2: 0.6387 - val_acc_ensemble: 0.8262 - val_acc_1: 0.7921 - val_acc_2: 0.7957\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2962 - loss_2: 0.3080 - acc_ensemble: 0.9460 - acc_1: 0.9080 - acc_2: 0.8950 - val_loss_1: 0.6111 - val_loss_2: 0.6429 - val_acc_ensemble: 0.8316 - val_acc_1: 0.8007 - val_acc_2: 0.7956\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2680 - loss_2: 0.3011 - acc_ensemble: 0.9390 - acc_1: 0.8990 - acc_2: 0.9140 - val_loss_1: 0.6399 - val_loss_2: 0.6247 - val_acc_ensemble: 0.8311 - val_acc_1: 0.7954 - val_acc_2: 0.7993\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2698 - loss_2: 0.2707 - acc_ensemble: 0.9360 - acc_1: 0.9160 - acc_2: 0.8920 - val_loss_1: 0.6413 - val_loss_2: 0.6490 - val_acc_ensemble: 0.8338 - val_acc_1: 0.7967 - val_acc_2: 0.7943\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2547 - loss_2: 0.2618 - acc_ensemble: 0.9400 - acc_1: 0.9100 - acc_2: 0.9120 - val_loss_1: 0.6511 - val_loss_2: 0.6477 - val_acc_ensemble: 0.8298 - val_acc_1: 0.7954 - val_acc_2: 0.7940\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2407 - loss_2: 0.2460 - acc_ensemble: 0.9510 - acc_1: 0.9180 - acc_2: 0.9330 - val_loss_1: 0.6446 - val_loss_2: 0.6401 - val_acc_ensemble: 0.8322 - val_acc_1: 0.7990 - val_acc_2: 0.8012\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2153 - loss_2: 0.2430 - acc_ensemble: 0.9460 - acc_1: 0.9190 - acc_2: 0.9300 - val_loss_1: 0.6306 - val_loss_2: 0.6474 - val_acc_ensemble: 0.8338 - val_acc_1: 0.8036 - val_acc_2: 0.7992\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2048 - loss_2: 0.2066 - acc_ensemble: 0.9670 - acc_1: 0.9190 - acc_2: 0.9190 - val_loss_1: 0.6671 - val_loss_2: 0.6350 - val_acc_ensemble: 0.8352 - val_acc_1: 0.8000 - val_acc_2: 0.8054\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.2192 - loss_2: 0.2258 - acc_ensemble: 0.9600 - acc_1: 0.9160 - acc_2: 0.9310 - val_loss_1: 0.6801 - val_loss_2: 0.6608 - val_acc_ensemble: 0.8326 - val_acc_1: 0.7947 - val_acc_2: 0.8002\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.1999 - loss_2: 0.1933 - acc_ensemble: 0.9550 - acc_1: 0.9330 - acc_2: 0.9310 - val_loss_1: 0.6695 - val_loss_2: 0.6673 - val_acc_ensemble: 0.8328 - val_acc_1: 0.7978 - val_acc_2: 0.8026\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.1792 - loss_2: 0.2001 - acc_ensemble: 0.9710 - acc_1: 0.9390 - acc_2: 0.9340 - val_loss_1: 0.6774 - val_loss_2: 0.6794 - val_acc_ensemble: 0.8349 - val_acc_1: 0.7967 - val_acc_2: 0.8006\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.2085 - loss_2: 0.1718 - acc_ensemble: 0.9660 - acc_1: 0.9360 - acc_2: 0.9430 - val_loss_1: 0.6723 - val_loss_2: 0.6665 - val_acc_ensemble: 0.8348 - val_acc_1: 0.8011 - val_acc_2: 0.8028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1836 - loss_2: 0.1742 - acc_ensemble: 0.9750 - acc_1: 0.9400 - acc_2: 0.9400 - val_loss_1: 0.6583 - val_loss_2: 0.6936 - val_acc_ensemble: 0.8345 - val_acc_1: 0.8044 - val_acc_2: 0.7977\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1457 - loss_2: 0.1787 - acc_ensemble: 0.9810 - acc_1: 0.9530 - acc_2: 0.9460 - val_loss_1: 0.7065 - val_loss_2: 0.6788 - val_acc_ensemble: 0.8339 - val_acc_1: 0.7977 - val_acc_2: 0.8046\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1699 - loss_2: 0.1670 - acc_ensemble: 0.9820 - acc_1: 0.9520 - acc_2: 0.9400 - val_loss_1: 0.6744 - val_loss_2: 0.7094 - val_acc_ensemble: 0.8353 - val_acc_1: 0.8022 - val_acc_2: 0.7998\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1519 - loss_2: 0.1398 - acc_ensemble: 0.9780 - acc_1: 0.9370 - acc_2: 0.9370 - val_loss_1: 0.6935 - val_loss_2: 0.7171 - val_acc_ensemble: 0.8355 - val_acc_1: 0.8002 - val_acc_2: 0.8004\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1688 - loss_2: 0.1578 - acc_ensemble: 0.9760 - acc_1: 0.9500 - acc_2: 0.9540 - val_loss_1: 0.7067 - val_loss_2: 0.7073 - val_acc_ensemble: 0.8380 - val_acc_1: 0.7949 - val_acc_2: 0.8065\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1381 - loss_2: 0.1539 - acc_ensemble: 0.9800 - acc_1: 0.9430 - acc_2: 0.9400 - val_loss_1: 0.6841 - val_loss_2: 0.6989 - val_acc_ensemble: 0.8413 - val_acc_1: 0.8046 - val_acc_2: 0.8067\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1453 - loss_2: 0.1510 - acc_ensemble: 0.9910 - acc_1: 0.9620 - acc_2: 0.9510 - val_loss_1: 0.6938 - val_loss_2: 0.7141 - val_acc_ensemble: 0.8424 - val_acc_1: 0.8047 - val_acc_2: 0.8046\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1276 - loss_2: 0.1446 - acc_ensemble: 0.9820 - acc_1: 0.9530 - acc_2: 0.9520 - val_loss_1: 0.7055 - val_loss_2: 0.7166 - val_acc_ensemble: 0.8383 - val_acc_1: 0.8071 - val_acc_2: 0.8027\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1181 - loss_2: 0.1328 - acc_ensemble: 0.9870 - acc_1: 0.9670 - acc_2: 0.9570 - val_loss_1: 0.7239 - val_loss_2: 0.7309 - val_acc_ensemble: 0.8378 - val_acc_1: 0.8045 - val_acc_2: 0.8056\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.1189 - loss_2: 0.1343 - acc_ensemble: 0.9820 - acc_1: 0.9570 - acc_2: 0.9650 - val_loss_1: 0.7291 - val_loss_2: 0.7414 - val_acc_ensemble: 0.8384 - val_acc_1: 0.8055 - val_acc_2: 0.8000\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.1211 - loss_2: 0.0837 - acc_ensemble: 0.9900 - acc_1: 0.9580 - acc_2: 0.9650 - val_loss_1: 0.7426 - val_loss_2: 0.7394 - val_acc_ensemble: 0.8344 - val_acc_1: 0.8005 - val_acc_2: 0.8062\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.1017 - loss_2: 0.1071 - acc_ensemble: 0.9900 - acc_1: 0.9630 - acc_2: 0.9530 - val_loss_1: 0.7359 - val_loss_2: 0.7928 - val_acc_ensemble: 0.8370 - val_acc_1: 0.7998 - val_acc_2: 0.7973\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.1080 - loss_2: 0.1294 - acc_ensemble: 0.9780 - acc_1: 0.9700 - acc_2: 0.9540 - val_loss_1: 0.7267 - val_loss_2: 0.7815 - val_acc_ensemble: 0.8419 - val_acc_1: 0.8088 - val_acc_2: 0.7998\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.75\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,24] [None,32,32,8]   1120       input              \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,24] [None,32,32,8]   80         conv2d_1_1         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,24] [None,32,32,8]   0          bn_1_1             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,24] [None,32,32,8]   13352      relu_1_1           \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,24] [None,32,32,8]   80         conv2d_1_2         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,24] [None,32,32,8]   0          bn_1_2             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,24] [None,16,16,8]   0          relu_1_2           \n",
      "                                    [None,16,16,24] [None,16,16,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,48] [None,16,16,16]  26704      avg_pool2d_1       \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,48] [None,16,16,16]  160        conv2d_2_1         \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,48] [None,16,16,16]  0          bn_2_1             \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,48] [None,16,16,16]  53200      relu_2_1           \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,48] [None,16,16,16]  160        conv2d_2_2         \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,48] [None,16,16,16]  0          bn_2_2             \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,48] [None,8,8,16]      0          relu_2_2           \n",
      "                                    [None,8,8,48] [None,8,8,16]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,96] [None,8,8,32]      106400     avg_pool2d_2       \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,96] [None,8,8,32]      320        conv2d_3_1         \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,96] [None,8,8,32]      0          bn_3_1             \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,96] [None,8,8,32]      212384     relu_3_1           \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,96] [None,8,8,32]      320        conv2d_3_2         \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,96] [None,8,8,32]      0          bn_3_2             \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,96] [None,32]              0          relu_3_2           \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,96] [None,32]              23968      global_avg_pool2d  \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,96] [None,32]              320        fc1                \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,96] [None,32]              0          bn_fc1             \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        1921       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 440489\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.6512 - loss_2: 1.6737 - acc_ensemble: 0.5170 - acc_1: 0.5160 - acc_2: 0.4920 - val_loss_1: 1.3885 - val_loss_2: 1.3797 - val_acc_ensemble: 0.5157 - val_acc_1: 0.4927 - val_acc_2: 0.4958\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.3013 - loss_2: 1.2807 - acc_ensemble: 0.6120 - acc_1: 0.5850 - acc_2: 0.5800 - val_loss_1: 1.2073 - val_loss_2: 1.1659 - val_acc_ensemble: 0.5919 - val_acc_1: 0.5684 - val_acc_2: 0.5749\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.1129 - loss_2: 1.1173 - acc_ensemble: 0.6760 - acc_1: 0.6460 - acc_2: 0.6270 - val_loss_1: 1.0427 - val_loss_2: 1.0885 - val_acc_ensemble: 0.6414 - val_acc_1: 0.6240 - val_acc_2: 0.6073\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 1.0110 - loss_2: 0.9934 - acc_ensemble: 0.7050 - acc_1: 0.6810 - acc_2: 0.6720 - val_loss_1: 0.9668 - val_loss_2: 0.9569 - val_acc_ensemble: 0.6755 - val_acc_1: 0.6499 - val_acc_2: 0.6568\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9251 - loss_2: 0.9153 - acc_ensemble: 0.7290 - acc_1: 0.7010 - acc_2: 0.6910 - val_loss_1: 0.9208 - val_loss_2: 0.9060 - val_acc_ensemble: 0.6951 - val_acc_1: 0.6724 - val_acc_2: 0.6774\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8416 - loss_2: 0.8392 - acc_ensemble: 0.7510 - acc_1: 0.7230 - acc_2: 0.7000 - val_loss_1: 0.8608 - val_loss_2: 0.8735 - val_acc_ensemble: 0.7170 - val_acc_1: 0.6936 - val_acc_2: 0.6901\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.7798 - loss_2: 0.7658 - acc_ensemble: 0.7800 - acc_1: 0.7480 - acc_2: 0.7400 - val_loss_1: 0.8223 - val_loss_2: 0.8048 - val_acc_ensemble: 0.7364 - val_acc_1: 0.7083 - val_acc_2: 0.7144\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7363 - loss_2: 0.7184 - acc_ensemble: 0.7830 - acc_1: 0.7600 - acc_2: 0.7710 - val_loss_1: 0.7890 - val_loss_2: 0.7815 - val_acc_ensemble: 0.7464 - val_acc_1: 0.7197 - val_acc_2: 0.7226\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.6735 - loss_2: 0.6764 - acc_ensemble: 0.7960 - acc_1: 0.7850 - acc_2: 0.7710 - val_loss_1: 0.7729 - val_loss_2: 0.7677 - val_acc_ensemble: 0.7497 - val_acc_1: 0.7257 - val_acc_2: 0.7259\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6644 - loss_2: 0.6581 - acc_ensemble: 0.8210 - acc_1: 0.8020 - acc_2: 0.7960 - val_loss_1: 0.7222 - val_loss_2: 0.7209 - val_acc_ensemble: 0.7662 - val_acc_1: 0.7493 - val_acc_2: 0.7502\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6149 - loss_2: 0.5976 - acc_ensemble: 0.8290 - acc_1: 0.8080 - acc_2: 0.8040 - val_loss_1: 0.7608 - val_loss_2: 0.7179 - val_acc_ensemble: 0.7633 - val_acc_1: 0.7346 - val_acc_2: 0.7493\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.5740 - loss_2: 0.5715 - acc_ensemble: 0.8400 - acc_1: 0.8010 - acc_2: 0.8280 - val_loss_1: 0.7018 - val_loss_2: 0.6955 - val_acc_ensemble: 0.7758 - val_acc_1: 0.7575 - val_acc_2: 0.7608\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5487 - loss_2: 0.5336 - acc_ensemble: 0.8500 - acc_1: 0.8250 - acc_2: 0.8230 - val_loss_1: 0.6780 - val_loss_2: 0.6713 - val_acc_ensemble: 0.7872 - val_acc_1: 0.7613 - val_acc_2: 0.7649\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5523 - loss_2: 0.5094 - acc_ensemble: 0.8710 - acc_1: 0.8420 - acc_2: 0.8260 - val_loss_1: 0.6846 - val_loss_2: 0.6817 - val_acc_ensemble: 0.7854 - val_acc_1: 0.7605 - val_acc_2: 0.7653\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.4953 - loss_2: 0.4875 - acc_ensemble: 0.8650 - acc_1: 0.8420 - acc_2: 0.8370 - val_loss_1: 0.6606 - val_loss_2: 0.6598 - val_acc_ensemble: 0.7954 - val_acc_1: 0.7698 - val_acc_2: 0.7736\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.4925 - loss_2: 0.4819 - acc_ensemble: 0.8720 - acc_1: 0.8410 - acc_2: 0.8430 - val_loss_1: 0.6608 - val_loss_2: 0.6692 - val_acc_ensemble: 0.7985 - val_acc_1: 0.7731 - val_acc_2: 0.7726\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4657 - loss_2: 0.4324 - acc_ensemble: 0.8870 - acc_1: 0.8670 - acc_2: 0.8640 - val_loss_1: 0.6350 - val_loss_2: 0.6545 - val_acc_ensemble: 0.8008 - val_acc_1: 0.7771 - val_acc_2: 0.7754\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4391 - loss_2: 0.4129 - acc_ensemble: 0.8950 - acc_1: 0.8600 - acc_2: 0.8560 - val_loss_1: 0.6435 - val_loss_2: 0.6358 - val_acc_ensemble: 0.8061 - val_acc_1: 0.7780 - val_acc_2: 0.7862\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.4177 - loss_2: 0.4109 - acc_ensemble: 0.8950 - acc_1: 0.8740 - acc_2: 0.8820 - val_loss_1: 0.6448 - val_loss_2: 0.6318 - val_acc_ensemble: 0.8082 - val_acc_1: 0.7792 - val_acc_2: 0.7881\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.4069 - loss_2: 0.3942 - acc_ensemble: 0.9010 - acc_1: 0.8820 - acc_2: 0.8750 - val_loss_1: 0.6228 - val_loss_2: 0.6328 - val_acc_ensemble: 0.8121 - val_acc_1: 0.7887 - val_acc_2: 0.7838\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3757 - loss_2: 0.3936 - acc_ensemble: 0.9070 - acc_1: 0.8700 - acc_2: 0.8660 - val_loss_1: 0.6387 - val_loss_2: 0.6341 - val_acc_ensemble: 0.8111 - val_acc_1: 0.7870 - val_acc_2: 0.7908\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3783 - loss_2: 0.3601 - acc_ensemble: 0.9160 - acc_1: 0.8790 - acc_2: 0.8880 - val_loss_1: 0.6319 - val_loss_2: 0.6370 - val_acc_ensemble: 0.8108 - val_acc_1: 0.7915 - val_acc_2: 0.7903\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3348 - loss_2: 0.3305 - acc_ensemble: 0.9230 - acc_1: 0.8910 - acc_2: 0.9020 - val_loss_1: 0.6339 - val_loss_2: 0.6231 - val_acc_ensemble: 0.8136 - val_acc_1: 0.7904 - val_acc_2: 0.7941\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3273 - loss_2: 0.3041 - acc_ensemble: 0.9250 - acc_1: 0.9100 - acc_2: 0.9020 - val_loss_1: 0.6300 - val_loss_2: 0.6399 - val_acc_ensemble: 0.8148 - val_acc_1: 0.7911 - val_acc_2: 0.7876\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3003 - loss_2: 0.2880 - acc_ensemble: 0.9310 - acc_1: 0.8950 - acc_2: 0.8970 - val_loss_1: 0.6574 - val_loss_2: 0.6489 - val_acc_ensemble: 0.8155 - val_acc_1: 0.7903 - val_acc_2: 0.7881\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.3102 - loss_2: 0.3011 - acc_ensemble: 0.9260 - acc_1: 0.9160 - acc_2: 0.8960 - val_loss_1: 0.6196 - val_loss_2: 0.6411 - val_acc_ensemble: 0.8240 - val_acc_1: 0.7987 - val_acc_2: 0.7944\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2816 - loss_2: 0.2787 - acc_ensemble: 0.9280 - acc_1: 0.9010 - acc_2: 0.9100 - val_loss_1: 0.6526 - val_loss_2: 0.6476 - val_acc_ensemble: 0.8220 - val_acc_1: 0.7936 - val_acc_2: 0.7917\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2721 - loss_2: 0.2645 - acc_ensemble: 0.9370 - acc_1: 0.9090 - acc_2: 0.9260 - val_loss_1: 0.6487 - val_loss_2: 0.6381 - val_acc_ensemble: 0.8184 - val_acc_1: 0.7934 - val_acc_2: 0.7981\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2619 - loss_2: 0.2622 - acc_ensemble: 0.9400 - acc_1: 0.9100 - acc_2: 0.9220 - val_loss_1: 0.6619 - val_loss_2: 0.6549 - val_acc_ensemble: 0.8219 - val_acc_1: 0.7943 - val_acc_2: 0.7979\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2671 - loss_2: 0.2466 - acc_ensemble: 0.9360 - acc_1: 0.9140 - acc_2: 0.9180 - val_loss_1: 0.6413 - val_loss_2: 0.6371 - val_acc_ensemble: 0.8193 - val_acc_1: 0.7939 - val_acc_2: 0.7972\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2507 - loss_2: 0.2371 - acc_ensemble: 0.9480 - acc_1: 0.9240 - acc_2: 0.9130 - val_loss_1: 0.6350 - val_loss_2: 0.6455 - val_acc_ensemble: 0.8242 - val_acc_1: 0.8030 - val_acc_2: 0.7968\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2426 - loss_2: 0.2233 - acc_ensemble: 0.9510 - acc_1: 0.9340 - acc_2: 0.9330 - val_loss_1: 0.6365 - val_loss_2: 0.6613 - val_acc_ensemble: 0.8257 - val_acc_1: 0.8028 - val_acc_2: 0.7971\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2125 - loss_2: 0.1919 - acc_ensemble: 0.9530 - acc_1: 0.9230 - acc_2: 0.9360 - val_loss_1: 0.6505 - val_loss_2: 0.6637 - val_acc_ensemble: 0.8247 - val_acc_1: 0.8015 - val_acc_2: 0.7972\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.2158 - loss_2: 0.1881 - acc_ensemble: 0.9600 - acc_1: 0.9220 - acc_2: 0.9350 - val_loss_1: 0.7002 - val_loss_2: 0.7012 - val_acc_ensemble: 0.8158 - val_acc_1: 0.7928 - val_acc_2: 0.7923\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.1884 - loss_2: 0.1965 - acc_ensemble: 0.9630 - acc_1: 0.9390 - acc_2: 0.9340 - val_loss_1: 0.6489 - val_loss_2: 0.6783 - val_acc_ensemble: 0.8288 - val_acc_1: 0.8062 - val_acc_2: 0.7981\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.1829 - loss_2: 0.1960 - acc_ensemble: 0.9660 - acc_1: 0.9410 - acc_2: 0.9350 - val_loss_1: 0.6657 - val_loss_2: 0.6928 - val_acc_ensemble: 0.8291 - val_acc_1: 0.8092 - val_acc_2: 0.7980\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.1836 - loss_2: 0.1815 - acc_ensemble: 0.9670 - acc_1: 0.9420 - acc_2: 0.9420 - val_loss_1: 0.6667 - val_loss_2: 0.6702 - val_acc_ensemble: 0.8298 - val_acc_1: 0.8039 - val_acc_2: 0.8047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1730 - loss_2: 0.1669 - acc_ensemble: 0.9730 - acc_1: 0.9520 - acc_2: 0.9630 - val_loss_1: 0.6719 - val_loss_2: 0.6820 - val_acc_ensemble: 0.8283 - val_acc_1: 0.8060 - val_acc_2: 0.7990\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1463 - loss_2: 0.1416 - acc_ensemble: 0.9750 - acc_1: 0.9370 - acc_2: 0.9480 - val_loss_1: 0.7015 - val_loss_2: 0.7022 - val_acc_ensemble: 0.8292 - val_acc_1: 0.8005 - val_acc_2: 0.8017\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1849 - loss_2: 0.1463 - acc_ensemble: 0.9730 - acc_1: 0.9460 - acc_2: 0.9440 - val_loss_1: 0.6922 - val_loss_2: 0.7268 - val_acc_ensemble: 0.8257 - val_acc_1: 0.8018 - val_acc_2: 0.7957\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1634 - loss_2: 0.1394 - acc_ensemble: 0.9780 - acc_1: 0.9510 - acc_2: 0.9580 - val_loss_1: 0.6939 - val_loss_2: 0.7070 - val_acc_ensemble: 0.8288 - val_acc_1: 0.7971 - val_acc_2: 0.8016\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1409 - loss_2: 0.1374 - acc_ensemble: 0.9720 - acc_1: 0.9550 - acc_2: 0.9470 - val_loss_1: 0.6851 - val_loss_2: 0.7448 - val_acc_ensemble: 0.8281 - val_acc_1: 0.8100 - val_acc_2: 0.7948\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1273 - loss_2: 0.1285 - acc_ensemble: 0.9820 - acc_1: 0.9580 - acc_2: 0.9520 - val_loss_1: 0.7113 - val_loss_2: 0.7465 - val_acc_ensemble: 0.8237 - val_acc_1: 0.8038 - val_acc_2: 0.7929\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1384 - loss_2: 0.1181 - acc_ensemble: 0.9830 - acc_1: 0.9590 - acc_2: 0.9640 - val_loss_1: 0.7315 - val_loss_2: 0.7603 - val_acc_ensemble: 0.8267 - val_acc_1: 0.8019 - val_acc_2: 0.7981\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1347 - loss_2: 0.1322 - acc_ensemble: 0.9780 - acc_1: 0.9600 - acc_2: 0.9650 - val_loss_1: 0.7380 - val_loss_2: 0.7475 - val_acc_ensemble: 0.8260 - val_acc_1: 0.8054 - val_acc_2: 0.7984\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1365 - loss_2: 0.1279 - acc_ensemble: 0.9760 - acc_1: 0.9580 - acc_2: 0.9590 - val_loss_1: 0.7352 - val_loss_2: 0.7513 - val_acc_ensemble: 0.8278 - val_acc_1: 0.8017 - val_acc_2: 0.7982\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.1138 - loss_2: 0.1324 - acc_ensemble: 0.9820 - acc_1: 0.9630 - acc_2: 0.9610 - val_loss_1: 0.7463 - val_loss_2: 0.7448 - val_acc_ensemble: 0.8262 - val_acc_1: 0.8036 - val_acc_2: 0.7990\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.0961 - loss_2: 0.1068 - acc_ensemble: 0.9910 - acc_1: 0.9630 - acc_2: 0.9620 - val_loss_1: 0.7583 - val_loss_2: 0.7607 - val_acc_ensemble: 0.8314 - val_acc_1: 0.7985 - val_acc_2: 0.8035\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.1017 - loss_2: 0.0843 - acc_ensemble: 0.9880 - acc_1: 0.9690 - acc_2: 0.9690 - val_loss_1: 0.7766 - val_loss_2: 0.7800 - val_acc_ensemble: 0.8266 - val_acc_1: 0.8015 - val_acc_2: 0.8054\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.1035 - loss_2: 0.0990 - acc_ensemble: 0.9850 - acc_1: 0.9690 - acc_2: 0.9640 - val_loss_1: 0.7612 - val_loss_2: 0.7997 - val_acc_ensemble: 0.8249 - val_acc_1: 0.8031 - val_acc_2: 0.8019\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.75\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,24] [None,32,32,8]   1120       input              \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,24] [None,32,32,8]   80         conv2d_1_1         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,24] [None,32,32,8]   0          bn_1_1             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,24] [None,32,32,8]   13352      relu_1_1           \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,24] [None,32,32,8]   80         conv2d_1_2         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,24] [None,32,32,8]   0          bn_1_2             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,24] [None,16,16,8]   0          relu_1_2           \n",
      "                                    [None,16,16,24] [None,16,16,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,48] [None,16,16,16]  26704      avg_pool2d_1       \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,48] [None,16,16,16]  160        conv2d_2_1         \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,48] [None,16,16,16]  0          bn_2_1             \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,48] [None,16,16,16]  53200      relu_2_1           \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,48] [None,16,16,16]  160        conv2d_2_2         \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,48] [None,16,16,16]  0          bn_2_2             \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,48] [None,8,8,16]      0          relu_2_2           \n",
      "                                    [None,8,8,48] [None,8,8,16]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,96] [None,8,8,32]      106400     avg_pool2d_2       \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,96] [None,8,8,32]      320        conv2d_3_1         \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,96] [None,8,8,32]      0          bn_3_1             \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,96] [None,8,8,32]      212384     relu_3_1           \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,96] [None,8,8,32]      320        conv2d_3_2         \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,96] [None,8,8,32]      0          bn_3_2             \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,96] [None,32]              0          relu_3_2           \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,96] [None,32]              23968      global_avg_pool2d  \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,96] [None,32]              320        fc1                \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,96] [None,32]              0          bn_fc1             \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        1921       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 440489\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.6938 - loss_2: 1.6637 - acc_ensemble: 0.5190 - acc_1: 0.4980 - acc_2: 0.4960 - val_loss_1: 1.4065 - val_loss_2: 1.4054 - val_acc_ensemble: 0.5040 - val_acc_1: 0.4820 - val_acc_2: 0.4857\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.2955 - loss_2: 1.2955 - acc_ensemble: 0.6180 - acc_1: 0.5960 - acc_2: 0.5980 - val_loss_1: 1.1993 - val_loss_2: 1.1814 - val_acc_ensemble: 0.5922 - val_acc_1: 0.5672 - val_acc_2: 0.5749\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.1189 - loss_2: 1.1067 - acc_ensemble: 0.6650 - acc_1: 0.6480 - acc_2: 0.6340 - val_loss_1: 1.0518 - val_loss_2: 1.0634 - val_acc_ensemble: 0.6385 - val_acc_1: 0.6205 - val_acc_2: 0.6174\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 0.9787 - loss_2: 0.9701 - acc_ensemble: 0.7060 - acc_1: 0.6830 - acc_2: 0.6770 - val_loss_1: 0.9568 - val_loss_2: 0.9910 - val_acc_ensemble: 0.6710 - val_acc_1: 0.6586 - val_acc_2: 0.6525\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.8981 - loss_2: 0.8751 - acc_ensemble: 0.7410 - acc_1: 0.7270 - acc_2: 0.7280 - val_loss_1: 0.8885 - val_loss_2: 0.8914 - val_acc_ensemble: 0.7022 - val_acc_1: 0.6839 - val_acc_2: 0.6888\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8279 - loss_2: 0.8231 - acc_ensemble: 0.7730 - acc_1: 0.7530 - acc_2: 0.7400 - val_loss_1: 0.8383 - val_loss_2: 0.8240 - val_acc_ensemble: 0.7242 - val_acc_1: 0.7042 - val_acc_2: 0.7110\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.7564 - loss_2: 0.7537 - acc_ensemble: 0.7770 - acc_1: 0.7490 - acc_2: 0.7530 - val_loss_1: 0.8064 - val_loss_2: 0.8036 - val_acc_ensemble: 0.7336 - val_acc_1: 0.7182 - val_acc_2: 0.7159\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7131 - loss_2: 0.6954 - acc_ensemble: 0.8050 - acc_1: 0.7750 - acc_2: 0.7860 - val_loss_1: 0.7544 - val_loss_2: 0.7692 - val_acc_ensemble: 0.7494 - val_acc_1: 0.7350 - val_acc_2: 0.7278\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.6392 - loss_2: 0.6527 - acc_ensemble: 0.8080 - acc_1: 0.7810 - acc_2: 0.7800 - val_loss_1: 0.7420 - val_loss_2: 0.7325 - val_acc_ensemble: 0.7590 - val_acc_1: 0.7393 - val_acc_2: 0.7430\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6359 - loss_2: 0.6155 - acc_ensemble: 0.8300 - acc_1: 0.8130 - acc_2: 0.7890 - val_loss_1: 0.7077 - val_loss_2: 0.7216 - val_acc_ensemble: 0.7672 - val_acc_1: 0.7496 - val_acc_2: 0.7450\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.5830 - loss_2: 0.5899 - acc_ensemble: 0.8340 - acc_1: 0.8070 - acc_2: 0.8120 - val_loss_1: 0.6884 - val_loss_2: 0.6860 - val_acc_ensemble: 0.7764 - val_acc_1: 0.7592 - val_acc_2: 0.7608\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.5511 - loss_2: 0.5407 - acc_ensemble: 0.8450 - acc_1: 0.8330 - acc_2: 0.8180 - val_loss_1: 0.6845 - val_loss_2: 0.6746 - val_acc_ensemble: 0.7793 - val_acc_1: 0.7604 - val_acc_2: 0.7678\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5252 - loss_2: 0.5156 - acc_ensemble: 0.8590 - acc_1: 0.8300 - acc_2: 0.8410 - val_loss_1: 0.6610 - val_loss_2: 0.6570 - val_acc_ensemble: 0.7871 - val_acc_1: 0.7705 - val_acc_2: 0.7674\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5011 - loss_2: 0.4702 - acc_ensemble: 0.8610 - acc_1: 0.8510 - acc_2: 0.8280 - val_loss_1: 0.6683 - val_loss_2: 0.6562 - val_acc_ensemble: 0.7910 - val_acc_1: 0.7679 - val_acc_2: 0.7756\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.4705 - loss_2: 0.4590 - acc_ensemble: 0.8510 - acc_1: 0.8370 - acc_2: 0.8380 - val_loss_1: 0.6735 - val_loss_2: 0.6557 - val_acc_ensemble: 0.7877 - val_acc_1: 0.7714 - val_acc_2: 0.7737\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.4394 - loss_2: 0.4342 - acc_ensemble: 0.8770 - acc_1: 0.8670 - acc_2: 0.8490 - val_loss_1: 0.6408 - val_loss_2: 0.6321 - val_acc_ensemble: 0.8016 - val_acc_1: 0.7818 - val_acc_2: 0.7819\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4164 - loss_2: 0.4362 - acc_ensemble: 0.8810 - acc_1: 0.8650 - acc_2: 0.8540 - val_loss_1: 0.6424 - val_loss_2: 0.6243 - val_acc_ensemble: 0.8038 - val_acc_1: 0.7817 - val_acc_2: 0.7876\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.3964 - loss_2: 0.3791 - acc_ensemble: 0.8950 - acc_1: 0.8900 - acc_2: 0.8600 - val_loss_1: 0.6338 - val_loss_2: 0.6199 - val_acc_ensemble: 0.8088 - val_acc_1: 0.7874 - val_acc_2: 0.7923\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.3549 - loss_2: 0.3598 - acc_ensemble: 0.8930 - acc_1: 0.8720 - acc_2: 0.8620 - val_loss_1: 0.6528 - val_loss_2: 0.6294 - val_acc_ensemble: 0.8007 - val_acc_1: 0.7840 - val_acc_2: 0.7892\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.3693 - loss_2: 0.3713 - acc_ensemble: 0.9040 - acc_1: 0.8880 - acc_2: 0.8940 - val_loss_1: 0.6313 - val_loss_2: 0.6113 - val_acc_ensemble: 0.8061 - val_acc_1: 0.7913 - val_acc_2: 0.7922\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3628 - loss_2: 0.3448 - acc_ensemble: 0.9100 - acc_1: 0.8910 - acc_2: 0.8890 - val_loss_1: 0.6304 - val_loss_2: 0.6406 - val_acc_ensemble: 0.8091 - val_acc_1: 0.7906 - val_acc_2: 0.7864\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3192 - loss_2: 0.3336 - acc_ensemble: 0.9150 - acc_1: 0.8950 - acc_2: 0.9030 - val_loss_1: 0.6443 - val_loss_2: 0.6264 - val_acc_ensemble: 0.8109 - val_acc_1: 0.7922 - val_acc_2: 0.7955\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.2885 - loss_2: 0.3005 - acc_ensemble: 0.9370 - acc_1: 0.9090 - acc_2: 0.9150 - val_loss_1: 0.6248 - val_loss_2: 0.6188 - val_acc_ensemble: 0.8173 - val_acc_1: 0.7995 - val_acc_2: 0.7991\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.2967 - loss_2: 0.2842 - acc_ensemble: 0.9260 - acc_1: 0.9150 - acc_2: 0.9020 - val_loss_1: 0.6304 - val_loss_2: 0.6239 - val_acc_ensemble: 0.8177 - val_acc_1: 0.7974 - val_acc_2: 0.7972\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.2913 - loss_2: 0.2535 - acc_ensemble: 0.9380 - acc_1: 0.9170 - acc_2: 0.9110 - val_loss_1: 0.6309 - val_loss_2: 0.6292 - val_acc_ensemble: 0.8145 - val_acc_1: 0.7987 - val_acc_2: 0.8002\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.2652 - loss_2: 0.2867 - acc_ensemble: 0.9390 - acc_1: 0.9280 - acc_2: 0.9140 - val_loss_1: 0.6210 - val_loss_2: 0.6383 - val_acc_ensemble: 0.8154 - val_acc_1: 0.8037 - val_acc_2: 0.7972\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2485 - loss_2: 0.2659 - acc_ensemble: 0.9400 - acc_1: 0.9160 - acc_2: 0.9250 - val_loss_1: 0.6348 - val_loss_2: 0.6319 - val_acc_ensemble: 0.8214 - val_acc_1: 0.7998 - val_acc_2: 0.8023\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2393 - loss_2: 0.2275 - acc_ensemble: 0.9490 - acc_1: 0.9290 - acc_2: 0.9360 - val_loss_1: 0.6452 - val_loss_2: 0.6270 - val_acc_ensemble: 0.8183 - val_acc_1: 0.8010 - val_acc_2: 0.8034\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2169 - loss_2: 0.2289 - acc_ensemble: 0.9510 - acc_1: 0.9380 - acc_2: 0.9300 - val_loss_1: 0.6498 - val_loss_2: 0.6555 - val_acc_ensemble: 0.8172 - val_acc_1: 0.7994 - val_acc_2: 0.7998\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2205 - loss_2: 0.2102 - acc_ensemble: 0.9450 - acc_1: 0.9350 - acc_2: 0.9240 - val_loss_1: 0.6463 - val_loss_2: 0.6537 - val_acc_ensemble: 0.8175 - val_acc_1: 0.7988 - val_acc_2: 0.7987\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.1899 - loss_2: 0.2330 - acc_ensemble: 0.9550 - acc_1: 0.9400 - acc_2: 0.9330 - val_loss_1: 0.6453 - val_loss_2: 0.6536 - val_acc_ensemble: 0.8223 - val_acc_1: 0.8060 - val_acc_2: 0.8009\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2157 - loss_2: 0.2055 - acc_ensemble: 0.9540 - acc_1: 0.9360 - acc_2: 0.9320 - val_loss_1: 0.6590 - val_loss_2: 0.6480 - val_acc_ensemble: 0.8217 - val_acc_1: 0.8037 - val_acc_2: 0.8021\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.1771 - loss_2: 0.1917 - acc_ensemble: 0.9580 - acc_1: 0.9480 - acc_2: 0.9320 - val_loss_1: 0.6646 - val_loss_2: 0.6418 - val_acc_ensemble: 0.8237 - val_acc_1: 0.8060 - val_acc_2: 0.8065\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.1810 - loss_2: 0.1615 - acc_ensemble: 0.9570 - acc_1: 0.9500 - acc_2: 0.9330 - val_loss_1: 0.6713 - val_loss_2: 0.6687 - val_acc_ensemble: 0.8259 - val_acc_1: 0.8046 - val_acc_2: 0.8043\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.1575 - loss_2: 0.1576 - acc_ensemble: 0.9660 - acc_1: 0.9420 - acc_2: 0.9460 - val_loss_1: 0.6977 - val_loss_2: 0.6757 - val_acc_ensemble: 0.8227 - val_acc_1: 0.8022 - val_acc_2: 0.8053\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.1561 - loss_2: 0.1771 - acc_ensemble: 0.9710 - acc_1: 0.9450 - acc_2: 0.9520 - val_loss_1: 0.6879 - val_loss_2: 0.6724 - val_acc_ensemble: 0.8245 - val_acc_1: 0.8016 - val_acc_2: 0.8042\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.1578 - loss_2: 0.1343 - acc_ensemble: 0.9620 - acc_1: 0.9430 - acc_2: 0.9570 - val_loss_1: 0.6882 - val_loss_2: 0.7086 - val_acc_ensemble: 0.8220 - val_acc_1: 0.8054 - val_acc_2: 0.7970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1362 - loss_2: 0.1662 - acc_ensemble: 0.9740 - acc_1: 0.9490 - acc_2: 0.9560 - val_loss_1: 0.6973 - val_loss_2: 0.6842 - val_acc_ensemble: 0.8233 - val_acc_1: 0.8072 - val_acc_2: 0.8038\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1452 - loss_2: 0.1450 - acc_ensemble: 0.9790 - acc_1: 0.9560 - acc_2: 0.9590 - val_loss_1: 0.7147 - val_loss_2: 0.6966 - val_acc_ensemble: 0.8198 - val_acc_1: 0.8049 - val_acc_2: 0.8037\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1273 - loss_2: 0.1199 - acc_ensemble: 0.9840 - acc_1: 0.9620 - acc_2: 0.9630 - val_loss_1: 0.7079 - val_loss_2: 0.7038 - val_acc_ensemble: 0.8234 - val_acc_1: 0.8079 - val_acc_2: 0.8054\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1231 - loss_2: 0.1205 - acc_ensemble: 0.9730 - acc_1: 0.9480 - acc_2: 0.9600 - val_loss_1: 0.7080 - val_loss_2: 0.7070 - val_acc_ensemble: 0.8278 - val_acc_1: 0.8089 - val_acc_2: 0.8083\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1064 - loss_2: 0.1345 - acc_ensemble: 0.9790 - acc_1: 0.9480 - acc_2: 0.9680 - val_loss_1: 0.7470 - val_loss_2: 0.7150 - val_acc_ensemble: 0.8272 - val_acc_1: 0.8010 - val_acc_2: 0.8050\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1212 - loss_2: 0.1229 - acc_ensemble: 0.9770 - acc_1: 0.9620 - acc_2: 0.9700 - val_loss_1: 0.7236 - val_loss_2: 0.7393 - val_acc_ensemble: 0.8258 - val_acc_1: 0.8071 - val_acc_2: 0.8036\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1072 - loss_2: 0.1101 - acc_ensemble: 0.9770 - acc_1: 0.9530 - acc_2: 0.9660 - val_loss_1: 0.7473 - val_loss_2: 0.7403 - val_acc_ensemble: 0.8238 - val_acc_1: 0.8033 - val_acc_2: 0.8051\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1102 - loss_2: 0.1038 - acc_ensemble: 0.9850 - acc_1: 0.9600 - acc_2: 0.9690 - val_loss_1: 0.7812 - val_loss_2: 0.7442 - val_acc_ensemble: 0.8245 - val_acc_1: 0.8021 - val_acc_2: 0.8106\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1187 - loss_2: 0.0934 - acc_ensemble: 0.9830 - acc_1: 0.9640 - acc_2: 0.9680 - val_loss_1: 0.7710 - val_loss_2: 0.7385 - val_acc_ensemble: 0.8219 - val_acc_1: 0.8044 - val_acc_2: 0.8070\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.0796 - loss_2: 0.0869 - acc_ensemble: 0.9890 - acc_1: 0.9660 - acc_2: 0.9750 - val_loss_1: 0.7752 - val_loss_2: 0.7531 - val_acc_ensemble: 0.8260 - val_acc_1: 0.8005 - val_acc_2: 0.8079\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.0778 - loss_2: 0.0856 - acc_ensemble: 0.9880 - acc_1: 0.9690 - acc_2: 0.9820 - val_loss_1: 0.7814 - val_loss_2: 0.7564 - val_acc_ensemble: 0.8264 - val_acc_1: 0.8079 - val_acc_2: 0.8096\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.0855 - loss_2: 0.0844 - acc_ensemble: 0.9840 - acc_1: 0.9600 - acc_2: 0.9730 - val_loss_1: 0.8182 - val_loss_2: 0.7562 - val_acc_ensemble: 0.8246 - val_acc_1: 0.8013 - val_acc_2: 0.8097\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.1213 - loss_2: 0.0914 - acc_ensemble: 0.9800 - acc_1: 0.9580 - acc_2: 0.9740 - val_loss_1: 0.7971 - val_loss_2: 0.7782 - val_acc_ensemble: 0.8267 - val_acc_1: 0.8069 - val_acc_2: 0.8069\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.75\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,24] [None,32,32,8]   1120       input              \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,24] [None,32,32,8]   80         conv2d_1_1         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,24] [None,32,32,8]   0          bn_1_1             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,24] [None,32,32,8]   13352      relu_1_1           \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,24] [None,32,32,8]   80         conv2d_1_2         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,24] [None,32,32,8]   0          bn_1_2             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,24] [None,16,16,8]   0          relu_1_2           \n",
      "                                    [None,16,16,24] [None,16,16,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,48] [None,16,16,16]  26704      avg_pool2d_1       \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,48] [None,16,16,16]  160        conv2d_2_1         \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,48] [None,16,16,16]  0          bn_2_1             \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,48] [None,16,16,16]  53200      relu_2_1           \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,48] [None,16,16,16]  160        conv2d_2_2         \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,48] [None,16,16,16]  0          bn_2_2             \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,48] [None,8,8,16]      0          relu_2_2           \n",
      "                                    [None,8,8,48] [None,8,8,16]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,96] [None,8,8,32]      106400     avg_pool2d_2       \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,96] [None,8,8,32]      320        conv2d_3_1         \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,96] [None,8,8,32]      0          bn_3_1             \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,96] [None,8,8,32]      212384     relu_3_1           \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,96] [None,8,8,32]      320        conv2d_3_2         \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,96] [None,8,8,32]      0          bn_3_2             \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,96] [None,32]              0          relu_3_2           \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,96] [None,32]              23968      global_avg_pool2d  \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,96] [None,32]              320        fc1                \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,96] [None,32]              0          bn_fc1             \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        1921       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 440489\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.6549 - loss_2: 1.6639 - acc_ensemble: 0.5350 - acc_1: 0.5180 - acc_2: 0.5110 - val_loss_1: 1.3700 - val_loss_2: 1.3588 - val_acc_ensemble: 0.5215 - val_acc_1: 0.5008 - val_acc_2: 0.5059\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.2422 - loss_2: 1.2628 - acc_ensemble: 0.6110 - acc_1: 0.5670 - acc_2: 0.5800 - val_loss_1: 1.1997 - val_loss_2: 1.1822 - val_acc_ensemble: 0.5888 - val_acc_1: 0.5644 - val_acc_2: 0.5702\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.0781 - loss_2: 1.1153 - acc_ensemble: 0.6910 - acc_1: 0.6650 - acc_2: 0.6420 - val_loss_1: 1.0215 - val_loss_2: 1.0477 - val_acc_ensemble: 0.6497 - val_acc_1: 0.6309 - val_acc_2: 0.6246\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 0.9631 - loss_2: 0.9728 - acc_ensemble: 0.7080 - acc_1: 0.7070 - acc_2: 0.6660 - val_loss_1: 0.9168 - val_loss_2: 0.9766 - val_acc_ensemble: 0.6833 - val_acc_1: 0.6729 - val_acc_2: 0.6471\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.8723 - loss_2: 0.8863 - acc_ensemble: 0.7240 - acc_1: 0.7110 - acc_2: 0.6760 - val_loss_1: 0.8907 - val_loss_2: 0.9194 - val_acc_ensemble: 0.6967 - val_acc_1: 0.6813 - val_acc_2: 0.6719\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8072 - loss_2: 0.8200 - acc_ensemble: 0.7530 - acc_1: 0.7460 - acc_2: 0.7300 - val_loss_1: 0.8391 - val_loss_2: 0.8533 - val_acc_ensemble: 0.7189 - val_acc_1: 0.7059 - val_acc_2: 0.7027\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.7475 - loss_2: 0.7545 - acc_ensemble: 0.7660 - acc_1: 0.7510 - acc_2: 0.7470 - val_loss_1: 0.7776 - val_loss_2: 0.7867 - val_acc_ensemble: 0.7429 - val_acc_1: 0.7254 - val_acc_2: 0.7253\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7015 - loss_2: 0.7015 - acc_ensemble: 0.7970 - acc_1: 0.7770 - acc_2: 0.7620 - val_loss_1: 0.7757 - val_loss_2: 0.8003 - val_acc_ensemble: 0.7449 - val_acc_1: 0.7230 - val_acc_2: 0.7179\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.6630 - loss_2: 0.6615 - acc_ensemble: 0.7960 - acc_1: 0.7940 - acc_2: 0.7800 - val_loss_1: 0.7401 - val_loss_2: 0.7369 - val_acc_ensemble: 0.7626 - val_acc_1: 0.7448 - val_acc_2: 0.7406\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6239 - loss_2: 0.5988 - acc_ensemble: 0.8000 - acc_1: 0.8030 - acc_2: 0.7930 - val_loss_1: 0.6955 - val_loss_2: 0.7009 - val_acc_ensemble: 0.7698 - val_acc_1: 0.7550 - val_acc_2: 0.7587\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.5696 - loss_2: 0.6032 - acc_ensemble: 0.8280 - acc_1: 0.8210 - acc_2: 0.8080 - val_loss_1: 0.6906 - val_loss_2: 0.6960 - val_acc_ensemble: 0.7787 - val_acc_1: 0.7590 - val_acc_2: 0.7610\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.5599 - loss_2: 0.5580 - acc_ensemble: 0.8430 - acc_1: 0.8280 - acc_2: 0.8200 - val_loss_1: 0.6776 - val_loss_2: 0.6818 - val_acc_ensemble: 0.7830 - val_acc_1: 0.7633 - val_acc_2: 0.7655\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5256 - loss_2: 0.5396 - acc_ensemble: 0.8410 - acc_1: 0.8220 - acc_2: 0.8240 - val_loss_1: 0.6776 - val_loss_2: 0.6777 - val_acc_ensemble: 0.7849 - val_acc_1: 0.7632 - val_acc_2: 0.7686\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.4947 - loss_2: 0.5075 - acc_ensemble: 0.8550 - acc_1: 0.8280 - acc_2: 0.8390 - val_loss_1: 0.6631 - val_loss_2: 0.6566 - val_acc_ensemble: 0.7905 - val_acc_1: 0.7667 - val_acc_2: 0.7725\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.4541 - loss_2: 0.4841 - acc_ensemble: 0.8780 - acc_1: 0.8630 - acc_2: 0.8360 - val_loss_1: 0.6372 - val_loss_2: 0.6484 - val_acc_ensemble: 0.7974 - val_acc_1: 0.7815 - val_acc_2: 0.7781\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.4471 - loss_2: 0.4247 - acc_ensemble: 0.8790 - acc_1: 0.8720 - acc_2: 0.8540 - val_loss_1: 0.6589 - val_loss_2: 0.6567 - val_acc_ensemble: 0.7952 - val_acc_1: 0.7782 - val_acc_2: 0.7762\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4160 - loss_2: 0.4326 - acc_ensemble: 0.9020 - acc_1: 0.8750 - acc_2: 0.8590 - val_loss_1: 0.6329 - val_loss_2: 0.6363 - val_acc_ensemble: 0.8054 - val_acc_1: 0.7853 - val_acc_2: 0.7863\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.3907 - loss_2: 0.4110 - acc_ensemble: 0.8900 - acc_1: 0.8640 - acc_2: 0.8720 - val_loss_1: 0.6553 - val_loss_2: 0.6297 - val_acc_ensemble: 0.8044 - val_acc_1: 0.7819 - val_acc_2: 0.7860\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.4033 - loss_2: 0.3880 - acc_ensemble: 0.9160 - acc_1: 0.8980 - acc_2: 0.8890 - val_loss_1: 0.6148 - val_loss_2: 0.6196 - val_acc_ensemble: 0.8099 - val_acc_1: 0.7909 - val_acc_2: 0.7908\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.3620 - loss_2: 0.3826 - acc_ensemble: 0.9140 - acc_1: 0.8910 - acc_2: 0.8860 - val_loss_1: 0.6160 - val_loss_2: 0.6336 - val_acc_ensemble: 0.8102 - val_acc_1: 0.7986 - val_acc_2: 0.7918\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3137 - loss_2: 0.3706 - acc_ensemble: 0.9200 - acc_1: 0.9000 - acc_2: 0.8960 - val_loss_1: 0.6452 - val_loss_2: 0.6233 - val_acc_ensemble: 0.8076 - val_acc_1: 0.7876 - val_acc_2: 0.7921\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3420 - loss_2: 0.3380 - acc_ensemble: 0.9280 - acc_1: 0.8990 - acc_2: 0.8880 - val_loss_1: 0.6166 - val_loss_2: 0.6177 - val_acc_ensemble: 0.8169 - val_acc_1: 0.7976 - val_acc_2: 0.7944\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3256 - loss_2: 0.3170 - acc_ensemble: 0.9320 - acc_1: 0.9130 - acc_2: 0.9090 - val_loss_1: 0.6235 - val_loss_2: 0.6174 - val_acc_ensemble: 0.8174 - val_acc_1: 0.7963 - val_acc_2: 0.8029\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.3057 - loss_2: 0.3038 - acc_ensemble: 0.9340 - acc_1: 0.9140 - acc_2: 0.9080 - val_loss_1: 0.6314 - val_loss_2: 0.6158 - val_acc_ensemble: 0.8212 - val_acc_1: 0.7973 - val_acc_2: 0.8006\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.2905 - loss_2: 0.2910 - acc_ensemble: 0.9430 - acc_1: 0.9240 - acc_2: 0.9210 - val_loss_1: 0.6179 - val_loss_2: 0.6173 - val_acc_ensemble: 0.8167 - val_acc_1: 0.7976 - val_acc_2: 0.8004\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.2773 - loss_2: 0.2653 - acc_ensemble: 0.9450 - acc_1: 0.9230 - acc_2: 0.9230 - val_loss_1: 0.6398 - val_loss_2: 0.6261 - val_acc_ensemble: 0.8190 - val_acc_1: 0.7955 - val_acc_2: 0.7993\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2478 - loss_2: 0.2424 - acc_ensemble: 0.9500 - acc_1: 0.9250 - acc_2: 0.9260 - val_loss_1: 0.6391 - val_loss_2: 0.6335 - val_acc_ensemble: 0.8206 - val_acc_1: 0.8007 - val_acc_2: 0.8000\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2514 - loss_2: 0.2227 - acc_ensemble: 0.9560 - acc_1: 0.9290 - acc_2: 0.9170 - val_loss_1: 0.6388 - val_loss_2: 0.6593 - val_acc_ensemble: 0.8181 - val_acc_1: 0.7988 - val_acc_2: 0.7941\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2416 - loss_2: 0.2347 - acc_ensemble: 0.9590 - acc_1: 0.9350 - acc_2: 0.9460 - val_loss_1: 0.6438 - val_loss_2: 0.6408 - val_acc_ensemble: 0.8170 - val_acc_1: 0.7963 - val_acc_2: 0.7986\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2357 - loss_2: 0.2312 - acc_ensemble: 0.9430 - acc_1: 0.9290 - acc_2: 0.9290 - val_loss_1: 0.6596 - val_loss_2: 0.6454 - val_acc_ensemble: 0.8202 - val_acc_1: 0.7956 - val_acc_2: 0.7986\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2076 - loss_2: 0.1882 - acc_ensemble: 0.9500 - acc_1: 0.9280 - acc_2: 0.9340 - val_loss_1: 0.6506 - val_loss_2: 0.6479 - val_acc_ensemble: 0.8212 - val_acc_1: 0.8016 - val_acc_2: 0.8079\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.2044 - loss_2: 0.2075 - acc_ensemble: 0.9410 - acc_1: 0.9250 - acc_2: 0.9150 - val_loss_1: 0.6581 - val_loss_2: 0.6761 - val_acc_ensemble: 0.8221 - val_acc_1: 0.8019 - val_acc_2: 0.7970\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.1825 - loss_2: 0.1851 - acc_ensemble: 0.9660 - acc_1: 0.9540 - acc_2: 0.9410 - val_loss_1: 0.6530 - val_loss_2: 0.6773 - val_acc_ensemble: 0.8246 - val_acc_1: 0.8058 - val_acc_2: 0.8003\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.1746 - loss_2: 0.1876 - acc_ensemble: 0.9680 - acc_1: 0.9530 - acc_2: 0.9360 - val_loss_1: 0.6571 - val_loss_2: 0.6684 - val_acc_ensemble: 0.8243 - val_acc_1: 0.8060 - val_acc_2: 0.8018\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.1849 - loss_2: 0.1813 - acc_ensemble: 0.9680 - acc_1: 0.9430 - acc_2: 0.9430 - val_loss_1: 0.6797 - val_loss_2: 0.6734 - val_acc_ensemble: 0.8268 - val_acc_1: 0.8005 - val_acc_2: 0.8046\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.1782 - loss_2: 0.1856 - acc_ensemble: 0.9620 - acc_1: 0.9360 - acc_2: 0.9440 - val_loss_1: 0.6844 - val_loss_2: 0.6771 - val_acc_ensemble: 0.8208 - val_acc_1: 0.8009 - val_acc_2: 0.8060\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.1544 - loss_2: 0.1644 - acc_ensemble: 0.9710 - acc_1: 0.9510 - acc_2: 0.9590 - val_loss_1: 0.6891 - val_loss_2: 0.6586 - val_acc_ensemble: 0.8281 - val_acc_1: 0.8040 - val_acc_2: 0.8093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1461 - loss_2: 0.1406 - acc_ensemble: 0.9670 - acc_1: 0.9560 - acc_2: 0.9520 - val_loss_1: 0.7081 - val_loss_2: 0.6897 - val_acc_ensemble: 0.8256 - val_acc_1: 0.8002 - val_acc_2: 0.8050\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1535 - loss_2: 0.1594 - acc_ensemble: 0.9780 - acc_1: 0.9610 - acc_2: 0.9450 - val_loss_1: 0.6980 - val_loss_2: 0.6934 - val_acc_ensemble: 0.8267 - val_acc_1: 0.8062 - val_acc_2: 0.8078\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1425 - loss_2: 0.1354 - acc_ensemble: 0.9770 - acc_1: 0.9640 - acc_2: 0.9560 - val_loss_1: 0.7171 - val_loss_2: 0.7112 - val_acc_ensemble: 0.8204 - val_acc_1: 0.8006 - val_acc_2: 0.8012\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1151 - loss_2: 0.1239 - acc_ensemble: 0.9750 - acc_1: 0.9500 - acc_2: 0.9470 - val_loss_1: 0.7215 - val_loss_2: 0.7244 - val_acc_ensemble: 0.8260 - val_acc_1: 0.8058 - val_acc_2: 0.8079\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1122 - loss_2: 0.1330 - acc_ensemble: 0.9810 - acc_1: 0.9690 - acc_2: 0.9550 - val_loss_1: 0.7382 - val_loss_2: 0.7130 - val_acc_ensemble: 0.8248 - val_acc_1: 0.7992 - val_acc_2: 0.8079\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1047 - loss_2: 0.1034 - acc_ensemble: 0.9870 - acc_1: 0.9650 - acc_2: 0.9730 - val_loss_1: 0.7342 - val_loss_2: 0.7145 - val_acc_ensemble: 0.8293 - val_acc_1: 0.8055 - val_acc_2: 0.8092\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1116 - loss_2: 0.1115 - acc_ensemble: 0.9710 - acc_1: 0.9590 - acc_2: 0.9580 - val_loss_1: 0.7495 - val_loss_2: 0.7397 - val_acc_ensemble: 0.8251 - val_acc_1: 0.8034 - val_acc_2: 0.8096\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1110 - loss_2: 0.1013 - acc_ensemble: 0.9860 - acc_1: 0.9690 - acc_2: 0.9580 - val_loss_1: 0.7350 - val_loss_2: 0.7660 - val_acc_ensemble: 0.8283 - val_acc_1: 0.8073 - val_acc_2: 0.8041\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.0961 - loss_2: 0.1022 - acc_ensemble: 0.9800 - acc_1: 0.9560 - acc_2: 0.9580 - val_loss_1: 0.8001 - val_loss_2: 0.7813 - val_acc_ensemble: 0.8216 - val_acc_1: 0.7977 - val_acc_2: 0.8032\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.1066 - loss_2: 0.0986 - acc_ensemble: 0.9850 - acc_1: 0.9650 - acc_2: 0.9690 - val_loss_1: 0.7661 - val_loss_2: 0.7713 - val_acc_ensemble: 0.8276 - val_acc_1: 0.8063 - val_acc_2: 0.8097\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.0802 - loss_2: 0.0896 - acc_ensemble: 0.9780 - acc_1: 0.9660 - acc_2: 0.9610 - val_loss_1: 0.7684 - val_loss_2: 0.7770 - val_acc_ensemble: 0.8279 - val_acc_1: 0.8072 - val_acc_2: 0.8126\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.0962 - loss_2: 0.0886 - acc_ensemble: 0.9820 - acc_1: 0.9700 - acc_2: 0.9670 - val_loss_1: 0.7953 - val_loss_2: 0.7892 - val_acc_ensemble: 0.8274 - val_acc_1: 0.8037 - val_acc_2: 0.8068\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.0856 - loss_2: 0.0798 - acc_ensemble: 0.9830 - acc_1: 0.9640 - acc_2: 0.9730 - val_loss_1: 0.7743 - val_loss_2: 0.7816 - val_acc_ensemble: 0.8303 - val_acc_1: 0.8095 - val_acc_2: 0.8116\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S0.75\n",
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,24] [None,32,32,8]   1120       input              \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,24] [None,32,32,8]   80         conv2d_1_1         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,24] [None,32,32,8]   0          bn_1_1             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,24] [None,32,32,8]   13352      relu_1_1           \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,24] [None,32,32,8]   80         conv2d_1_2         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,24] [None,32,32,8]   0          bn_1_2             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,24] [None,16,16,8]   0          relu_1_2           \n",
      "                                    [None,16,16,24] [None,16,16,8]                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,48] [None,16,16,16]  26704      avg_pool2d_1       \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,48] [None,16,16,16]  160        conv2d_2_1         \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,48] [None,16,16,16]  0          bn_2_1             \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,48] [None,16,16,16]  53200      relu_2_1           \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,48] [None,16,16,16]  160        conv2d_2_2         \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,48] [None,16,16,16]  0          bn_2_2             \n",
      "                                    [None,16,16,48] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,48] [None,8,8,16]      0          relu_2_2           \n",
      "                                    [None,8,8,48] [None,8,8,16]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,96] [None,8,8,32]      106400     avg_pool2d_2       \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,96] [None,8,8,32]      320        conv2d_3_1         \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,96] [None,8,8,32]      0          bn_3_1             \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,96] [None,8,8,32]      212384     relu_3_1           \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,96] [None,8,8,32]      320        conv2d_3_2         \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,96] [None,8,8,32]      0          bn_3_2             \n",
      "                                    [None,8,8,96] [None,8,8,32]                                    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,96] [None,32]              0          relu_3_2           \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,96] [None,32]              23968      global_avg_pool2d  \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,96] [None,32]              320        fc1                \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,96] [None,32]              0          bn_fc1             \n",
      "                                    [None,96] [None,32]                                            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]                        1921       relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 440489\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 16s - loss_1: 1.7046 - loss_2: 1.6818 - acc_ensemble: 0.5150 - acc_1: 0.4870 - acc_2: 0.5010 - val_loss_1: 1.4312 - val_loss_2: 1.4322 - val_acc_ensemble: 0.4962 - val_acc_1: 0.4725 - val_acc_2: 0.4740\n",
      "Epoch 2/50\n",
      "100/100 - 11s - loss_1: 1.3215 - loss_2: 1.3183 - acc_ensemble: 0.5970 - acc_1: 0.5640 - acc_2: 0.5540 - val_loss_1: 1.2419 - val_loss_2: 1.2538 - val_acc_ensemble: 0.5772 - val_acc_1: 0.5538 - val_acc_2: 0.5464\n",
      "Epoch 3/50\n",
      "100/100 - 11s - loss_1: 1.1518 - loss_2: 1.1349 - acc_ensemble: 0.6510 - acc_1: 0.6320 - acc_2: 0.6170 - val_loss_1: 1.0789 - val_loss_2: 1.0806 - val_acc_ensemble: 0.6345 - val_acc_1: 0.6122 - val_acc_2: 0.6137\n",
      "Epoch 4/50\n",
      "100/100 - 11s - loss_1: 1.0077 - loss_2: 1.0084 - acc_ensemble: 0.6830 - acc_1: 0.6630 - acc_2: 0.6570 - val_loss_1: 1.0265 - val_loss_2: 0.9877 - val_acc_ensemble: 0.6636 - val_acc_1: 0.6294 - val_acc_2: 0.6470\n",
      "Epoch 5/50\n",
      "100/100 - 11s - loss_1: 0.9198 - loss_2: 0.9221 - acc_ensemble: 0.7260 - acc_1: 0.7250 - acc_2: 0.6930 - val_loss_1: 0.9007 - val_loss_2: 0.9307 - val_acc_ensemble: 0.6926 - val_acc_1: 0.6806 - val_acc_2: 0.6649\n",
      "Epoch 6/50\n",
      "100/100 - 11s - loss_1: 0.8192 - loss_2: 0.8601 - acc_ensemble: 0.7480 - acc_1: 0.7310 - acc_2: 0.7190 - val_loss_1: 0.8523 - val_loss_2: 0.8500 - val_acc_ensemble: 0.7199 - val_acc_1: 0.6990 - val_acc_2: 0.6997\n",
      "Epoch 7/50\n",
      "100/100 - 11s - loss_1: 0.7807 - loss_2: 0.7855 - acc_ensemble: 0.7500 - acc_1: 0.7350 - acc_2: 0.7230 - val_loss_1: 0.8171 - val_loss_2: 0.8491 - val_acc_ensemble: 0.7262 - val_acc_1: 0.7153 - val_acc_2: 0.6982\n",
      "Epoch 8/50\n",
      "100/100 - 11s - loss_1: 0.7302 - loss_2: 0.7620 - acc_ensemble: 0.7860 - acc_1: 0.7480 - acc_2: 0.7510 - val_loss_1: 0.7962 - val_loss_2: 0.8080 - val_acc_ensemble: 0.7395 - val_acc_1: 0.7207 - val_acc_2: 0.7195\n",
      "Epoch 9/50\n",
      "100/100 - 11s - loss_1: 0.6570 - loss_2: 0.6938 - acc_ensemble: 0.7940 - acc_1: 0.7630 - acc_2: 0.7700 - val_loss_1: 0.7634 - val_loss_2: 0.7772 - val_acc_ensemble: 0.7488 - val_acc_1: 0.7316 - val_acc_2: 0.7273\n",
      "Epoch 10/50\n",
      "100/100 - 11s - loss_1: 0.6398 - loss_2: 0.6483 - acc_ensemble: 0.8070 - acc_1: 0.7950 - acc_2: 0.7690 - val_loss_1: 0.7176 - val_loss_2: 0.7434 - val_acc_ensemble: 0.7605 - val_acc_1: 0.7479 - val_acc_2: 0.7384\n",
      "Epoch 11/50\n",
      "100/100 - 11s - loss_1: 0.6008 - loss_2: 0.6242 - acc_ensemble: 0.8290 - acc_1: 0.8030 - acc_2: 0.8020 - val_loss_1: 0.7172 - val_loss_2: 0.7100 - val_acc_ensemble: 0.7738 - val_acc_1: 0.7532 - val_acc_2: 0.7512\n",
      "Epoch 12/50\n",
      "100/100 - 11s - loss_1: 0.5823 - loss_2: 0.5971 - acc_ensemble: 0.8250 - acc_1: 0.8090 - acc_2: 0.8030 - val_loss_1: 0.7041 - val_loss_2: 0.6943 - val_acc_ensemble: 0.7737 - val_acc_1: 0.7526 - val_acc_2: 0.7570\n",
      "Epoch 13/50\n",
      "100/100 - 11s - loss_1: 0.5436 - loss_2: 0.5564 - acc_ensemble: 0.8280 - acc_1: 0.7920 - acc_2: 0.8170 - val_loss_1: 0.6982 - val_loss_2: 0.7072 - val_acc_ensemble: 0.7779 - val_acc_1: 0.7556 - val_acc_2: 0.7569\n",
      "Epoch 14/50\n",
      "100/100 - 11s - loss_1: 0.5173 - loss_2: 0.5208 - acc_ensemble: 0.8410 - acc_1: 0.8130 - acc_2: 0.8300 - val_loss_1: 0.6689 - val_loss_2: 0.6732 - val_acc_ensemble: 0.7851 - val_acc_1: 0.7709 - val_acc_2: 0.7655\n",
      "Epoch 15/50\n",
      "100/100 - 11s - loss_1: 0.4689 - loss_2: 0.4890 - acc_ensemble: 0.8540 - acc_1: 0.8440 - acc_2: 0.8320 - val_loss_1: 0.6732 - val_loss_2: 0.6580 - val_acc_ensemble: 0.7903 - val_acc_1: 0.7685 - val_acc_2: 0.7739\n",
      "Epoch 16/50\n",
      "100/100 - 11s - loss_1: 0.4637 - loss_2: 0.4958 - acc_ensemble: 0.8620 - acc_1: 0.8320 - acc_2: 0.8350 - val_loss_1: 0.6499 - val_loss_2: 0.6718 - val_acc_ensemble: 0.7940 - val_acc_1: 0.7752 - val_acc_2: 0.7701\n",
      "Epoch 17/50\n",
      "100/100 - 11s - loss_1: 0.4457 - loss_2: 0.4339 - acc_ensemble: 0.8680 - acc_1: 0.8550 - acc_2: 0.8580 - val_loss_1: 0.6429 - val_loss_2: 0.6512 - val_acc_ensemble: 0.8022 - val_acc_1: 0.7842 - val_acc_2: 0.7784\n",
      "Epoch 18/50\n",
      "100/100 - 11s - loss_1: 0.4134 - loss_2: 0.4235 - acc_ensemble: 0.8810 - acc_1: 0.8470 - acc_2: 0.8550 - val_loss_1: 0.6489 - val_loss_2: 0.6475 - val_acc_ensemble: 0.8000 - val_acc_1: 0.7842 - val_acc_2: 0.7820\n",
      "Epoch 19/50\n",
      "100/100 - 11s - loss_1: 0.3799 - loss_2: 0.3977 - acc_ensemble: 0.8810 - acc_1: 0.8590 - acc_2: 0.8580 - val_loss_1: 0.6482 - val_loss_2: 0.6526 - val_acc_ensemble: 0.8012 - val_acc_1: 0.7841 - val_acc_2: 0.7796\n",
      "Epoch 20/50\n",
      "100/100 - 11s - loss_1: 0.3716 - loss_2: 0.3871 - acc_ensemble: 0.8950 - acc_1: 0.8580 - acc_2: 0.8790 - val_loss_1: 0.6303 - val_loss_2: 0.6482 - val_acc_ensemble: 0.8087 - val_acc_1: 0.7836 - val_acc_2: 0.7858\n",
      "Epoch 21/50\n",
      "100/100 - 11s - loss_1: 0.3585 - loss_2: 0.3499 - acc_ensemble: 0.9100 - acc_1: 0.8840 - acc_2: 0.8770 - val_loss_1: 0.6310 - val_loss_2: 0.6271 - val_acc_ensemble: 0.8074 - val_acc_1: 0.7882 - val_acc_2: 0.7923\n",
      "Epoch 22/50\n",
      "100/100 - 11s - loss_1: 0.3512 - loss_2: 0.3600 - acc_ensemble: 0.9060 - acc_1: 0.8790 - acc_2: 0.8710 - val_loss_1: 0.6324 - val_loss_2: 0.6194 - val_acc_ensemble: 0.8112 - val_acc_1: 0.7908 - val_acc_2: 0.7934\n",
      "Epoch 23/50\n",
      "100/100 - 11s - loss_1: 0.3600 - loss_2: 0.3416 - acc_ensemble: 0.9070 - acc_1: 0.8880 - acc_2: 0.8690 - val_loss_1: 0.6238 - val_loss_2: 0.6419 - val_acc_ensemble: 0.8104 - val_acc_1: 0.7920 - val_acc_2: 0.7919\n",
      "Epoch 24/50\n",
      "100/100 - 11s - loss_1: 0.2940 - loss_2: 0.3254 - acc_ensemble: 0.9110 - acc_1: 0.8930 - acc_2: 0.8840 - val_loss_1: 0.6304 - val_loss_2: 0.6230 - val_acc_ensemble: 0.8153 - val_acc_1: 0.7960 - val_acc_2: 0.7931\n",
      "Epoch 25/50\n",
      "100/100 - 11s - loss_1: 0.3096 - loss_2: 0.2977 - acc_ensemble: 0.9270 - acc_1: 0.9050 - acc_2: 0.8980 - val_loss_1: 0.6191 - val_loss_2: 0.6258 - val_acc_ensemble: 0.8170 - val_acc_1: 0.8020 - val_acc_2: 0.7971\n",
      "Epoch 26/50\n",
      "100/100 - 11s - loss_1: 0.2606 - loss_2: 0.3064 - acc_ensemble: 0.9270 - acc_1: 0.9000 - acc_2: 0.9060 - val_loss_1: 0.6484 - val_loss_2: 0.6561 - val_acc_ensemble: 0.8120 - val_acc_1: 0.7915 - val_acc_2: 0.7885\n",
      "Epoch 27/50\n",
      "100/100 - 11s - loss_1: 0.2819 - loss_2: 0.2714 - acc_ensemble: 0.9260 - acc_1: 0.9070 - acc_2: 0.8980 - val_loss_1: 0.6272 - val_loss_2: 0.6238 - val_acc_ensemble: 0.8192 - val_acc_1: 0.8014 - val_acc_2: 0.7993\n",
      "Epoch 28/50\n",
      "100/100 - 11s - loss_1: 0.2585 - loss_2: 0.2875 - acc_ensemble: 0.9430 - acc_1: 0.9180 - acc_2: 0.9130 - val_loss_1: 0.6366 - val_loss_2: 0.6446 - val_acc_ensemble: 0.8193 - val_acc_1: 0.7975 - val_acc_2: 0.7970\n",
      "Epoch 29/50\n",
      "100/100 - 11s - loss_1: 0.2348 - loss_2: 0.2708 - acc_ensemble: 0.9440 - acc_1: 0.9290 - acc_2: 0.9130 - val_loss_1: 0.6339 - val_loss_2: 0.6299 - val_acc_ensemble: 0.8244 - val_acc_1: 0.7991 - val_acc_2: 0.8013\n",
      "Epoch 30/50\n",
      "100/100 - 11s - loss_1: 0.2208 - loss_2: 0.2208 - acc_ensemble: 0.9320 - acc_1: 0.9150 - acc_2: 0.9150 - val_loss_1: 0.6466 - val_loss_2: 0.6384 - val_acc_ensemble: 0.8238 - val_acc_1: 0.7974 - val_acc_2: 0.8002\n",
      "Epoch 31/50\n",
      "100/100 - 11s - loss_1: 0.2432 - loss_2: 0.2233 - acc_ensemble: 0.9400 - acc_1: 0.9200 - acc_2: 0.9080 - val_loss_1: 0.6588 - val_loss_2: 0.6693 - val_acc_ensemble: 0.8157 - val_acc_1: 0.7954 - val_acc_2: 0.7917\n",
      "Epoch 32/50\n",
      "100/100 - 11s - loss_1: 0.1946 - loss_2: 0.2352 - acc_ensemble: 0.9490 - acc_1: 0.9280 - acc_2: 0.9340 - val_loss_1: 0.6538 - val_loss_2: 0.6345 - val_acc_ensemble: 0.8247 - val_acc_1: 0.8008 - val_acc_2: 0.8039\n",
      "Epoch 33/50\n",
      "100/100 - 11s - loss_1: 0.2120 - loss_2: 0.2202 - acc_ensemble: 0.9450 - acc_1: 0.9230 - acc_2: 0.9370 - val_loss_1: 0.6749 - val_loss_2: 0.6479 - val_acc_ensemble: 0.8192 - val_acc_1: 0.7971 - val_acc_2: 0.8045\n",
      "Epoch 34/50\n",
      "100/100 - 11s - loss_1: 0.1891 - loss_2: 0.2140 - acc_ensemble: 0.9520 - acc_1: 0.9360 - acc_2: 0.9360 - val_loss_1: 0.6759 - val_loss_2: 0.6745 - val_acc_ensemble: 0.8182 - val_acc_1: 0.7953 - val_acc_2: 0.7964\n",
      "Epoch 35/50\n",
      "100/100 - 11s - loss_1: 0.1691 - loss_2: 0.1712 - acc_ensemble: 0.9630 - acc_1: 0.9230 - acc_2: 0.9430 - val_loss_1: 0.6926 - val_loss_2: 0.6534 - val_acc_ensemble: 0.8222 - val_acc_1: 0.7997 - val_acc_2: 0.8062\n",
      "Epoch 36/50\n",
      "100/100 - 11s - loss_1: 0.1681 - loss_2: 0.1567 - acc_ensemble: 0.9640 - acc_1: 0.9350 - acc_2: 0.9320 - val_loss_1: 0.6822 - val_loss_2: 0.6551 - val_acc_ensemble: 0.8225 - val_acc_1: 0.8023 - val_acc_2: 0.8036\n",
      "Epoch 37/50\n",
      "100/100 - 11s - loss_1: 0.2029 - loss_2: 0.1657 - acc_ensemble: 0.9680 - acc_1: 0.9410 - acc_2: 0.9500 - val_loss_1: 0.6777 - val_loss_2: 0.6712 - val_acc_ensemble: 0.8283 - val_acc_1: 0.8028 - val_acc_2: 0.8095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 11s - loss_1: 0.1758 - loss_2: 0.1761 - acc_ensemble: 0.9780 - acc_1: 0.9530 - acc_2: 0.9520 - val_loss_1: 0.6597 - val_loss_2: 0.6816 - val_acc_ensemble: 0.8275 - val_acc_1: 0.8084 - val_acc_2: 0.8063\n",
      "Epoch 39/50\n",
      "100/100 - 11s - loss_1: 0.1475 - loss_2: 0.1510 - acc_ensemble: 0.9630 - acc_1: 0.9480 - acc_2: 0.9430 - val_loss_1: 0.6829 - val_loss_2: 0.6860 - val_acc_ensemble: 0.8289 - val_acc_1: 0.8068 - val_acc_2: 0.8055\n",
      "Epoch 40/50\n",
      "100/100 - 11s - loss_1: 0.1482 - loss_2: 0.1412 - acc_ensemble: 0.9780 - acc_1: 0.9570 - acc_2: 0.9590 - val_loss_1: 0.6864 - val_loss_2: 0.6925 - val_acc_ensemble: 0.8283 - val_acc_1: 0.8068 - val_acc_2: 0.8086\n",
      "Epoch 41/50\n",
      "100/100 - 11s - loss_1: 0.1266 - loss_2: 0.1413 - acc_ensemble: 0.9680 - acc_1: 0.9510 - acc_2: 0.9520 - val_loss_1: 0.7118 - val_loss_2: 0.7133 - val_acc_ensemble: 0.8273 - val_acc_1: 0.7994 - val_acc_2: 0.8078\n",
      "Epoch 42/50\n",
      "100/100 - 11s - loss_1: 0.1268 - loss_2: 0.1551 - acc_ensemble: 0.9640 - acc_1: 0.9590 - acc_2: 0.9410 - val_loss_1: 0.7213 - val_loss_2: 0.7427 - val_acc_ensemble: 0.8215 - val_acc_1: 0.8027 - val_acc_2: 0.7972\n",
      "Epoch 43/50\n",
      "100/100 - 11s - loss_1: 0.1215 - loss_2: 0.1277 - acc_ensemble: 0.9800 - acc_1: 0.9660 - acc_2: 0.9610 - val_loss_1: 0.7086 - val_loss_2: 0.7287 - val_acc_ensemble: 0.8231 - val_acc_1: 0.8049 - val_acc_2: 0.8043\n",
      "Epoch 44/50\n",
      "100/100 - 11s - loss_1: 0.1133 - loss_2: 0.1182 - acc_ensemble: 0.9740 - acc_1: 0.9570 - acc_2: 0.9550 - val_loss_1: 0.7517 - val_loss_2: 0.7463 - val_acc_ensemble: 0.8207 - val_acc_1: 0.8000 - val_acc_2: 0.8019\n",
      "Epoch 45/50\n",
      "100/100 - 11s - loss_1: 0.1163 - loss_2: 0.1127 - acc_ensemble: 0.9840 - acc_1: 0.9660 - acc_2: 0.9570 - val_loss_1: 0.7188 - val_loss_2: 0.7324 - val_acc_ensemble: 0.8241 - val_acc_1: 0.8088 - val_acc_2: 0.8024\n",
      "Epoch 46/50\n",
      "100/100 - 11s - loss_1: 0.1062 - loss_2: 0.0995 - acc_ensemble: 0.9820 - acc_1: 0.9650 - acc_2: 0.9620 - val_loss_1: 0.7534 - val_loss_2: 0.7362 - val_acc_ensemble: 0.8276 - val_acc_1: 0.7997 - val_acc_2: 0.8086\n",
      "Epoch 47/50\n",
      "100/100 - 11s - loss_1: 0.0963 - loss_2: 0.0894 - acc_ensemble: 0.9860 - acc_1: 0.9590 - acc_2: 0.9630 - val_loss_1: 0.7670 - val_loss_2: 0.7498 - val_acc_ensemble: 0.8256 - val_acc_1: 0.8029 - val_acc_2: 0.8067\n",
      "Epoch 48/50\n",
      "100/100 - 11s - loss_1: 0.1020 - loss_2: 0.1137 - acc_ensemble: 0.9820 - acc_1: 0.9670 - acc_2: 0.9640 - val_loss_1: 0.7810 - val_loss_2: 0.7685 - val_acc_ensemble: 0.8251 - val_acc_1: 0.8015 - val_acc_2: 0.8044\n",
      "Epoch 49/50\n",
      "100/100 - 11s - loss_1: 0.0918 - loss_2: 0.1107 - acc_ensemble: 0.9840 - acc_1: 0.9710 - acc_2: 0.9670 - val_loss_1: 0.7839 - val_loss_2: 0.7835 - val_acc_ensemble: 0.8222 - val_acc_1: 0.8006 - val_acc_2: 0.8022\n",
      "Epoch 50/50\n",
      "100/100 - 11s - loss_1: 0.1068 - loss_2: 0.0995 - acc_ensemble: 0.9820 - acc_1: 0.9700 - acc_2: 0.9730 - val_loss_1: 0.7782 - val_loss_2: 0.7686 - val_acc_ensemble: 0.8267 - val_acc_1: 0.8058 - val_acc_2: 0.8049\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S1.00\n",
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,32] []  896        input              \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_1         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,32] []  0          bn_1_1             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,32] []  9248       relu_1_1           \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_2         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,32] []  0          bn_1_2             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,32] []  0          relu_1_2           \n",
      "                                    [None,16,16,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,64] []  18496      avg_pool2d_1       \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_1         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,64] []  0          bn_2_1             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,64] []  36928      relu_2_1           \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_2         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,64] []  0          bn_2_2             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,64] []    0          relu_2_2           \n",
      "                                    [None,8,8,64] []                                  \n",
      "--------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,128] []   73856      avg_pool2d_2       \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,128] []   256        conv2d_3_1         \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,128] []   0          bn_3_1             \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,128] []   147584     relu_3_1           \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,128] []   256        conv2d_3_2         \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,128] []   0          bn_3_2             \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,128] []       0          relu_3_2           \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,128] []       16512      global_avg_pool2d  \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,128] []       256        fc1                \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,128] []       0          bn_fc1             \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]           1290       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 305962\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 9s - loss_1: 1.5685 - loss_2: 1.5675 - acc_ensemble: 0.5330 - acc_1: 0.5330 - acc_2: 0.5330 - val_loss_1: 1.2953 - val_loss_2: 1.2953 - val_acc_ensemble: 0.5268 - val_acc_1: 0.5268 - val_acc_2: 0.5268\n",
      "Epoch 2/50\n",
      "100/100 - 8s - loss_1: 1.1796 - loss_2: 1.1832 - acc_ensemble: 0.6390 - acc_1: 0.6390 - acc_2: 0.6390 - val_loss_1: 1.0984 - val_loss_2: 1.0984 - val_acc_ensemble: 0.6078 - val_acc_1: 0.6078 - val_acc_2: 0.6078\n",
      "Epoch 3/50\n",
      "100/100 - 8s - loss_1: 0.9914 - loss_2: 1.0084 - acc_ensemble: 0.6780 - acc_1: 0.6780 - acc_2: 0.6780 - val_loss_1: 0.9636 - val_loss_2: 0.9636 - val_acc_ensemble: 0.6555 - val_acc_1: 0.6555 - val_acc_2: 0.6555\n",
      "Epoch 4/50\n",
      "100/100 - 8s - loss_1: 0.8850 - loss_2: 0.8847 - acc_ensemble: 0.7240 - acc_1: 0.7240 - acc_2: 0.7240 - val_loss_1: 0.9116 - val_loss_2: 0.9116 - val_acc_ensemble: 0.6793 - val_acc_1: 0.6793 - val_acc_2: 0.6793\n",
      "Epoch 5/50\n",
      "100/100 - 8s - loss_1: 0.8243 - loss_2: 0.8433 - acc_ensemble: 0.7480 - acc_1: 0.7480 - acc_2: 0.7480 - val_loss_1: 0.8657 - val_loss_2: 0.8657 - val_acc_ensemble: 0.6959 - val_acc_1: 0.6959 - val_acc_2: 0.6959\n",
      "Epoch 6/50\n",
      "100/100 - 8s - loss_1: 0.7467 - loss_2: 0.7378 - acc_ensemble: 0.7810 - acc_1: 0.7810 - acc_2: 0.7810 - val_loss_1: 0.8146 - val_loss_2: 0.8146 - val_acc_ensemble: 0.7187 - val_acc_1: 0.7187 - val_acc_2: 0.7187\n",
      "Epoch 7/50\n",
      "100/100 - 8s - loss_1: 0.6697 - loss_2: 0.6894 - acc_ensemble: 0.7760 - acc_1: 0.7760 - acc_2: 0.7760 - val_loss_1: 0.8068 - val_loss_2: 0.8068 - val_acc_ensemble: 0.7168 - val_acc_1: 0.7168 - val_acc_2: 0.7168\n",
      "Epoch 8/50\n",
      "100/100 - 8s - loss_1: 0.6439 - loss_2: 0.6337 - acc_ensemble: 0.8000 - acc_1: 0.8000 - acc_2: 0.8000 - val_loss_1: 0.7561 - val_loss_2: 0.7561 - val_acc_ensemble: 0.7372 - val_acc_1: 0.7372 - val_acc_2: 0.7372\n",
      "Epoch 9/50\n",
      "100/100 - 8s - loss_1: 0.5844 - loss_2: 0.5725 - acc_ensemble: 0.8140 - acc_1: 0.8140 - acc_2: 0.8140 - val_loss_1: 0.7615 - val_loss_2: 0.7615 - val_acc_ensemble: 0.7338 - val_acc_1: 0.7338 - val_acc_2: 0.7338\n",
      "Epoch 10/50\n",
      "100/100 - 8s - loss_1: 0.5663 - loss_2: 0.5531 - acc_ensemble: 0.8310 - acc_1: 0.8310 - acc_2: 0.8310 - val_loss_1: 0.7385 - val_loss_2: 0.7385 - val_acc_ensemble: 0.7419 - val_acc_1: 0.7419 - val_acc_2: 0.7419\n",
      "Epoch 11/50\n",
      "100/100 - 8s - loss_1: 0.5135 - loss_2: 0.5096 - acc_ensemble: 0.8320 - acc_1: 0.8320 - acc_2: 0.8320 - val_loss_1: 0.7211 - val_loss_2: 0.7211 - val_acc_ensemble: 0.7524 - val_acc_1: 0.7524 - val_acc_2: 0.7524\n",
      "Epoch 12/50\n",
      "100/100 - 8s - loss_1: 0.4695 - loss_2: 0.4567 - acc_ensemble: 0.8740 - acc_1: 0.8740 - acc_2: 0.8740 - val_loss_1: 0.7140 - val_loss_2: 0.7140 - val_acc_ensemble: 0.7559 - val_acc_1: 0.7559 - val_acc_2: 0.7559\n",
      "Epoch 13/50\n",
      "100/100 - 8s - loss_1: 0.4330 - loss_2: 0.4474 - acc_ensemble: 0.8580 - acc_1: 0.8580 - acc_2: 0.8580 - val_loss_1: 0.7336 - val_loss_2: 0.7336 - val_acc_ensemble: 0.7533 - val_acc_1: 0.7533 - val_acc_2: 0.7533\n",
      "Epoch 14/50\n",
      "100/100 - 8s - loss_1: 0.4048 - loss_2: 0.4180 - acc_ensemble: 0.8840 - acc_1: 0.8840 - acc_2: 0.8840 - val_loss_1: 0.6998 - val_loss_2: 0.6998 - val_acc_ensemble: 0.7669 - val_acc_1: 0.7669 - val_acc_2: 0.7669\n",
      "Epoch 15/50\n",
      "100/100 - 8s - loss_1: 0.3541 - loss_2: 0.3585 - acc_ensemble: 0.8700 - acc_1: 0.8700 - acc_2: 0.8700 - val_loss_1: 0.7208 - val_loss_2: 0.7208 - val_acc_ensemble: 0.7601 - val_acc_1: 0.7601 - val_acc_2: 0.7601\n",
      "Epoch 16/50\n",
      "100/100 - 8s - loss_1: 0.3333 - loss_2: 0.3510 - acc_ensemble: 0.8860 - acc_1: 0.8860 - acc_2: 0.8860 - val_loss_1: 0.6991 - val_loss_2: 0.6991 - val_acc_ensemble: 0.7712 - val_acc_1: 0.7712 - val_acc_2: 0.7712\n",
      "Epoch 17/50\n",
      "100/100 - 8s - loss_1: 0.3246 - loss_2: 0.3099 - acc_ensemble: 0.9000 - acc_1: 0.9000 - acc_2: 0.9000 - val_loss_1: 0.6964 - val_loss_2: 0.6964 - val_acc_ensemble: 0.7741 - val_acc_1: 0.7741 - val_acc_2: 0.7741\n",
      "Epoch 18/50\n",
      "100/100 - 8s - loss_1: 0.2975 - loss_2: 0.3057 - acc_ensemble: 0.9100 - acc_1: 0.9100 - acc_2: 0.9100 - val_loss_1: 0.7272 - val_loss_2: 0.7272 - val_acc_ensemble: 0.7674 - val_acc_1: 0.7674 - val_acc_2: 0.7674\n",
      "Epoch 19/50\n",
      "100/100 - 8s - loss_1: 0.2656 - loss_2: 0.2675 - acc_ensemble: 0.9120 - acc_1: 0.9120 - acc_2: 0.9120 - val_loss_1: 0.7197 - val_loss_2: 0.7197 - val_acc_ensemble: 0.7736 - val_acc_1: 0.7736 - val_acc_2: 0.7736\n",
      "Epoch 20/50\n",
      "100/100 - 8s - loss_1: 0.2447 - loss_2: 0.2473 - acc_ensemble: 0.9350 - acc_1: 0.9350 - acc_2: 0.9350 - val_loss_1: 0.7441 - val_loss_2: 0.7441 - val_acc_ensemble: 0.7674 - val_acc_1: 0.7674 - val_acc_2: 0.7674\n",
      "Epoch 21/50\n",
      "100/100 - 8s - loss_1: 0.2228 - loss_2: 0.2396 - acc_ensemble: 0.9320 - acc_1: 0.9320 - acc_2: 0.9320 - val_loss_1: 0.7629 - val_loss_2: 0.7629 - val_acc_ensemble: 0.7743 - val_acc_1: 0.7743 - val_acc_2: 0.7743\n",
      "Epoch 22/50\n",
      "100/100 - 8s - loss_1: 0.2033 - loss_2: 0.2152 - acc_ensemble: 0.9330 - acc_1: 0.9330 - acc_2: 0.9330 - val_loss_1: 0.7426 - val_loss_2: 0.7426 - val_acc_ensemble: 0.7777 - val_acc_1: 0.7777 - val_acc_2: 0.7777\n",
      "Epoch 23/50\n",
      "100/100 - 8s - loss_1: 0.1804 - loss_2: 0.1850 - acc_ensemble: 0.9340 - acc_1: 0.9340 - acc_2: 0.9340 - val_loss_1: 0.7653 - val_loss_2: 0.7653 - val_acc_ensemble: 0.7722 - val_acc_1: 0.7722 - val_acc_2: 0.7722\n",
      "Epoch 24/50\n",
      "100/100 - 8s - loss_1: 0.1884 - loss_2: 0.1797 - acc_ensemble: 0.9460 - acc_1: 0.9460 - acc_2: 0.9460 - val_loss_1: 0.7830 - val_loss_2: 0.7830 - val_acc_ensemble: 0.7713 - val_acc_1: 0.7713 - val_acc_2: 0.7713\n",
      "Epoch 25/50\n",
      "100/100 - 8s - loss_1: 0.1641 - loss_2: 0.1574 - acc_ensemble: 0.9500 - acc_1: 0.9500 - acc_2: 0.9500 - val_loss_1: 0.8029 - val_loss_2: 0.8029 - val_acc_ensemble: 0.7705 - val_acc_1: 0.7705 - val_acc_2: 0.7705\n",
      "Epoch 26/50\n",
      "100/100 - 8s - loss_1: 0.1594 - loss_2: 0.1627 - acc_ensemble: 0.9590 - acc_1: 0.9590 - acc_2: 0.9590 - val_loss_1: 0.8191 - val_loss_2: 0.8191 - val_acc_ensemble: 0.7729 - val_acc_1: 0.7729 - val_acc_2: 0.7729\n",
      "Epoch 27/50\n",
      "100/100 - 8s - loss_1: 0.1528 - loss_2: 0.1568 - acc_ensemble: 0.9640 - acc_1: 0.9640 - acc_2: 0.9640 - val_loss_1: 0.7779 - val_loss_2: 0.7779 - val_acc_ensemble: 0.7820 - val_acc_1: 0.7820 - val_acc_2: 0.7820\n",
      "Epoch 28/50\n",
      "100/100 - 8s - loss_1: 0.1385 - loss_2: 0.1298 - acc_ensemble: 0.9660 - acc_1: 0.9660 - acc_2: 0.9660 - val_loss_1: 0.8147 - val_loss_2: 0.8147 - val_acc_ensemble: 0.7798 - val_acc_1: 0.7798 - val_acc_2: 0.7798\n",
      "Epoch 29/50\n",
      "100/100 - 8s - loss_1: 0.1133 - loss_2: 0.1180 - acc_ensemble: 0.9620 - acc_1: 0.9620 - acc_2: 0.9620 - val_loss_1: 0.8330 - val_loss_2: 0.8330 - val_acc_ensemble: 0.7798 - val_acc_1: 0.7798 - val_acc_2: 0.7798\n",
      "Epoch 30/50\n",
      "100/100 - 8s - loss_1: 0.1051 - loss_2: 0.0946 - acc_ensemble: 0.9670 - acc_1: 0.9670 - acc_2: 0.9670 - val_loss_1: 0.8421 - val_loss_2: 0.8421 - val_acc_ensemble: 0.7792 - val_acc_1: 0.7792 - val_acc_2: 0.7792\n",
      "Epoch 31/50\n",
      "100/100 - 8s - loss_1: 0.1108 - loss_2: 0.1125 - acc_ensemble: 0.9630 - acc_1: 0.9630 - acc_2: 0.9630 - val_loss_1: 0.8551 - val_loss_2: 0.8551 - val_acc_ensemble: 0.7814 - val_acc_1: 0.7814 - val_acc_2: 0.7814\n",
      "Epoch 32/50\n",
      "100/100 - 8s - loss_1: 0.1046 - loss_2: 0.0947 - acc_ensemble: 0.9740 - acc_1: 0.9740 - acc_2: 0.9740 - val_loss_1: 0.8562 - val_loss_2: 0.8562 - val_acc_ensemble: 0.7776 - val_acc_1: 0.7776 - val_acc_2: 0.7776\n",
      "Epoch 33/50\n",
      "100/100 - 8s - loss_1: 0.0862 - loss_2: 0.0979 - acc_ensemble: 0.9790 - acc_1: 0.9790 - acc_2: 0.9790 - val_loss_1: 0.8750 - val_loss_2: 0.8750 - val_acc_ensemble: 0.7804 - val_acc_1: 0.7804 - val_acc_2: 0.7804\n",
      "Epoch 34/50\n",
      "100/100 - 8s - loss_1: 0.0764 - loss_2: 0.0736 - acc_ensemble: 0.9650 - acc_1: 0.9650 - acc_2: 0.9650 - val_loss_1: 0.9105 - val_loss_2: 0.9105 - val_acc_ensemble: 0.7791 - val_acc_1: 0.7791 - val_acc_2: 0.7791\n",
      "Epoch 35/50\n",
      "100/100 - 8s - loss_1: 0.0853 - loss_2: 0.0787 - acc_ensemble: 0.9750 - acc_1: 0.9750 - acc_2: 0.9750 - val_loss_1: 0.9048 - val_loss_2: 0.9048 - val_acc_ensemble: 0.7787 - val_acc_1: 0.7787 - val_acc_2: 0.7787\n",
      "Epoch 36/50\n",
      "100/100 - 8s - loss_1: 0.0718 - loss_2: 0.0745 - acc_ensemble: 0.9740 - acc_1: 0.9740 - acc_2: 0.9740 - val_loss_1: 0.9385 - val_loss_2: 0.9385 - val_acc_ensemble: 0.7795 - val_acc_1: 0.7795 - val_acc_2: 0.7795\n",
      "Epoch 37/50\n",
      "100/100 - 8s - loss_1: 0.0867 - loss_2: 0.0866 - acc_ensemble: 0.9810 - acc_1: 0.9810 - acc_2: 0.9810 - val_loss_1: 0.9354 - val_loss_2: 0.9354 - val_acc_ensemble: 0.7775 - val_acc_1: 0.7775 - val_acc_2: 0.7775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 8s - loss_1: 0.0738 - loss_2: 0.0735 - acc_ensemble: 0.9750 - acc_1: 0.9750 - acc_2: 0.9750 - val_loss_1: 0.9337 - val_loss_2: 0.9337 - val_acc_ensemble: 0.7818 - val_acc_1: 0.7818 - val_acc_2: 0.7818\n",
      "Epoch 39/50\n",
      "100/100 - 8s - loss_1: 0.0581 - loss_2: 0.0602 - acc_ensemble: 0.9850 - acc_1: 0.9850 - acc_2: 0.9850 - val_loss_1: 0.9491 - val_loss_2: 0.9491 - val_acc_ensemble: 0.7808 - val_acc_1: 0.7808 - val_acc_2: 0.7808\n",
      "Epoch 40/50\n",
      "100/100 - 8s - loss_1: 0.0731 - loss_2: 0.0607 - acc_ensemble: 0.9720 - acc_1: 0.9720 - acc_2: 0.9720 - val_loss_1: 0.9654 - val_loss_2: 0.9654 - val_acc_ensemble: 0.7765 - val_acc_1: 0.7765 - val_acc_2: 0.7765\n",
      "Epoch 41/50\n",
      "100/100 - 8s - loss_1: 0.0612 - loss_2: 0.0694 - acc_ensemble: 0.9830 - acc_1: 0.9830 - acc_2: 0.9830 - val_loss_1: 0.9775 - val_loss_2: 0.9775 - val_acc_ensemble: 0.7829 - val_acc_1: 0.7829 - val_acc_2: 0.7829\n",
      "Epoch 42/50\n",
      "100/100 - 8s - loss_1: 0.0849 - loss_2: 0.0733 - acc_ensemble: 0.9820 - acc_1: 0.9820 - acc_2: 0.9820 - val_loss_1: 0.9956 - val_loss_2: 0.9956 - val_acc_ensemble: 0.7749 - val_acc_1: 0.7749 - val_acc_2: 0.7749\n",
      "Epoch 43/50\n",
      "100/100 - 8s - loss_1: 0.0610 - loss_2: 0.0606 - acc_ensemble: 0.9840 - acc_1: 0.9840 - acc_2: 0.9840 - val_loss_1: 0.9612 - val_loss_2: 0.9612 - val_acc_ensemble: 0.7872 - val_acc_1: 0.7872 - val_acc_2: 0.7872\n",
      "Epoch 44/50\n",
      "100/100 - 8s - loss_1: 0.0553 - loss_2: 0.0534 - acc_ensemble: 0.9920 - acc_1: 0.9920 - acc_2: 0.9920 - val_loss_1: 0.9767 - val_loss_2: 0.9767 - val_acc_ensemble: 0.7817 - val_acc_1: 0.7817 - val_acc_2: 0.7817\n",
      "Epoch 45/50\n",
      "100/100 - 8s - loss_1: 0.0480 - loss_2: 0.0459 - acc_ensemble: 0.9880 - acc_1: 0.9880 - acc_2: 0.9880 - val_loss_1: 0.9867 - val_loss_2: 0.9867 - val_acc_ensemble: 0.7844 - val_acc_1: 0.7844 - val_acc_2: 0.7844\n",
      "Epoch 46/50\n",
      "100/100 - 8s - loss_1: 0.0543 - loss_2: 0.0509 - acc_ensemble: 0.9820 - acc_1: 0.9820 - acc_2: 0.9820 - val_loss_1: 0.9843 - val_loss_2: 0.9843 - val_acc_ensemble: 0.7793 - val_acc_1: 0.7793 - val_acc_2: 0.7793\n",
      "Epoch 47/50\n",
      "100/100 - 8s - loss_1: 0.0496 - loss_2: 0.0504 - acc_ensemble: 0.9890 - acc_1: 0.9890 - acc_2: 0.9890 - val_loss_1: 1.0063 - val_loss_2: 1.0063 - val_acc_ensemble: 0.7829 - val_acc_1: 0.7829 - val_acc_2: 0.7829\n",
      "Epoch 48/50\n",
      "100/100 - 8s - loss_1: 0.0446 - loss_2: 0.0472 - acc_ensemble: 0.9860 - acc_1: 0.9860 - acc_2: 0.9860 - val_loss_1: 1.0662 - val_loss_2: 1.0662 - val_acc_ensemble: 0.7786 - val_acc_1: 0.7786 - val_acc_2: 0.7786\n",
      "Epoch 49/50\n",
      "100/100 - 8s - loss_1: 0.0529 - loss_2: 0.0484 - acc_ensemble: 0.9880 - acc_1: 0.9880 - acc_2: 0.9880 - val_loss_1: 1.0356 - val_loss_2: 1.0356 - val_acc_ensemble: 0.7825 - val_acc_1: 0.7825 - val_acc_2: 0.7825\n",
      "Epoch 50/50\n",
      "100/100 - 8s - loss_1: 0.0603 - loss_2: 0.0556 - acc_ensemble: 0.9770 - acc_1: 0.9770 - acc_2: 0.9770 - val_loss_1: 1.0558 - val_loss_2: 1.0558 - val_acc_ensemble: 0.7776 - val_acc_1: 0.7776 - val_acc_2: 0.7776\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S1.00\n",
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,32] []  896        input              \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_1         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,32] []  0          bn_1_1             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,32] []  9248       relu_1_1           \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_2         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,32] []  0          bn_1_2             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,32] []  0          relu_1_2           \n",
      "                                    [None,16,16,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,64] []  18496      avg_pool2d_1       \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_1         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,64] []  0          bn_2_1             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,64] []  36928      relu_2_1           \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_2         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,64] []  0          bn_2_2             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,64] []    0          relu_2_2           \n",
      "                                    [None,8,8,64] []                                  \n",
      "--------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,128] []   73856      avg_pool2d_2       \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,128] []   256        conv2d_3_1         \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,128] []   0          bn_3_1             \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,128] []   147584     relu_3_1           \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,128] []   256        conv2d_3_2         \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,128] []   0          bn_3_2             \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,128] []       0          relu_3_2           \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,128] []       16512      global_avg_pool2d  \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,128] []       256        fc1                \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,128] []       0          bn_fc1             \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]           1290       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 305962\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 9s - loss_1: 1.5767 - loss_2: 1.5808 - acc_ensemble: 0.5430 - acc_1: 0.5430 - acc_2: 0.5430 - val_loss_1: 1.3055 - val_loss_2: 1.3055 - val_acc_ensemble: 0.5189 - val_acc_1: 0.5189 - val_acc_2: 0.5189\n",
      "Epoch 2/50\n",
      "100/100 - 8s - loss_1: 1.1914 - loss_2: 1.1930 - acc_ensemble: 0.6260 - acc_1: 0.6260 - acc_2: 0.6260 - val_loss_1: 1.1098 - val_loss_2: 1.1098 - val_acc_ensemble: 0.5972 - val_acc_1: 0.5972 - val_acc_2: 0.5972\n",
      "Epoch 3/50\n",
      "100/100 - 8s - loss_1: 1.0261 - loss_2: 1.0158 - acc_ensemble: 0.6590 - acc_1: 0.6590 - acc_2: 0.6590 - val_loss_1: 1.0142 - val_loss_2: 1.0142 - val_acc_ensemble: 0.6389 - val_acc_1: 0.6389 - val_acc_2: 0.6389\n",
      "Epoch 4/50\n",
      "100/100 - 8s - loss_1: 0.8997 - loss_2: 0.9085 - acc_ensemble: 0.7140 - acc_1: 0.7140 - acc_2: 0.7140 - val_loss_1: 0.9255 - val_loss_2: 0.9255 - val_acc_ensemble: 0.6704 - val_acc_1: 0.6704 - val_acc_2: 0.6704\n",
      "Epoch 5/50\n",
      "100/100 - 8s - loss_1: 0.7993 - loss_2: 0.7944 - acc_ensemble: 0.7530 - acc_1: 0.7530 - acc_2: 0.7530 - val_loss_1: 0.8504 - val_loss_2: 0.8504 - val_acc_ensemble: 0.7004 - val_acc_1: 0.7004 - val_acc_2: 0.7004\n",
      "Epoch 6/50\n",
      "100/100 - 8s - loss_1: 0.7347 - loss_2: 0.7318 - acc_ensemble: 0.7900 - acc_1: 0.7900 - acc_2: 0.7900 - val_loss_1: 0.8195 - val_loss_2: 0.8195 - val_acc_ensemble: 0.7083 - val_acc_1: 0.7083 - val_acc_2: 0.7083\n",
      "Epoch 7/50\n",
      "100/100 - 8s - loss_1: 0.6620 - loss_2: 0.6761 - acc_ensemble: 0.7950 - acc_1: 0.7950 - acc_2: 0.7950 - val_loss_1: 0.7857 - val_loss_2: 0.7857 - val_acc_ensemble: 0.7261 - val_acc_1: 0.7261 - val_acc_2: 0.7261\n",
      "Epoch 8/50\n",
      "100/100 - 8s - loss_1: 0.6137 - loss_2: 0.6246 - acc_ensemble: 0.8130 - acc_1: 0.8130 - acc_2: 0.8130 - val_loss_1: 0.7484 - val_loss_2: 0.7484 - val_acc_ensemble: 0.7394 - val_acc_1: 0.7394 - val_acc_2: 0.7394\n",
      "Epoch 9/50\n",
      "100/100 - 8s - loss_1: 0.5720 - loss_2: 0.5712 - acc_ensemble: 0.8230 - acc_1: 0.8230 - acc_2: 0.8230 - val_loss_1: 0.7245 - val_loss_2: 0.7245 - val_acc_ensemble: 0.7453 - val_acc_1: 0.7453 - val_acc_2: 0.7453\n",
      "Epoch 10/50\n",
      "100/100 - 8s - loss_1: 0.5235 - loss_2: 0.5110 - acc_ensemble: 0.8510 - acc_1: 0.8510 - acc_2: 0.8510 - val_loss_1: 0.7329 - val_loss_2: 0.7329 - val_acc_ensemble: 0.7479 - val_acc_1: 0.7479 - val_acc_2: 0.7479\n",
      "Epoch 11/50\n",
      "100/100 - 8s - loss_1: 0.4644 - loss_2: 0.4910 - acc_ensemble: 0.8490 - acc_1: 0.8490 - acc_2: 0.8490 - val_loss_1: 0.7298 - val_loss_2: 0.7298 - val_acc_ensemble: 0.7486 - val_acc_1: 0.7486 - val_acc_2: 0.7486\n",
      "Epoch 12/50\n",
      "100/100 - 8s - loss_1: 0.4523 - loss_2: 0.4526 - acc_ensemble: 0.8570 - acc_1: 0.8570 - acc_2: 0.8570 - val_loss_1: 0.7303 - val_loss_2: 0.7303 - val_acc_ensemble: 0.7578 - val_acc_1: 0.7578 - val_acc_2: 0.7578\n",
      "Epoch 13/50\n",
      "100/100 - 8s - loss_1: 0.4134 - loss_2: 0.4187 - acc_ensemble: 0.8770 - acc_1: 0.8770 - acc_2: 0.8770 - val_loss_1: 0.7002 - val_loss_2: 0.7002 - val_acc_ensemble: 0.7640 - val_acc_1: 0.7640 - val_acc_2: 0.7640\n",
      "Epoch 14/50\n",
      "100/100 - 8s - loss_1: 0.3714 - loss_2: 0.3913 - acc_ensemble: 0.8880 - acc_1: 0.8880 - acc_2: 0.8880 - val_loss_1: 0.6978 - val_loss_2: 0.6978 - val_acc_ensemble: 0.7677 - val_acc_1: 0.7677 - val_acc_2: 0.7677\n",
      "Epoch 15/50\n",
      "100/100 - 8s - loss_1: 0.3555 - loss_2: 0.3508 - acc_ensemble: 0.9010 - acc_1: 0.9010 - acc_2: 0.9010 - val_loss_1: 0.6931 - val_loss_2: 0.6931 - val_acc_ensemble: 0.7666 - val_acc_1: 0.7666 - val_acc_2: 0.7666\n",
      "Epoch 16/50\n",
      "100/100 - 8s - loss_1: 0.3270 - loss_2: 0.3178 - acc_ensemble: 0.9170 - acc_1: 0.9170 - acc_2: 0.9170 - val_loss_1: 0.6952 - val_loss_2: 0.6952 - val_acc_ensemble: 0.7733 - val_acc_1: 0.7733 - val_acc_2: 0.7733\n",
      "Epoch 17/50\n",
      "100/100 - 8s - loss_1: 0.2871 - loss_2: 0.2811 - acc_ensemble: 0.9170 - acc_1: 0.9170 - acc_2: 0.9170 - val_loss_1: 0.7120 - val_loss_2: 0.7120 - val_acc_ensemble: 0.7782 - val_acc_1: 0.7782 - val_acc_2: 0.7782\n",
      "Epoch 18/50\n",
      "100/100 - 8s - loss_1: 0.2907 - loss_2: 0.2876 - acc_ensemble: 0.9110 - acc_1: 0.9110 - acc_2: 0.9110 - val_loss_1: 0.7066 - val_loss_2: 0.7066 - val_acc_ensemble: 0.7739 - val_acc_1: 0.7739 - val_acc_2: 0.7739\n",
      "Epoch 19/50\n",
      "100/100 - 8s - loss_1: 0.2531 - loss_2: 0.2436 - acc_ensemble: 0.9370 - acc_1: 0.9370 - acc_2: 0.9370 - val_loss_1: 0.7170 - val_loss_2: 0.7170 - val_acc_ensemble: 0.7785 - val_acc_1: 0.7785 - val_acc_2: 0.7785\n",
      "Epoch 20/50\n",
      "100/100 - 8s - loss_1: 0.2326 - loss_2: 0.2374 - acc_ensemble: 0.9250 - acc_1: 0.9250 - acc_2: 0.9250 - val_loss_1: 0.7360 - val_loss_2: 0.7360 - val_acc_ensemble: 0.7802 - val_acc_1: 0.7802 - val_acc_2: 0.7802\n",
      "Epoch 21/50\n",
      "100/100 - 8s - loss_1: 0.2130 - loss_2: 0.2142 - acc_ensemble: 0.9410 - acc_1: 0.9410 - acc_2: 0.9410 - val_loss_1: 0.7322 - val_loss_2: 0.7322 - val_acc_ensemble: 0.7782 - val_acc_1: 0.7782 - val_acc_2: 0.7782\n",
      "Epoch 22/50\n",
      "100/100 - 8s - loss_1: 0.1907 - loss_2: 0.1873 - acc_ensemble: 0.9540 - acc_1: 0.9540 - acc_2: 0.9540 - val_loss_1: 0.7891 - val_loss_2: 0.7891 - val_acc_ensemble: 0.7721 - val_acc_1: 0.7721 - val_acc_2: 0.7721\n",
      "Epoch 23/50\n",
      "100/100 - 8s - loss_1: 0.1803 - loss_2: 0.1773 - acc_ensemble: 0.9470 - acc_1: 0.9470 - acc_2: 0.9470 - val_loss_1: 0.7669 - val_loss_2: 0.7669 - val_acc_ensemble: 0.7751 - val_acc_1: 0.7751 - val_acc_2: 0.7751\n",
      "Epoch 24/50\n",
      "100/100 - 8s - loss_1: 0.1655 - loss_2: 0.1731 - acc_ensemble: 0.9570 - acc_1: 0.9570 - acc_2: 0.9570 - val_loss_1: 0.7651 - val_loss_2: 0.7651 - val_acc_ensemble: 0.7786 - val_acc_1: 0.7786 - val_acc_2: 0.7786\n",
      "Epoch 25/50\n",
      "100/100 - 8s - loss_1: 0.1509 - loss_2: 0.1426 - acc_ensemble: 0.9680 - acc_1: 0.9680 - acc_2: 0.9680 - val_loss_1: 0.7776 - val_loss_2: 0.7776 - val_acc_ensemble: 0.7809 - val_acc_1: 0.7809 - val_acc_2: 0.7809\n",
      "Epoch 26/50\n",
      "100/100 - 8s - loss_1: 0.1482 - loss_2: 0.1392 - acc_ensemble: 0.9610 - acc_1: 0.9610 - acc_2: 0.9610 - val_loss_1: 0.8018 - val_loss_2: 0.8018 - val_acc_ensemble: 0.7831 - val_acc_1: 0.7831 - val_acc_2: 0.7831\n",
      "Epoch 27/50\n",
      "100/100 - 8s - loss_1: 0.1501 - loss_2: 0.1331 - acc_ensemble: 0.9700 - acc_1: 0.9700 - acc_2: 0.9700 - val_loss_1: 0.8163 - val_loss_2: 0.8163 - val_acc_ensemble: 0.7771 - val_acc_1: 0.7771 - val_acc_2: 0.7771\n",
      "Epoch 28/50\n",
      "100/100 - 8s - loss_1: 0.1148 - loss_2: 0.1317 - acc_ensemble: 0.9640 - acc_1: 0.9640 - acc_2: 0.9640 - val_loss_1: 0.8089 - val_loss_2: 0.8089 - val_acc_ensemble: 0.7811 - val_acc_1: 0.7811 - val_acc_2: 0.7811\n",
      "Epoch 29/50\n",
      "100/100 - 8s - loss_1: 0.0988 - loss_2: 0.1056 - acc_ensemble: 0.9650 - acc_1: 0.9650 - acc_2: 0.9650 - val_loss_1: 0.8533 - val_loss_2: 0.8533 - val_acc_ensemble: 0.7801 - val_acc_1: 0.7801 - val_acc_2: 0.7801\n",
      "Epoch 30/50\n",
      "100/100 - 8s - loss_1: 0.0928 - loss_2: 0.0877 - acc_ensemble: 0.9620 - acc_1: 0.9620 - acc_2: 0.9620 - val_loss_1: 0.8689 - val_loss_2: 0.8689 - val_acc_ensemble: 0.7826 - val_acc_1: 0.7826 - val_acc_2: 0.7826\n",
      "Epoch 31/50\n",
      "100/100 - 8s - loss_1: 0.0886 - loss_2: 0.0842 - acc_ensemble: 0.9630 - acc_1: 0.9630 - acc_2: 0.9630 - val_loss_1: 0.8821 - val_loss_2: 0.8821 - val_acc_ensemble: 0.7799 - val_acc_1: 0.7799 - val_acc_2: 0.7799\n",
      "Epoch 32/50\n",
      "100/100 - 8s - loss_1: 0.0896 - loss_2: 0.0909 - acc_ensemble: 0.9750 - acc_1: 0.9750 - acc_2: 0.9750 - val_loss_1: 0.8878 - val_loss_2: 0.8878 - val_acc_ensemble: 0.7792 - val_acc_1: 0.7792 - val_acc_2: 0.7792\n",
      "Epoch 33/50\n",
      "100/100 - 8s - loss_1: 0.0984 - loss_2: 0.0950 - acc_ensemble: 0.9660 - acc_1: 0.9660 - acc_2: 0.9660 - val_loss_1: 0.9377 - val_loss_2: 0.9377 - val_acc_ensemble: 0.7745 - val_acc_1: 0.7745 - val_acc_2: 0.7745\n",
      "Epoch 34/50\n",
      "100/100 - 8s - loss_1: 0.0856 - loss_2: 0.0838 - acc_ensemble: 0.9740 - acc_1: 0.9740 - acc_2: 0.9740 - val_loss_1: 0.9229 - val_loss_2: 0.9229 - val_acc_ensemble: 0.7770 - val_acc_1: 0.7770 - val_acc_2: 0.7770\n",
      "Epoch 35/50\n",
      "100/100 - 8s - loss_1: 0.0823 - loss_2: 0.0874 - acc_ensemble: 0.9760 - acc_1: 0.9760 - acc_2: 0.9760 - val_loss_1: 0.9474 - val_loss_2: 0.9474 - val_acc_ensemble: 0.7720 - val_acc_1: 0.7720 - val_acc_2: 0.7720\n",
      "Epoch 36/50\n",
      "100/100 - 8s - loss_1: 0.0687 - loss_2: 0.0739 - acc_ensemble: 0.9710 - acc_1: 0.9710 - acc_2: 0.9710 - val_loss_1: 0.9045 - val_loss_2: 0.9045 - val_acc_ensemble: 0.7851 - val_acc_1: 0.7851 - val_acc_2: 0.7851\n",
      "Epoch 37/50\n",
      "100/100 - 8s - loss_1: 0.0715 - loss_2: 0.0698 - acc_ensemble: 0.9820 - acc_1: 0.9820 - acc_2: 0.9820 - val_loss_1: 0.9307 - val_loss_2: 0.9307 - val_acc_ensemble: 0.7828 - val_acc_1: 0.7828 - val_acc_2: 0.7828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 8s - loss_1: 0.0708 - loss_2: 0.0708 - acc_ensemble: 0.9810 - acc_1: 0.9810 - acc_2: 0.9810 - val_loss_1: 0.9673 - val_loss_2: 0.9673 - val_acc_ensemble: 0.7832 - val_acc_1: 0.7832 - val_acc_2: 0.7832\n",
      "Epoch 39/50\n",
      "100/100 - 8s - loss_1: 0.0685 - loss_2: 0.0769 - acc_ensemble: 0.9720 - acc_1: 0.9720 - acc_2: 0.9720 - val_loss_1: 0.9724 - val_loss_2: 0.9724 - val_acc_ensemble: 0.7772 - val_acc_1: 0.7772 - val_acc_2: 0.7772\n",
      "Epoch 40/50\n",
      "100/100 - 8s - loss_1: 0.0678 - loss_2: 0.0586 - acc_ensemble: 0.9760 - acc_1: 0.9760 - acc_2: 0.9760 - val_loss_1: 0.9666 - val_loss_2: 0.9666 - val_acc_ensemble: 0.7781 - val_acc_1: 0.7781 - val_acc_2: 0.7781\n",
      "Epoch 41/50\n",
      "100/100 - 8s - loss_1: 0.0601 - loss_2: 0.0498 - acc_ensemble: 0.9820 - acc_1: 0.9820 - acc_2: 0.9820 - val_loss_1: 0.9949 - val_loss_2: 0.9949 - val_acc_ensemble: 0.7812 - val_acc_1: 0.7812 - val_acc_2: 0.7812\n",
      "Epoch 42/50\n",
      "100/100 - 8s - loss_1: 0.0612 - loss_2: 0.0583 - acc_ensemble: 0.9800 - acc_1: 0.9800 - acc_2: 0.9800 - val_loss_1: 1.0058 - val_loss_2: 1.0058 - val_acc_ensemble: 0.7812 - val_acc_1: 0.7812 - val_acc_2: 0.7812\n",
      "Epoch 43/50\n",
      "100/100 - 8s - loss_1: 0.0507 - loss_2: 0.0507 - acc_ensemble: 0.9810 - acc_1: 0.9810 - acc_2: 0.9810 - val_loss_1: 1.0178 - val_loss_2: 1.0178 - val_acc_ensemble: 0.7756 - val_acc_1: 0.7756 - val_acc_2: 0.7756\n",
      "Epoch 44/50\n",
      "100/100 - 8s - loss_1: 0.0540 - loss_2: 0.0472 - acc_ensemble: 0.9750 - acc_1: 0.9750 - acc_2: 0.9750 - val_loss_1: 1.0388 - val_loss_2: 1.0388 - val_acc_ensemble: 0.7776 - val_acc_1: 0.7776 - val_acc_2: 0.7776\n",
      "Epoch 45/50\n",
      "100/100 - 8s - loss_1: 0.0598 - loss_2: 0.0606 - acc_ensemble: 0.9770 - acc_1: 0.9770 - acc_2: 0.9770 - val_loss_1: 1.0247 - val_loss_2: 1.0247 - val_acc_ensemble: 0.7793 - val_acc_1: 0.7793 - val_acc_2: 0.7793\n",
      "Epoch 46/50\n",
      "100/100 - 8s - loss_1: 0.0602 - loss_2: 0.0642 - acc_ensemble: 0.9770 - acc_1: 0.9770 - acc_2: 0.9770 - val_loss_1: 1.0246 - val_loss_2: 1.0246 - val_acc_ensemble: 0.7833 - val_acc_1: 0.7833 - val_acc_2: 0.7833\n",
      "Epoch 47/50\n",
      "100/100 - 8s - loss_1: 0.0666 - loss_2: 0.0639 - acc_ensemble: 0.9830 - acc_1: 0.9830 - acc_2: 0.9830 - val_loss_1: 1.0601 - val_loss_2: 1.0601 - val_acc_ensemble: 0.7758 - val_acc_1: 0.7758 - val_acc_2: 0.7758\n",
      "Epoch 48/50\n",
      "100/100 - 8s - loss_1: 0.0545 - loss_2: 0.0534 - acc_ensemble: 0.9850 - acc_1: 0.9850 - acc_2: 0.9850 - val_loss_1: 1.0314 - val_loss_2: 1.0314 - val_acc_ensemble: 0.7829 - val_acc_1: 0.7829 - val_acc_2: 0.7829\n",
      "Epoch 49/50\n",
      "100/100 - 8s - loss_1: 0.0415 - loss_2: 0.0406 - acc_ensemble: 0.9880 - acc_1: 0.9880 - acc_2: 0.9880 - val_loss_1: 1.0657 - val_loss_2: 1.0657 - val_acc_ensemble: 0.7799 - val_acc_1: 0.7799 - val_acc_2: 0.7799\n",
      "Epoch 50/50\n",
      "100/100 - 8s - loss_1: 0.0343 - loss_2: 0.0405 - acc_ensemble: 0.9870 - acc_1: 0.9870 - acc_2: 0.9870 - val_loss_1: 1.0614 - val_loss_2: 1.0614 - val_acc_ensemble: 0.7858 - val_acc_1: 0.7858 - val_acc_2: 0.7858\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S1.00\n",
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,32] []  896        input              \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_1         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,32] []  0          bn_1_1             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,32] []  9248       relu_1_1           \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_2         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,32] []  0          bn_1_2             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,32] []  0          relu_1_2           \n",
      "                                    [None,16,16,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,64] []  18496      avg_pool2d_1       \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_1         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,64] []  0          bn_2_1             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,64] []  36928      relu_2_1           \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_2         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,64] []  0          bn_2_2             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,64] []    0          relu_2_2           \n",
      "                                    [None,8,8,64] []                                  \n",
      "--------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,128] []   73856      avg_pool2d_2       \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,128] []   256        conv2d_3_1         \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,128] []   0          bn_3_1             \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,128] []   147584     relu_3_1           \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,128] []   256        conv2d_3_2         \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,128] []   0          bn_3_2             \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,128] []       0          relu_3_2           \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,128] []       16512      global_avg_pool2d  \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,128] []       256        fc1                \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,128] []       0          bn_fc1             \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]           1290       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 305962\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 9s - loss_1: 1.5644 - loss_2: 1.5747 - acc_ensemble: 0.5510 - acc_1: 0.5510 - acc_2: 0.5510 - val_loss_1: 1.2983 - val_loss_2: 1.2983 - val_acc_ensemble: 0.5312 - val_acc_1: 0.5312 - val_acc_2: 0.5312\n",
      "Epoch 2/50\n",
      "100/100 - 8s - loss_1: 1.1837 - loss_2: 1.1874 - acc_ensemble: 0.6190 - acc_1: 0.6190 - acc_2: 0.6190 - val_loss_1: 1.1194 - val_loss_2: 1.1194 - val_acc_ensemble: 0.5942 - val_acc_1: 0.5942 - val_acc_2: 0.5942\n",
      "Epoch 3/50\n",
      "100/100 - 8s - loss_1: 0.9881 - loss_2: 0.9941 - acc_ensemble: 0.6770 - acc_1: 0.6770 - acc_2: 0.6770 - val_loss_1: 0.9988 - val_loss_2: 0.9988 - val_acc_ensemble: 0.6501 - val_acc_1: 0.6501 - val_acc_2: 0.6501\n",
      "Epoch 4/50\n",
      "100/100 - 8s - loss_1: 0.8849 - loss_2: 0.8886 - acc_ensemble: 0.7240 - acc_1: 0.7240 - acc_2: 0.7240 - val_loss_1: 0.8817 - val_loss_2: 0.8817 - val_acc_ensemble: 0.6863 - val_acc_1: 0.6863 - val_acc_2: 0.6863\n",
      "Epoch 5/50\n",
      "100/100 - 8s - loss_1: 0.7855 - loss_2: 0.8030 - acc_ensemble: 0.7450 - acc_1: 0.7450 - acc_2: 0.7450 - val_loss_1: 0.8335 - val_loss_2: 0.8335 - val_acc_ensemble: 0.7035 - val_acc_1: 0.7035 - val_acc_2: 0.7035\n",
      "Epoch 6/50\n",
      "100/100 - 8s - loss_1: 0.7251 - loss_2: 0.7195 - acc_ensemble: 0.7670 - acc_1: 0.7670 - acc_2: 0.7670 - val_loss_1: 0.8153 - val_loss_2: 0.8153 - val_acc_ensemble: 0.7097 - val_acc_1: 0.7097 - val_acc_2: 0.7097\n",
      "Epoch 7/50\n",
      "100/100 - 8s - loss_1: 0.6561 - loss_2: 0.6719 - acc_ensemble: 0.7830 - acc_1: 0.7830 - acc_2: 0.7830 - val_loss_1: 0.7687 - val_loss_2: 0.7687 - val_acc_ensemble: 0.7269 - val_acc_1: 0.7269 - val_acc_2: 0.7269\n",
      "Epoch 8/50\n",
      "100/100 - 8s - loss_1: 0.6225 - loss_2: 0.6406 - acc_ensemble: 0.8010 - acc_1: 0.8010 - acc_2: 0.8010 - val_loss_1: 0.7422 - val_loss_2: 0.7422 - val_acc_ensemble: 0.7398 - val_acc_1: 0.7398 - val_acc_2: 0.7398\n",
      "Epoch 9/50\n",
      "100/100 - 8s - loss_1: 0.5585 - loss_2: 0.5547 - acc_ensemble: 0.8140 - acc_1: 0.8140 - acc_2: 0.8140 - val_loss_1: 0.7171 - val_loss_2: 0.7171 - val_acc_ensemble: 0.7505 - val_acc_1: 0.7505 - val_acc_2: 0.7505\n",
      "Epoch 10/50\n",
      "100/100 - 8s - loss_1: 0.5122 - loss_2: 0.5167 - acc_ensemble: 0.8430 - acc_1: 0.8430 - acc_2: 0.8430 - val_loss_1: 0.7105 - val_loss_2: 0.7105 - val_acc_ensemble: 0.7572 - val_acc_1: 0.7572 - val_acc_2: 0.7572\n",
      "Epoch 11/50\n",
      "100/100 - 8s - loss_1: 0.4888 - loss_2: 0.4903 - acc_ensemble: 0.8470 - acc_1: 0.8470 - acc_2: 0.8470 - val_loss_1: 0.6914 - val_loss_2: 0.6914 - val_acc_ensemble: 0.7603 - val_acc_1: 0.7603 - val_acc_2: 0.7603\n",
      "Epoch 12/50\n",
      "100/100 - 8s - loss_1: 0.4534 - loss_2: 0.4459 - acc_ensemble: 0.8720 - acc_1: 0.8720 - acc_2: 0.8720 - val_loss_1: 0.6929 - val_loss_2: 0.6929 - val_acc_ensemble: 0.7658 - val_acc_1: 0.7658 - val_acc_2: 0.7658\n",
      "Epoch 13/50\n",
      "100/100 - 8s - loss_1: 0.4169 - loss_2: 0.4203 - acc_ensemble: 0.8840 - acc_1: 0.8840 - acc_2: 0.8840 - val_loss_1: 0.6824 - val_loss_2: 0.6824 - val_acc_ensemble: 0.7658 - val_acc_1: 0.7658 - val_acc_2: 0.7658\n",
      "Epoch 14/50\n",
      "100/100 - 8s - loss_1: 0.3772 - loss_2: 0.3786 - acc_ensemble: 0.8900 - acc_1: 0.8900 - acc_2: 0.8900 - val_loss_1: 0.6658 - val_loss_2: 0.6658 - val_acc_ensemble: 0.7754 - val_acc_1: 0.7754 - val_acc_2: 0.7754\n",
      "Epoch 15/50\n",
      "100/100 - 8s - loss_1: 0.3580 - loss_2: 0.3586 - acc_ensemble: 0.8970 - acc_1: 0.8970 - acc_2: 0.8970 - val_loss_1: 0.7001 - val_loss_2: 0.7001 - val_acc_ensemble: 0.7645 - val_acc_1: 0.7645 - val_acc_2: 0.7645\n",
      "Epoch 16/50\n",
      "100/100 - 8s - loss_1: 0.3358 - loss_2: 0.3287 - acc_ensemble: 0.9140 - acc_1: 0.9140 - acc_2: 0.9140 - val_loss_1: 0.6726 - val_loss_2: 0.6726 - val_acc_ensemble: 0.7779 - val_acc_1: 0.7779 - val_acc_2: 0.7779\n",
      "Epoch 17/50\n",
      "100/100 - 8s - loss_1: 0.2972 - loss_2: 0.2905 - acc_ensemble: 0.9250 - acc_1: 0.9250 - acc_2: 0.9250 - val_loss_1: 0.6558 - val_loss_2: 0.6558 - val_acc_ensemble: 0.7839 - val_acc_1: 0.7839 - val_acc_2: 0.7839\n",
      "Epoch 18/50\n",
      "100/100 - 8s - loss_1: 0.2929 - loss_2: 0.2883 - acc_ensemble: 0.9170 - acc_1: 0.9170 - acc_2: 0.9170 - val_loss_1: 0.6908 - val_loss_2: 0.6908 - val_acc_ensemble: 0.7770 - val_acc_1: 0.7770 - val_acc_2: 0.7770\n",
      "Epoch 19/50\n",
      "100/100 - 8s - loss_1: 0.2511 - loss_2: 0.2639 - acc_ensemble: 0.9180 - acc_1: 0.9180 - acc_2: 0.9180 - val_loss_1: 0.6905 - val_loss_2: 0.6905 - val_acc_ensemble: 0.7830 - val_acc_1: 0.7830 - val_acc_2: 0.7830\n",
      "Epoch 20/50\n",
      "100/100 - 8s - loss_1: 0.2437 - loss_2: 0.2466 - acc_ensemble: 0.9320 - acc_1: 0.9320 - acc_2: 0.9320 - val_loss_1: 0.7002 - val_loss_2: 0.7002 - val_acc_ensemble: 0.7820 - val_acc_1: 0.7820 - val_acc_2: 0.7820\n",
      "Epoch 21/50\n",
      "100/100 - 8s - loss_1: 0.2250 - loss_2: 0.2137 - acc_ensemble: 0.9450 - acc_1: 0.9450 - acc_2: 0.9450 - val_loss_1: 0.7165 - val_loss_2: 0.7165 - val_acc_ensemble: 0.7818 - val_acc_1: 0.7818 - val_acc_2: 0.7818\n",
      "Epoch 22/50\n",
      "100/100 - 8s - loss_1: 0.1962 - loss_2: 0.1950 - acc_ensemble: 0.9520 - acc_1: 0.9520 - acc_2: 0.9520 - val_loss_1: 0.7120 - val_loss_2: 0.7120 - val_acc_ensemble: 0.7840 - val_acc_1: 0.7840 - val_acc_2: 0.7840\n",
      "Epoch 23/50\n",
      "100/100 - 8s - loss_1: 0.1750 - loss_2: 0.1870 - acc_ensemble: 0.9530 - acc_1: 0.9530 - acc_2: 0.9530 - val_loss_1: 0.7223 - val_loss_2: 0.7223 - val_acc_ensemble: 0.7828 - val_acc_1: 0.7828 - val_acc_2: 0.7828\n",
      "Epoch 24/50\n",
      "100/100 - 8s - loss_1: 0.1651 - loss_2: 0.1618 - acc_ensemble: 0.9490 - acc_1: 0.9490 - acc_2: 0.9490 - val_loss_1: 0.7497 - val_loss_2: 0.7497 - val_acc_ensemble: 0.7824 - val_acc_1: 0.7824 - val_acc_2: 0.7824\n",
      "Epoch 25/50\n",
      "100/100 - 8s - loss_1: 0.1643 - loss_2: 0.1676 - acc_ensemble: 0.9600 - acc_1: 0.9600 - acc_2: 0.9600 - val_loss_1: 0.7479 - val_loss_2: 0.7479 - val_acc_ensemble: 0.7852 - val_acc_1: 0.7852 - val_acc_2: 0.7852\n",
      "Epoch 26/50\n",
      "100/100 - 8s - loss_1: 0.1399 - loss_2: 0.1373 - acc_ensemble: 0.9540 - acc_1: 0.9540 - acc_2: 0.9540 - val_loss_1: 0.7597 - val_loss_2: 0.7597 - val_acc_ensemble: 0.7837 - val_acc_1: 0.7837 - val_acc_2: 0.7837\n",
      "Epoch 27/50\n",
      "100/100 - 8s - loss_1: 0.1444 - loss_2: 0.1480 - acc_ensemble: 0.9530 - acc_1: 0.9530 - acc_2: 0.9530 - val_loss_1: 0.8057 - val_loss_2: 0.8057 - val_acc_ensemble: 0.7788 - val_acc_1: 0.7788 - val_acc_2: 0.7788\n",
      "Epoch 28/50\n",
      "100/100 - 8s - loss_1: 0.1290 - loss_2: 0.1326 - acc_ensemble: 0.9700 - acc_1: 0.9700 - acc_2: 0.9700 - val_loss_1: 0.7744 - val_loss_2: 0.7744 - val_acc_ensemble: 0.7863 - val_acc_1: 0.7863 - val_acc_2: 0.7863\n",
      "Epoch 29/50\n",
      "100/100 - 8s - loss_1: 0.1195 - loss_2: 0.1226 - acc_ensemble: 0.9600 - acc_1: 0.9600 - acc_2: 0.9600 - val_loss_1: 0.8126 - val_loss_2: 0.8126 - val_acc_ensemble: 0.7798 - val_acc_1: 0.7798 - val_acc_2: 0.7798\n",
      "Epoch 30/50\n",
      "100/100 - 8s - loss_1: 0.1183 - loss_2: 0.1085 - acc_ensemble: 0.9740 - acc_1: 0.9740 - acc_2: 0.9740 - val_loss_1: 0.8111 - val_loss_2: 0.8111 - val_acc_ensemble: 0.7855 - val_acc_1: 0.7855 - val_acc_2: 0.7855\n",
      "Epoch 31/50\n",
      "100/100 - 8s - loss_1: 0.0999 - loss_2: 0.0861 - acc_ensemble: 0.9680 - acc_1: 0.9680 - acc_2: 0.9680 - val_loss_1: 0.8236 - val_loss_2: 0.8236 - val_acc_ensemble: 0.7840 - val_acc_1: 0.7840 - val_acc_2: 0.7840\n",
      "Epoch 32/50\n",
      "100/100 - 8s - loss_1: 0.0891 - loss_2: 0.0844 - acc_ensemble: 0.9750 - acc_1: 0.9750 - acc_2: 0.9750 - val_loss_1: 0.8498 - val_loss_2: 0.8498 - val_acc_ensemble: 0.7825 - val_acc_1: 0.7825 - val_acc_2: 0.7825\n",
      "Epoch 33/50\n",
      "100/100 - 8s - loss_1: 0.0933 - loss_2: 0.0841 - acc_ensemble: 0.9820 - acc_1: 0.9820 - acc_2: 0.9820 - val_loss_1: 0.8469 - val_loss_2: 0.8469 - val_acc_ensemble: 0.7866 - val_acc_1: 0.7866 - val_acc_2: 0.7866\n",
      "Epoch 34/50\n",
      "100/100 - 8s - loss_1: 0.0758 - loss_2: 0.0872 - acc_ensemble: 0.9740 - acc_1: 0.9740 - acc_2: 0.9740 - val_loss_1: 0.8597 - val_loss_2: 0.8597 - val_acc_ensemble: 0.7834 - val_acc_1: 0.7834 - val_acc_2: 0.7834\n",
      "Epoch 35/50\n",
      "100/100 - 8s - loss_1: 0.0702 - loss_2: 0.0747 - acc_ensemble: 0.9770 - acc_1: 0.9770 - acc_2: 0.9770 - val_loss_1: 0.8742 - val_loss_2: 0.8742 - val_acc_ensemble: 0.7842 - val_acc_1: 0.7842 - val_acc_2: 0.7842\n",
      "Epoch 36/50\n",
      "100/100 - 8s - loss_1: 0.0672 - loss_2: 0.0668 - acc_ensemble: 0.9750 - acc_1: 0.9750 - acc_2: 0.9750 - val_loss_1: 0.9141 - val_loss_2: 0.9141 - val_acc_ensemble: 0.7808 - val_acc_1: 0.7808 - val_acc_2: 0.7808\n",
      "Epoch 37/50\n",
      "100/100 - 8s - loss_1: 0.0700 - loss_2: 0.0742 - acc_ensemble: 0.9840 - acc_1: 0.9840 - acc_2: 0.9840 - val_loss_1: 0.8879 - val_loss_2: 0.8879 - val_acc_ensemble: 0.7869 - val_acc_1: 0.7869 - val_acc_2: 0.7869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 8s - loss_1: 0.0749 - loss_2: 0.0732 - acc_ensemble: 0.9740 - acc_1: 0.9740 - acc_2: 0.9740 - val_loss_1: 0.9160 - val_loss_2: 0.9160 - val_acc_ensemble: 0.7812 - val_acc_1: 0.7812 - val_acc_2: 0.7812\n",
      "Epoch 39/50\n",
      "100/100 - 8s - loss_1: 0.0823 - loss_2: 0.0754 - acc_ensemble: 0.9780 - acc_1: 0.9780 - acc_2: 0.9780 - val_loss_1: 0.8844 - val_loss_2: 0.8844 - val_acc_ensemble: 0.7940 - val_acc_1: 0.7940 - val_acc_2: 0.7940\n",
      "Epoch 40/50\n",
      "100/100 - 8s - loss_1: 0.0686 - loss_2: 0.0637 - acc_ensemble: 0.9790 - acc_1: 0.9790 - acc_2: 0.9790 - val_loss_1: 0.9240 - val_loss_2: 0.9240 - val_acc_ensemble: 0.7844 - val_acc_1: 0.7844 - val_acc_2: 0.7844\n",
      "Epoch 41/50\n",
      "100/100 - 8s - loss_1: 0.0593 - loss_2: 0.0581 - acc_ensemble: 0.9880 - acc_1: 0.9880 - acc_2: 0.9880 - val_loss_1: 0.9239 - val_loss_2: 0.9239 - val_acc_ensemble: 0.7896 - val_acc_1: 0.7896 - val_acc_2: 0.7896\n",
      "Epoch 42/50\n",
      "100/100 - 8s - loss_1: 0.0649 - loss_2: 0.0655 - acc_ensemble: 0.9790 - acc_1: 0.9790 - acc_2: 0.9790 - val_loss_1: 0.9555 - val_loss_2: 0.9555 - val_acc_ensemble: 0.7803 - val_acc_1: 0.7803 - val_acc_2: 0.7803\n",
      "Epoch 43/50\n",
      "100/100 - 8s - loss_1: 0.0615 - loss_2: 0.0600 - acc_ensemble: 0.9770 - acc_1: 0.9770 - acc_2: 0.9770 - val_loss_1: 0.9291 - val_loss_2: 0.9291 - val_acc_ensemble: 0.7894 - val_acc_1: 0.7894 - val_acc_2: 0.7894\n",
      "Epoch 44/50\n",
      "100/100 - 8s - loss_1: 0.0679 - loss_2: 0.0681 - acc_ensemble: 0.9790 - acc_1: 0.9790 - acc_2: 0.9790 - val_loss_1: 0.9987 - val_loss_2: 0.9987 - val_acc_ensemble: 0.7810 - val_acc_1: 0.7810 - val_acc_2: 0.7810\n",
      "Epoch 45/50\n",
      "100/100 - 8s - loss_1: 0.0597 - loss_2: 0.0585 - acc_ensemble: 0.9840 - acc_1: 0.9840 - acc_2: 0.9840 - val_loss_1: 0.9438 - val_loss_2: 0.9438 - val_acc_ensemble: 0.7868 - val_acc_1: 0.7868 - val_acc_2: 0.7868\n",
      "Epoch 46/50\n",
      "100/100 - 8s - loss_1: 0.0515 - loss_2: 0.0459 - acc_ensemble: 0.9870 - acc_1: 0.9870 - acc_2: 0.9870 - val_loss_1: 0.9768 - val_loss_2: 0.9768 - val_acc_ensemble: 0.7851 - val_acc_1: 0.7851 - val_acc_2: 0.7851\n",
      "Epoch 47/50\n",
      "100/100 - 8s - loss_1: 0.0561 - loss_2: 0.0534 - acc_ensemble: 0.9770 - acc_1: 0.9770 - acc_2: 0.9770 - val_loss_1: 0.9798 - val_loss_2: 0.9798 - val_acc_ensemble: 0.7854 - val_acc_1: 0.7854 - val_acc_2: 0.7854\n",
      "Epoch 48/50\n",
      "100/100 - 8s - loss_1: 0.0448 - loss_2: 0.0518 - acc_ensemble: 0.9810 - acc_1: 0.9810 - acc_2: 0.9810 - val_loss_1: 0.9865 - val_loss_2: 0.9865 - val_acc_ensemble: 0.7862 - val_acc_1: 0.7862 - val_acc_2: 0.7862\n",
      "Epoch 49/50\n",
      "100/100 - 8s - loss_1: 0.0481 - loss_2: 0.0483 - acc_ensemble: 0.9830 - acc_1: 0.9830 - acc_2: 0.9830 - val_loss_1: 1.0118 - val_loss_2: 1.0118 - val_acc_ensemble: 0.7830 - val_acc_1: 0.7830 - val_acc_2: 0.7830\n",
      "Epoch 50/50\n",
      "100/100 - 8s - loss_1: 0.0568 - loss_2: 0.0495 - acc_ensemble: 0.9880 - acc_1: 0.9880 - acc_2: 0.9880 - val_loss_1: 0.9649 - val_loss_2: 0.9649 - val_acc_ensemble: 0.7916 - val_acc_1: 0.7916 - val_acc_2: 0.7916\n",
      "sensitivity-2/vb-cifar10-cnn/B2/S1.00\n",
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,32] []  896        input              \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_1         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,32] []  0          bn_1_1             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,32] []  9248       relu_1_1           \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_2         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,32] []  0          bn_1_2             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,32] []  0          relu_1_2           \n",
      "                                    [None,16,16,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,64] []  18496      avg_pool2d_1       \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_1         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,64] []  0          bn_2_1             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,64] []  36928      relu_2_1           \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_2         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,64] []  0          bn_2_2             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [None,8,8,64] []    0          relu_2_2           \n",
      "                                    [None,8,8,64] []                                  \n",
      "--------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [None,8,8,128] []   73856      avg_pool2d_2       \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [None,8,8,128] []   256        conv2d_3_1         \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [None,8,8,128] []   0          bn_3_1             \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [None,8,8,128] []   147584     relu_3_1           \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [None,8,8,128] []   256        conv2d_3_2         \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [None,8,8,128] []   0          bn_3_2             \n",
      "                                    [None,8,8,128] []                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [None,128] []       0          relu_3_2           \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [None,128] []       16512      global_avg_pool2d  \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [None,128] []       256        fc1                \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [None,128] []       0          bn_fc1             \n",
      "                                    [None,128] []                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]           1290       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 305962\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 9s - loss_1: 1.5789 - loss_2: 1.5771 - acc_ensemble: 0.5350 - acc_1: 0.5350 - acc_2: 0.5350 - val_loss_1: 1.2962 - val_loss_2: 1.2962 - val_acc_ensemble: 0.5304 - val_acc_1: 0.5304 - val_acc_2: 0.5304\n",
      "Epoch 2/50\n",
      "100/100 - 8s - loss_1: 1.1809 - loss_2: 1.1850 - acc_ensemble: 0.6290 - acc_1: 0.6290 - acc_2: 0.6290 - val_loss_1: 1.1341 - val_loss_2: 1.1341 - val_acc_ensemble: 0.5934 - val_acc_1: 0.5934 - val_acc_2: 0.5934\n",
      "Epoch 3/50\n",
      "100/100 - 8s - loss_1: 1.0198 - loss_2: 1.0298 - acc_ensemble: 0.6730 - acc_1: 0.6730 - acc_2: 0.6730 - val_loss_1: 1.0045 - val_loss_2: 1.0045 - val_acc_ensemble: 0.6425 - val_acc_1: 0.6425 - val_acc_2: 0.6425\n",
      "Epoch 4/50\n",
      "100/100 - 8s - loss_1: 0.9053 - loss_2: 0.9009 - acc_ensemble: 0.7110 - acc_1: 0.7110 - acc_2: 0.7110 - val_loss_1: 0.9383 - val_loss_2: 0.9383 - val_acc_ensemble: 0.6652 - val_acc_1: 0.6652 - val_acc_2: 0.6652\n",
      "Epoch 5/50\n",
      "100/100 - 8s - loss_1: 0.8038 - loss_2: 0.8180 - acc_ensemble: 0.7310 - acc_1: 0.7310 - acc_2: 0.7310 - val_loss_1: 0.8773 - val_loss_2: 0.8773 - val_acc_ensemble: 0.6888 - val_acc_1: 0.6888 - val_acc_2: 0.6888\n",
      "Epoch 6/50\n",
      "100/100 - 8s - loss_1: 0.7432 - loss_2: 0.7504 - acc_ensemble: 0.7700 - acc_1: 0.7700 - acc_2: 0.7700 - val_loss_1: 0.8568 - val_loss_2: 0.8568 - val_acc_ensemble: 0.7023 - val_acc_1: 0.7023 - val_acc_2: 0.7023\n",
      "Epoch 7/50\n",
      "100/100 - 8s - loss_1: 0.6680 - loss_2: 0.6795 - acc_ensemble: 0.7830 - acc_1: 0.7830 - acc_2: 0.7830 - val_loss_1: 0.8103 - val_loss_2: 0.8103 - val_acc_ensemble: 0.7171 - val_acc_1: 0.7171 - val_acc_2: 0.7171\n",
      "Epoch 8/50\n",
      "100/100 - 8s - loss_1: 0.6455 - loss_2: 0.6359 - acc_ensemble: 0.8020 - acc_1: 0.8020 - acc_2: 0.8020 - val_loss_1: 0.7858 - val_loss_2: 0.7858 - val_acc_ensemble: 0.7273 - val_acc_1: 0.7273 - val_acc_2: 0.7273\n",
      "Epoch 9/50\n",
      "100/100 - 8s - loss_1: 0.5736 - loss_2: 0.5815 - acc_ensemble: 0.8120 - acc_1: 0.8120 - acc_2: 0.8120 - val_loss_1: 0.7579 - val_loss_2: 0.7579 - val_acc_ensemble: 0.7324 - val_acc_1: 0.7324 - val_acc_2: 0.7324\n",
      "Epoch 10/50\n",
      "100/100 - 8s - loss_1: 0.5617 - loss_2: 0.5290 - acc_ensemble: 0.8320 - acc_1: 0.8320 - acc_2: 0.8320 - val_loss_1: 0.7646 - val_loss_2: 0.7646 - val_acc_ensemble: 0.7412 - val_acc_1: 0.7412 - val_acc_2: 0.7412\n",
      "Epoch 11/50\n",
      "100/100 - 8s - loss_1: 0.4937 - loss_2: 0.4985 - acc_ensemble: 0.8380 - acc_1: 0.8380 - acc_2: 0.8380 - val_loss_1: 0.7412 - val_loss_2: 0.7412 - val_acc_ensemble: 0.7464 - val_acc_1: 0.7464 - val_acc_2: 0.7464\n",
      "Epoch 12/50\n",
      "100/100 - 8s - loss_1: 0.4480 - loss_2: 0.4327 - acc_ensemble: 0.8460 - acc_1: 0.8460 - acc_2: 0.8460 - val_loss_1: 0.7515 - val_loss_2: 0.7515 - val_acc_ensemble: 0.7494 - val_acc_1: 0.7494 - val_acc_2: 0.7494\n",
      "Epoch 13/50\n",
      "100/100 - 8s - loss_1: 0.4307 - loss_2: 0.4292 - acc_ensemble: 0.8550 - acc_1: 0.8550 - acc_2: 0.8550 - val_loss_1: 0.7314 - val_loss_2: 0.7314 - val_acc_ensemble: 0.7562 - val_acc_1: 0.7562 - val_acc_2: 0.7562\n",
      "Epoch 14/50\n",
      "100/100 - 8s - loss_1: 0.3991 - loss_2: 0.4000 - acc_ensemble: 0.8810 - acc_1: 0.8810 - acc_2: 0.8810 - val_loss_1: 0.7520 - val_loss_2: 0.7520 - val_acc_ensemble: 0.7500 - val_acc_1: 0.7500 - val_acc_2: 0.7500\n",
      "Epoch 15/50\n",
      "100/100 - 8s - loss_1: 0.3775 - loss_2: 0.3707 - acc_ensemble: 0.9000 - acc_1: 0.9000 - acc_2: 0.9000 - val_loss_1: 0.7127 - val_loss_2: 0.7127 - val_acc_ensemble: 0.7635 - val_acc_1: 0.7635 - val_acc_2: 0.7635\n",
      "Epoch 16/50\n",
      "100/100 - 8s - loss_1: 0.3315 - loss_2: 0.3461 - acc_ensemble: 0.8960 - acc_1: 0.8960 - acc_2: 0.8960 - val_loss_1: 0.7371 - val_loss_2: 0.7371 - val_acc_ensemble: 0.7627 - val_acc_1: 0.7627 - val_acc_2: 0.7627\n",
      "Epoch 17/50\n",
      "100/100 - 8s - loss_1: 0.3154 - loss_2: 0.3251 - acc_ensemble: 0.9010 - acc_1: 0.9010 - acc_2: 0.9010 - val_loss_1: 0.7352 - val_loss_2: 0.7352 - val_acc_ensemble: 0.7604 - val_acc_1: 0.7604 - val_acc_2: 0.7604\n",
      "Epoch 18/50\n",
      "100/100 - 8s - loss_1: 0.2873 - loss_2: 0.2707 - acc_ensemble: 0.9220 - acc_1: 0.9220 - acc_2: 0.9220 - val_loss_1: 0.7203 - val_loss_2: 0.7203 - val_acc_ensemble: 0.7763 - val_acc_1: 0.7763 - val_acc_2: 0.7763\n",
      "Epoch 19/50\n",
      "100/100 - 8s - loss_1: 0.2744 - loss_2: 0.2551 - acc_ensemble: 0.9130 - acc_1: 0.9130 - acc_2: 0.9130 - val_loss_1: 0.7586 - val_loss_2: 0.7586 - val_acc_ensemble: 0.7659 - val_acc_1: 0.7659 - val_acc_2: 0.7659\n",
      "Epoch 20/50\n",
      "100/100 - 8s - loss_1: 0.2447 - loss_2: 0.2363 - acc_ensemble: 0.9160 - acc_1: 0.9160 - acc_2: 0.9160 - val_loss_1: 0.7638 - val_loss_2: 0.7638 - val_acc_ensemble: 0.7650 - val_acc_1: 0.7650 - val_acc_2: 0.7650\n",
      "Epoch 21/50\n",
      "100/100 - 8s - loss_1: 0.2175 - loss_2: 0.2300 - acc_ensemble: 0.9180 - acc_1: 0.9180 - acc_2: 0.9180 - val_loss_1: 0.7677 - val_loss_2: 0.7677 - val_acc_ensemble: 0.7676 - val_acc_1: 0.7676 - val_acc_2: 0.7676\n",
      "Epoch 22/50\n",
      "100/100 - 8s - loss_1: 0.1829 - loss_2: 0.2090 - acc_ensemble: 0.9320 - acc_1: 0.9320 - acc_2: 0.9320 - val_loss_1: 0.7672 - val_loss_2: 0.7672 - val_acc_ensemble: 0.7708 - val_acc_1: 0.7708 - val_acc_2: 0.7708\n",
      "Epoch 23/50\n",
      "100/100 - 8s - loss_1: 0.2003 - loss_2: 0.1801 - acc_ensemble: 0.9360 - acc_1: 0.9360 - acc_2: 0.9360 - val_loss_1: 0.7805 - val_loss_2: 0.7805 - val_acc_ensemble: 0.7745 - val_acc_1: 0.7745 - val_acc_2: 0.7745\n",
      "Epoch 24/50\n",
      "100/100 - 8s - loss_1: 0.1649 - loss_2: 0.1507 - acc_ensemble: 0.9540 - acc_1: 0.9540 - acc_2: 0.9540 - val_loss_1: 0.8036 - val_loss_2: 0.8036 - val_acc_ensemble: 0.7669 - val_acc_1: 0.7669 - val_acc_2: 0.7669\n",
      "Epoch 25/50\n",
      "100/100 - 8s - loss_1: 0.1535 - loss_2: 0.1620 - acc_ensemble: 0.9500 - acc_1: 0.9500 - acc_2: 0.9500 - val_loss_1: 0.8233 - val_loss_2: 0.8233 - val_acc_ensemble: 0.7670 - val_acc_1: 0.7670 - val_acc_2: 0.7670\n",
      "Epoch 26/50\n",
      "100/100 - 8s - loss_1: 0.1458 - loss_2: 0.1488 - acc_ensemble: 0.9520 - acc_1: 0.9520 - acc_2: 0.9520 - val_loss_1: 0.8218 - val_loss_2: 0.8218 - val_acc_ensemble: 0.7726 - val_acc_1: 0.7726 - val_acc_2: 0.7726\n",
      "Epoch 27/50\n",
      "100/100 - 8s - loss_1: 0.1313 - loss_2: 0.1314 - acc_ensemble: 0.9650 - acc_1: 0.9650 - acc_2: 0.9650 - val_loss_1: 0.8241 - val_loss_2: 0.8241 - val_acc_ensemble: 0.7765 - val_acc_1: 0.7765 - val_acc_2: 0.7765\n",
      "Epoch 28/50\n",
      "100/100 - 8s - loss_1: 0.1164 - loss_2: 0.1234 - acc_ensemble: 0.9660 - acc_1: 0.9660 - acc_2: 0.9660 - val_loss_1: 0.8559 - val_loss_2: 0.8559 - val_acc_ensemble: 0.7751 - val_acc_1: 0.7751 - val_acc_2: 0.7751\n",
      "Epoch 29/50\n",
      "100/100 - 8s - loss_1: 0.1083 - loss_2: 0.1050 - acc_ensemble: 0.9640 - acc_1: 0.9640 - acc_2: 0.9640 - val_loss_1: 0.8662 - val_loss_2: 0.8662 - val_acc_ensemble: 0.7743 - val_acc_1: 0.7743 - val_acc_2: 0.7743\n",
      "Epoch 30/50\n",
      "100/100 - 8s - loss_1: 0.0975 - loss_2: 0.0981 - acc_ensemble: 0.9630 - acc_1: 0.9630 - acc_2: 0.9630 - val_loss_1: 0.8777 - val_loss_2: 0.8777 - val_acc_ensemble: 0.7735 - val_acc_1: 0.7735 - val_acc_2: 0.7735\n",
      "Epoch 31/50\n",
      "100/100 - 8s - loss_1: 0.1058 - loss_2: 0.1056 - acc_ensemble: 0.9620 - acc_1: 0.9620 - acc_2: 0.9620 - val_loss_1: 0.8899 - val_loss_2: 0.8899 - val_acc_ensemble: 0.7720 - val_acc_1: 0.7720 - val_acc_2: 0.7720\n",
      "Epoch 32/50\n",
      "100/100 - 8s - loss_1: 0.0930 - loss_2: 0.0981 - acc_ensemble: 0.9740 - acc_1: 0.9740 - acc_2: 0.9740 - val_loss_1: 0.9162 - val_loss_2: 0.9162 - val_acc_ensemble: 0.7730 - val_acc_1: 0.7730 - val_acc_2: 0.7730\n",
      "Epoch 33/50\n",
      "100/100 - 8s - loss_1: 0.0887 - loss_2: 0.0965 - acc_ensemble: 0.9770 - acc_1: 0.9770 - acc_2: 0.9770 - val_loss_1: 0.9148 - val_loss_2: 0.9148 - val_acc_ensemble: 0.7774 - val_acc_1: 0.7774 - val_acc_2: 0.7774\n",
      "Epoch 34/50\n",
      "100/100 - 8s - loss_1: 0.0894 - loss_2: 0.0844 - acc_ensemble: 0.9640 - acc_1: 0.9640 - acc_2: 0.9640 - val_loss_1: 0.9441 - val_loss_2: 0.9441 - val_acc_ensemble: 0.7754 - val_acc_1: 0.7754 - val_acc_2: 0.7754\n",
      "Epoch 35/50\n",
      "100/100 - 8s - loss_1: 0.0751 - loss_2: 0.0809 - acc_ensemble: 0.9680 - acc_1: 0.9680 - acc_2: 0.9680 - val_loss_1: 0.9686 - val_loss_2: 0.9686 - val_acc_ensemble: 0.7725 - val_acc_1: 0.7725 - val_acc_2: 0.7725\n",
      "Epoch 36/50\n",
      "100/100 - 8s - loss_1: 0.0794 - loss_2: 0.0658 - acc_ensemble: 0.9740 - acc_1: 0.9740 - acc_2: 0.9740 - val_loss_1: 0.9569 - val_loss_2: 0.9569 - val_acc_ensemble: 0.7744 - val_acc_1: 0.7744 - val_acc_2: 0.7744\n",
      "Epoch 37/50\n",
      "100/100 - 8s - loss_1: 0.0706 - loss_2: 0.0698 - acc_ensemble: 0.9850 - acc_1: 0.9850 - acc_2: 0.9850 - val_loss_1: 0.9855 - val_loss_2: 0.9855 - val_acc_ensemble: 0.7715 - val_acc_1: 0.7715 - val_acc_2: 0.7715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "100/100 - 8s - loss_1: 0.0681 - loss_2: 0.0631 - acc_ensemble: 0.9830 - acc_1: 0.9830 - acc_2: 0.9830 - val_loss_1: 0.9719 - val_loss_2: 0.9719 - val_acc_ensemble: 0.7760 - val_acc_1: 0.7760 - val_acc_2: 0.7760\n",
      "Epoch 39/50\n",
      "100/100 - 8s - loss_1: 0.0656 - loss_2: 0.0658 - acc_ensemble: 0.9750 - acc_1: 0.9750 - acc_2: 0.9750 - val_loss_1: 0.9971 - val_loss_2: 0.9971 - val_acc_ensemble: 0.7725 - val_acc_1: 0.7725 - val_acc_2: 0.7725\n",
      "Epoch 40/50\n",
      "100/100 - 8s - loss_1: 0.0716 - loss_2: 0.0648 - acc_ensemble: 0.9790 - acc_1: 0.9790 - acc_2: 0.9790 - val_loss_1: 0.9838 - val_loss_2: 0.9838 - val_acc_ensemble: 0.7747 - val_acc_1: 0.7747 - val_acc_2: 0.7747\n",
      "Epoch 41/50\n",
      "100/100 - 8s - loss_1: 0.0652 - loss_2: 0.0541 - acc_ensemble: 0.9770 - acc_1: 0.9770 - acc_2: 0.9770 - val_loss_1: 0.9989 - val_loss_2: 0.9989 - val_acc_ensemble: 0.7749 - val_acc_1: 0.7749 - val_acc_2: 0.7749\n",
      "Epoch 42/50\n",
      "100/100 - 8s - loss_1: 0.0668 - loss_2: 0.0658 - acc_ensemble: 0.9750 - acc_1: 0.9750 - acc_2: 0.9750 - val_loss_1: 1.0238 - val_loss_2: 1.0238 - val_acc_ensemble: 0.7742 - val_acc_1: 0.7742 - val_acc_2: 0.7742\n",
      "Epoch 43/50\n",
      "100/100 - 8s - loss_1: 0.0621 - loss_2: 0.0666 - acc_ensemble: 0.9710 - acc_1: 0.9710 - acc_2: 0.9710 - val_loss_1: 1.0301 - val_loss_2: 1.0301 - val_acc_ensemble: 0.7728 - val_acc_1: 0.7728 - val_acc_2: 0.7728\n",
      "Epoch 44/50\n",
      "100/100 - 8s - loss_1: 0.0604 - loss_2: 0.0657 - acc_ensemble: 0.9880 - acc_1: 0.9880 - acc_2: 0.9880 - val_loss_1: 1.0219 - val_loss_2: 1.0219 - val_acc_ensemble: 0.7765 - val_acc_1: 0.7765 - val_acc_2: 0.7765\n",
      "Epoch 45/50\n",
      "100/100 - 8s - loss_1: 0.0552 - loss_2: 0.0608 - acc_ensemble: 0.9780 - acc_1: 0.9780 - acc_2: 0.9780 - val_loss_1: 1.0426 - val_loss_2: 1.0426 - val_acc_ensemble: 0.7727 - val_acc_1: 0.7727 - val_acc_2: 0.7727\n",
      "Epoch 46/50\n",
      "100/100 - 8s - loss_1: 0.0463 - loss_2: 0.0556 - acc_ensemble: 0.9870 - acc_1: 0.9870 - acc_2: 0.9870 - val_loss_1: 1.0190 - val_loss_2: 1.0190 - val_acc_ensemble: 0.7791 - val_acc_1: 0.7791 - val_acc_2: 0.7791\n",
      "Epoch 47/50\n",
      "100/100 - 8s - loss_1: 0.0381 - loss_2: 0.0396 - acc_ensemble: 0.9820 - acc_1: 0.9820 - acc_2: 0.9820 - val_loss_1: 1.0693 - val_loss_2: 1.0693 - val_acc_ensemble: 0.7782 - val_acc_1: 0.7782 - val_acc_2: 0.7782\n",
      "Epoch 48/50\n",
      "100/100 - 8s - loss_1: 0.0437 - loss_2: 0.0379 - acc_ensemble: 0.9840 - acc_1: 0.9840 - acc_2: 0.9840 - val_loss_1: 1.0612 - val_loss_2: 1.0612 - val_acc_ensemble: 0.7796 - val_acc_1: 0.7796 - val_acc_2: 0.7796\n",
      "Epoch 49/50\n",
      "100/100 - 8s - loss_1: 0.0529 - loss_2: 0.0544 - acc_ensemble: 0.9790 - acc_1: 0.9790 - acc_2: 0.9790 - val_loss_1: 1.0794 - val_loss_2: 1.0794 - val_acc_ensemble: 0.7792 - val_acc_1: 0.7792 - val_acc_2: 0.7792\n",
      "Epoch 50/50\n",
      "100/100 - 8s - loss_1: 0.0564 - loss_2: 0.0561 - acc_ensemble: 0.9740 - acc_1: 0.9740 - acc_2: 0.9740 - val_loss_1: 1.1026 - val_loss_2: 1.1026 - val_acc_ensemble: 0.7762 - val_acc_1: 0.7762 - val_acc_2: 0.7762\n",
      "sensitivity-2/vb-cifar10-cnn/B3/S0.00\n",
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [] [None,32,32,32]  2688       input              \n",
      "                                    [] [None,32,32,32]                                \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [] [None,32,32,32]  192        conv2d_1_1         \n",
      "                                    [] [None,32,32,32]                                \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [] [None,32,32,32]  0          bn_1_1             \n",
      "                                    [] [None,32,32,32]                                \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [] [None,32,32,32]  27744      relu_1_1           \n",
      "                                    [] [None,32,32,32]                                \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [] [None,32,32,32]  192        conv2d_1_2         \n",
      "                                    [] [None,32,32,32]                                \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [] [None,32,32,32]  0          bn_1_2             \n",
      "                                    [] [None,32,32,32]                                \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [] [None,16,16,32]  0          relu_1_2           \n",
      "                                    [] [None,16,16,32]                                \n",
      "                                    [] [None,16,16,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [] [None,16,16,64]  55488      avg_pool2d_1       \n",
      "                                    [] [None,16,16,64]                                \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [] [None,16,16,64]  384        conv2d_2_1         \n",
      "                                    [] [None,16,16,64]                                \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [] [None,16,16,64]  0          bn_2_1             \n",
      "                                    [] [None,16,16,64]                                \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [] [None,16,16,64]  110784     relu_2_1           \n",
      "                                    [] [None,16,16,64]                                \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [] [None,16,16,64]  384        conv2d_2_2         \n",
      "                                    [] [None,16,16,64]                                \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [] [None,16,16,64]  0          bn_2_2             \n",
      "                                    [] [None,16,16,64]                                \n",
      "                                    [] [None,16,16,64]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  avg_pool2d_2 (AveragePooling2D  [] [None,8,8,64]    0          relu_2_2           \n",
      "                                    [] [None,8,8,64]                                  \n",
      "                                    [] [None,8,8,64]                                  \n",
      "--------------------------------------------------------------------------------------\n",
      "14  conv2d_3_1 (Conv2D)             [] [None,8,8,128]   221568     avg_pool2d_2       \n",
      "                                    [] [None,8,8,128]                                 \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_3_1 (BatchNormalization)     [] [None,8,8,128]   768        conv2d_3_1         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_3_1 (Activation)           [] [None,8,8,128]   0          bn_3_1             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "17  conv2d_3_2 (Conv2D)             [] [None,8,8,128]   442752     relu_3_1           \n",
      "                                    [] [None,8,8,128]                                 \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "18  bn_3_2 (BatchNormalization)     [] [None,8,8,128]   768        conv2d_3_2         \n",
      "                                    [] [None,8,8,128]                                 \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "19  relu_3_2 (Activation)           [] [None,8,8,128]   0          bn_3_2             \n",
      "                                    [] [None,8,8,128]                                 \n",
      "                                    [] [None,8,8,128]                                 \n",
      "--------------------------------------------------------------------------------------\n",
      "20  global_avg_pool2d (GlobalAvera  [] [None,128]       0          relu_3_2           \n",
      "                                    [] [None,128]                                     \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "21  fc1 (Dense)                     [] [None,128]       49536      global_avg_pool2d  \n",
      "                                    [] [None,128]                                     \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "22  bn_fc1 (BatchNormalization)     [] [None,128]       768        fc1                \n",
      "                                    [] [None,128]                                     \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "23  relu_fc1 (Activation)           [] [None,128]       0          bn_fc1             \n",
      "                                    [] [None,128]                                     \n",
      "                                    [] [None,128]                                     \n",
      "--------------------------------------------------------------------------------------\n",
      "24  output (Dense)                  [None,10]           3870       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 917886\n",
      "50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 25s - loss_1: 1.6357 - loss_2: 1.6519 - loss_3: 1.6312 - acc_ensemble: 0.5250 - acc_1: 0.4840 - acc_2: 0.5030 - acc_3: 0.4940 - val_loss_1: 1.3648 - val_loss_2: 1.4200 - val_loss_3: 1.4112 - val_acc_ensemble: 0.5369 - val_acc_1: 0.4996 - val_acc_2: 0.4854 - val_acc_3: 0.4805\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Not a ndarray.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Not a ndarray.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-62050e109b0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mshared_frac\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_branches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_frac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# history = train(n_branches=1, shared_frac=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-efb747c7741a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_branches, shared_frac, model_id)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir -p '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     history = model.fit(EPOCHS, STEPS_PER_EPOCH, train_dict=train_dict,\n\u001b[0;32m---> 28\u001b[0;31m                         val_dict=val_dict) #, log_path=model_path)\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/vbranch/vbranch/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, steps_per_epoch, train_dict, val_dict, log_path, call_step, verbose)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mlog_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedulers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_branches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             self.assign_ops, call_step=call_step, verbose=verbose)\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/vbranch/vbranch/engine/training.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(train_init_op, test_init_op, train_dict, epochs, steps_per_epoch, loss_op, train_op, val_dict, save_model_path, callbacks, schedulers, n_branches, assign_ops, call_step, verbose)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mprogbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_init_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0msched_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mschedulers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Not a ndarray."
     ]
    }
   ],
   "source": [
    "for n_branches in range(2, 5):\n",
    "    for shared_frac in [0., 0.25, 0.5, 0.75, 1.]:\n",
    "        for t in range(4):\n",
    "            train(n_branches, shared_frac, model_id=t+1)\n",
    "# history = train(n_branches=1, shared_frac=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vbranch.utils.generic import get_model_path, get_vb_model_path\n",
    "from vbranch.utils.test import baseline_classification, compute_correlation_strength, compute_acc_from_logits\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and Strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we can compute the correlation between models and their strength. The formulas used are from the Random Forest paper:\n",
    "\n",
    "https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_strength(n_branches, shared_frac, model_id):\n",
    "    model_path = os.path.join('models', path(n_branches, shared_frac), \n",
    "                              'model_{}'.format(model_id))\n",
    "\n",
    "    test_init_ops = []\n",
    "    tensors = []\n",
    "    for i in range(n_branches):\n",
    "        test_init_ops.append('test_init_op_{}'.format(i+1))\n",
    "        tensors.append('model/output/vb{}/output:0'.format(i+1))\n",
    "\n",
    "    with TFSessionGrow() as sess:\n",
    "        restore_sess(sess, model_path)\n",
    "        sess.run(test_init_ops, feed_dict={'x:0':X_test, 'y:0': y_test, \n",
    "                                    'batch_size:0':len(X_test)})\n",
    "        outputs = sess.run(tensors)\n",
    "\n",
    "    return compute_correlation_strength(outputs, y_test, NUM_CLASSES, n_branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gong/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.00/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.00/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.25/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.25/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.25/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.25/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.50/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.50/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.50/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.50/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.75/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.75/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.75/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S0.75/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S1.00/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S1.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S1.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B2/S1.00/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.00/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.00/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.25/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.25/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.25/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.25/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.50/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.50/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.50/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.50/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.75/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.75/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.75/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S0.75/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S1.00/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S1.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S1.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B3/S1.00/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.00/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.00/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.25/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.25/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.25/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.25/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.50/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.50/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.50/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.50/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.75/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.75/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.75/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S0.75/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S1.00/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S1.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S1.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B4/S1.00/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.00/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.00/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.25/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.25/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.25/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.25/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.50/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.50/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.50/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.50/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.75/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.75/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.75/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S0.75/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S1.00/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S1.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S1.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B5/S1.00/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.00/model_1/ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.00/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.25/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.25/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.25/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.25/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.50/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.50/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.50/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.50/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.75/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.75/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.75/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S0.75/model_4/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S1.00/model_1/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S1.00/model_2/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S1.00/model_3/ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/sensitivity/vb-mnist-fcn3A/B6/S1.00/model_4/ckpt\n"
     ]
    }
   ],
   "source": [
    "correlation_results = {}\n",
    "strength_results = {}\n",
    "\n",
    "# num_branches = 4\n",
    "shared_frac_list = [0., 0.25, 0.5, 0.75, 1.]\n",
    "# shared_correlation_list = []\n",
    "# shared_strength_list = []\n",
    "n_trials = 4\n",
    "\n",
    "for b in range(2, 7):\n",
    "    correlation_results[b] = {}\n",
    "    strength_results[b] = {}\n",
    "    \n",
    "    for shared in shared_frac_list:\n",
    "        correlation_list = []\n",
    "        strength_list = []\n",
    "\n",
    "        for model_id in range(1, n_trials + 1):\n",
    "            tf.reset_default_graph()\n",
    "            c, s = correlation_strength(b, shared, model_id)\n",
    "            correlation_list.append(c)\n",
    "            strength_list.append(s)\n",
    "\n",
    "        correlation_results[b][shared] = [np.mean(correlation_list), np.std(correlation_list)]\n",
    "        strength_results[b][shared] = [np.mean(strength_list), np.std(strength_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/sensitivity/correlation-mnist-{}.json'.format(ARCHITECTURE), 'w') as f:\n",
    "    json.dump(correlation_results, f, indent=4)\n",
    "with open('results/sensitivity/strength-mnist-{}.json'.format(ARCHITECTURE), 'w') as f:\n",
    "    json.dump(strength_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = ['fcn', 'fcn2', 'fcn3', 'fcn2A', 'fcn3A']\n",
    "correlation = []\n",
    "strength = []\n",
    "\n",
    "for arch in architecture:\n",
    "    with open('results/sensitivity/correlation-mnist-{}.json'.format(arch), 'r') as f:\n",
    "        correlation.append(json.load(f))\n",
    "    with open('results/sensitivity/strength-mnist-{}.json'.format(arch), 'r') as f:\n",
    "        strength.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_strength(n_branches):\n",
    "    def mean_std(data):\n",
    "        mean = []\n",
    "        std = []\n",
    "        for frac in shared_frac_list:\n",
    "            mean.append(data[str(frac)][0])\n",
    "            std.append(data[str(frac)][1])\n",
    "        return np.array(mean), np.array(std)\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    for i, arch in enumerate(architecture):\n",
    "        data = correlation[i][str(n_branches)]\n",
    "        mean, std = mean_std(data)    \n",
    "        plt.errorbar(shared_frac_list, mean, 2*std / np.sqrt(n_trials), label=arch)\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.subplot(1,2,2)\n",
    "    for i, arch in enumerate(architecture):\n",
    "        data = strength[i][str(n_branches)]\n",
    "        mean, std = mean_std(data)    \n",
    "        plt.errorbar(shared_frac_list, mean, 2*std / np.sqrt(n_trials), label=arch)    \n",
    "        plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_corr_strength(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 18:55:04.781267 140469450823424 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/utils/training.py:130: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0801 18:55:04.802347 140469450823424 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/utils/training.py:152: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "W0801 18:55:04.810274 140469450823424 deprecation.py:323] From /home/gong/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
      "W0801 18:55:04.812236 140469450823424 deprecation.py:323] From /home/gong/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
      "W0801 18:55:04.814125 140469450823424 deprecation.py:323] From /home/gong/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
      "W0801 18:55:04.832661 140469450823424 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/vb_layers/core.py:63: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "W0801 18:55:04.833438 140469450823424 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/vb_layers/core.py:92: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0801 18:55:04.835860 140469450823424 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/layers/convolutional.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0801 18:55:04.836887 140469450823424 deprecation.py:506] From /home/gong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0801 18:55:04.945193 140469450823424 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/layers/pooling.py:22: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "W0801 18:55:05.110590 140469450823424 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/layers/core.py:83: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
      "\n",
      "W0801 18:55:05.167821 140469450823424 deprecation_wrapper.py:119] From /home/gong/research/vbranch/vbranch/vb_layers/core.py:132: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [] [None,32,32,32]  1792       input              \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_1         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [] [None,32,32,32]  0          bn_1_1             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [] [None,32,32,32]  18496      relu_1_1           \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [] [None,32,32,32]  128        conv2d_1_2         \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [] [None,32,32,32]  0          bn_1_2             \n",
      "                                    [] [None,32,32,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [] [None,16,16,32]  0          relu_1_2           \n",
      "                                    [] [None,16,16,32]                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,64] []  18496      avg_pool2d_1       \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_1         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,64] []  0          bn_2_1             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,64] []  36928      relu_2_1           \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_2         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,64] []  0          bn_2_2             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  global_avg_pool2d (GlobalAvera  [None,64] []        0          relu_2_2           \n",
      "                                    [None,64] []                                      \n",
      "--------------------------------------------------------------------------------------\n",
      "14  fc1 (Dense)                     [None,64] []        4160       global_avg_pool2d  \n",
      "                                    [None,64] []                                      \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_fc1 (BatchNormalization)     [None,64] []        128        fc1                \n",
      "                                    [None,64] []                                      \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_fc1 (Activation)           [None,64] []        0          bn_fc1             \n",
      "                                    [None,64] []                                      \n",
      "--------------------------------------------------------------------------------------\n",
      "17  output (Dense)                  [None,10]           1300       relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 81812\n",
      "i   Layer name                      Output shape                    Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,8] [None,32,32,24]  1568       input              \n",
      "                                    [None,32,32,8] [None,32,32,24]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,8] [None,32,32,24]  112        conv2d_1_1         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,8] [None,32,32,24]  0          bn_1_1             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,8] [None,32,32,24]  17976      relu_1_1           \n",
      "                                    [None,32,32,8] [None,32,32,24]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,8] [None,32,32,24]  112        conv2d_1_2         \n",
      "                                    [None,32,32,8] [None,32,32,24]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,8] [None,32,32,24]  0          bn_1_2             \n",
      "                                    [None,32,32,8] [None,32,32,24]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,8] [None,16,16,24]  0          relu_1_2           \n",
      "                                    [None,16,16,8] [None,16,16,24]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,64] []              18496      avg_pool2d_1       \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,64] []              128        conv2d_2_1         \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,64] []              0          bn_2_1             \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,64] []              36928      relu_2_1           \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,64] []              128        conv2d_2_2         \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,64] []              0          bn_2_2             \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "13  global_avg_pool2d (GlobalAvera  [None,64] []                    0          relu_2_2           \n",
      "                                    [None,64] []                                                  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "14  fc1 (Dense)                     [None,64] []                    4160       global_avg_pool2d  \n",
      "                                    [None,64] []                                                  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "15  bn_fc1 (BatchNormalization)     [None,64] []                    128        fc1                \n",
      "                                    [None,64] []                                                  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "16  relu_fc1 (Activation)           [None,64] []                    0          bn_fc1             \n",
      "                                    [None,64] []                                                  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "17  output (Dense)                  [None,10]                       1170       relu_fc1           \n",
      "                                    [None,10]                                                     \n",
      "--------------------------------------------------------------------------------------------------\n",
      "Total parameters: 80906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i   Layer name                      Output shape                     Num param  Inbound            \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                 \n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,16] [None,32,32,16]  1344       input              \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_1         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_1             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,16] [None,32,32,16]  16240      relu_1_1           \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,16] [None,32,32,16]  96         conv2d_1_2         \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,16] [None,32,32,16]  0          bn_1_2             \n",
      "                                    [None,32,32,16] [None,32,32,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,16] [None,16,16,16]  0          relu_1_2           \n",
      "                                    [None,16,16,16] [None,16,16,16]                                \n",
      "---------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,64] []               18496      avg_pool2d_1       \n",
      "                                    [None,16,16,64] []                                             \n",
      "---------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,64] []               128        conv2d_2_1         \n",
      "                                    [None,16,16,64] []                                             \n",
      "---------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,64] []               0          bn_2_1             \n",
      "                                    [None,16,16,64] []                                             \n",
      "---------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,64] []               36928      relu_2_1           \n",
      "                                    [None,16,16,64] []                                             \n",
      "---------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,64] []               128        conv2d_2_2         \n",
      "                                    [None,16,16,64] []                                             \n",
      "---------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,64] []               0          bn_2_2             \n",
      "                                    [None,16,16,64] []                                             \n",
      "---------------------------------------------------------------------------------------------------\n",
      "13  global_avg_pool2d (GlobalAvera  [None,64] []                     0          relu_2_2           \n",
      "                                    [None,64] []                                                   \n",
      "---------------------------------------------------------------------------------------------------\n",
      "14  fc1 (Dense)                     [None,64] []                     4160       global_avg_pool2d  \n",
      "                                    [None,64] []                                                   \n",
      "---------------------------------------------------------------------------------------------------\n",
      "15  bn_fc1 (BatchNormalization)     [None,64] []                     128        fc1                \n",
      "                                    [None,64] []                                                   \n",
      "---------------------------------------------------------------------------------------------------\n",
      "16  relu_fc1 (Activation)           [None,64] []                     0          bn_fc1             \n",
      "                                    [None,64] []                                                   \n",
      "---------------------------------------------------------------------------------------------------\n",
      "17  output (Dense)                  [None,10]                        975        relu_fc1           \n",
      "                                    [None,10]                                                      \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total parameters: 78719\n",
      "i   Layer name                      Output shape                    Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,24] [None,32,32,8]  1120       input              \n",
      "                                    [None,32,32,24] [None,32,32,8]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,24] [None,32,32,8]  80         conv2d_1_1         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,24] [None,32,32,8]  0          bn_1_1             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,24] [None,32,32,8]  13352      relu_1_1           \n",
      "                                    [None,32,32,24] [None,32,32,8]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,24] [None,32,32,8]  80         conv2d_1_2         \n",
      "                                    [None,32,32,24] [None,32,32,8]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,24] [None,32,32,8]  0          bn_1_2             \n",
      "                                    [None,32,32,24] [None,32,32,8]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,24] [None,16,16,8]  0          relu_1_2           \n",
      "                                    [None,16,16,24] [None,16,16,8]                                \n",
      "--------------------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,64] []              18496      avg_pool2d_1       \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,64] []              128        conv2d_2_1         \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,64] []              0          bn_2_1             \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,64] []              36928      relu_2_1           \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,64] []              128        conv2d_2_2         \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,64] []              0          bn_2_2             \n",
      "                                    [None,16,16,64] []                                            \n",
      "--------------------------------------------------------------------------------------------------\n",
      "13  global_avg_pool2d (GlobalAvera  [None,64] []                    0          relu_2_2           \n",
      "                                    [None,64] []                                                  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "14  fc1 (Dense)                     [None,64] []                    4160       global_avg_pool2d  \n",
      "                                    [None,64] []                                                  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "15  bn_fc1 (BatchNormalization)     [None,64] []                    128        fc1                \n",
      "                                    [None,64] []                                                  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "16  relu_fc1 (Activation)           [None,64] []                    0          bn_fc1             \n",
      "                                    [None,64] []                                                  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "17  output (Dense)                  [None,10]                       845        relu_fc1           \n",
      "                                    [None,10]                                                     \n",
      "--------------------------------------------------------------------------------------------------\n",
      "Total parameters: 75445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i   Layer name                      Output shape        Num param  Inbound            \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "    Input                           [None,32,32,3]                                    \n",
      "--------------------------------------------------------------------------------------\n",
      "0   conv2d_1_1 (Conv2D)             [None,32,32,32] []  896        input              \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "1   bn_1_1 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_1         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "2   relu_1_1 (Activation)           [None,32,32,32] []  0          bn_1_1             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "3   conv2d_1_2 (Conv2D)             [None,32,32,32] []  9248       relu_1_1           \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "4   bn_1_2 (BatchNormalization)     [None,32,32,32] []  64         conv2d_1_2         \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "5   relu_1_2 (Activation)           [None,32,32,32] []  0          bn_1_2             \n",
      "                                    [None,32,32,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "6   avg_pool2d_1 (AveragePooling2D  [None,16,16,32] []  0          relu_1_2           \n",
      "                                    [None,16,16,32] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "7   conv2d_2_1 (Conv2D)             [None,16,16,64] []  18496      avg_pool2d_1       \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "8   bn_2_1 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_1         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "9   relu_2_1 (Activation)           [None,16,16,64] []  0          bn_2_1             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "10  conv2d_2_2 (Conv2D)             [None,16,16,64] []  36928      relu_2_1           \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "11  bn_2_2 (BatchNormalization)     [None,16,16,64] []  128        conv2d_2_2         \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "12  relu_2_2 (Activation)           [None,16,16,64] []  0          bn_2_2             \n",
      "                                    [None,16,16,64] []                                \n",
      "--------------------------------------------------------------------------------------\n",
      "13  global_avg_pool2d (GlobalAvera  [None,64] []        0          relu_2_2           \n",
      "                                    [None,64] []                                      \n",
      "--------------------------------------------------------------------------------------\n",
      "14  fc1 (Dense)                     [None,64] []        4160       global_avg_pool2d  \n",
      "                                    [None,64] []                                      \n",
      "--------------------------------------------------------------------------------------\n",
      "15  bn_fc1 (BatchNormalization)     [None,64] []        128        fc1                \n",
      "                                    [None,64] []                                      \n",
      "--------------------------------------------------------------------------------------\n",
      "16  relu_fc1 (Activation)           [None,64] []        0          bn_fc1             \n",
      "                                    [None,64] []                                      \n",
      "--------------------------------------------------------------------------------------\n",
      "17  output (Dense)                  [None,10]           650        relu_fc1           \n",
      "                                    [None,10]                                         \n",
      "--------------------------------------------------------------------------------------\n",
      "Total parameters: 70890\n"
     ]
    }
   ],
   "source": [
    "# Vbranch params\n",
    "shared_frac_list = [0., 0.25, 0.5, 0.75, 1.]\n",
    "num_branches = 2\n",
    "\n",
    "vbranch_params = []\n",
    "for frac in shared_frac_list:\n",
    "    tf.reset_default_graph()\n",
    "    inputs = tf.placeholder('float32', [None, 32,32,3])\n",
    "    model = build_model(num_branches, frac)\n",
    "    model.summary()\n",
    "    vbranch_params.append(model.count_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ratio = [p / vbranch_params[-1] for p in vbranch_params]\n",
    "ideal_ratio = num_branches - np.array(shared_frac_list)**2 * (num_branches-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(shared_frac_list, param_ratio, color='orange')\n",
    "# plt.plot(shared_frac_list, [1]*len(shared_frac_list))\n",
    "plt.plot(shared_frac_list, ideal_ratio)\n",
    "\n",
    "plt.xlabel('shared frac')\n",
    "plt.ylabel('params / baseline')\n",
    "plt.title('{} parameter count'.format(ARCHITECTURE))\n",
    "\n",
    "plt.savefig('figs/cnn-small-parameter-count.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
