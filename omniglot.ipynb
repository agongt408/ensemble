{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omnigot One-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import cv2\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vbranch\n",
    "from vbranch.data import Omniglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "model_id = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./omniglot/python/images_background'):\n",
    "    with zipfile.ZipFile('./omniglot/python/images_background.zip','r') as zip_ref:\n",
    "        zip_ref.extractall('./omniglot/python')\n",
    "        \n",
    "if not os.path.isdir('./omniglot/python/images_evaluation'):\n",
    "    with zipfile.ZipFile('./omniglot/python/images_evaluation.zip','r') as zip_ref:\n",
    "        zip_ref.extractall('./omniglot/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = Omniglot('./omniglot/python/images_background/')\n",
    "test_generator = Omniglot('./omniglot/python/images_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_generator.next(4, 4, 4, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(batch[2, 3, 0].squeeze(), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 100\n",
    "model_path = './models/model_' + str(model_id)\n",
    "A, P, K = 4, 4, 4\n",
    "output_dim = 128\n",
    "input_dim = [None, 105, 105, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(A, P, K):\n",
    "    def func():\n",
    "        while True:\n",
    "            batch = train_generator.next(A, P, K)\n",
    "            batch = batch.astype('float32')\n",
    "            yield batch\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(batch_gen(A, P, K), 'float32', \n",
    "                                                 output_shapes=input_dim)\n",
    "\n",
    "# Dataset for feeding non-triplet batched images from memory\n",
    "x = tf.placeholder('float32', input_dim, name='x')\n",
    "batch_size = tf.placeholder('int64', name='batch_size')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(x).batch(batch_size)\n",
    "\n",
    "iter_ = tf.data.Iterator.from_structure('float32', input_dim)\n",
    "train_init_op = iter_.make_initializer(train_dataset)\n",
    "test_init_op = iter_.make_initializer(test_dataset, name='test_init_op')\n",
    "\n",
    "inputs = iter_.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('model_' + str(model_id)):\n",
    "    model = vbranch.models.simple_cnn(inputs, output_dim, 32, 64, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i   Layer name                  Output shape          Parameters\n",
      "----------------------------------------------------------------\n",
      "    Input                [None, 105, 105, 1]                    \n",
      "----------------------------------------------------------------\n",
      "0   conv2d_1_1          [None, 103, 103, 32]                 320\n",
      "----------------------------------------------------------------\n",
      "1   bn_1_1              [None, 103, 103, 32]                  64\n",
      "----------------------------------------------------------------\n",
      "2   relu_1_1            [None, 103, 103, 32]                   0\n",
      "----------------------------------------------------------------\n",
      "3   conv2d_1_2          [None, 101, 101, 32]                9248\n",
      "----------------------------------------------------------------\n",
      "4   bn_1_2              [None, 101, 101, 32]                  64\n",
      "----------------------------------------------------------------\n",
      "5   relu_1_2            [None, 101, 101, 32]                   0\n",
      "----------------------------------------------------------------\n",
      "6   avg_pool2d_1          [None, 50, 50, 32]                   0\n",
      "----------------------------------------------------------------\n",
      "7   conv2d_2_1            [None, 48, 48, 64]               18496\n",
      "----------------------------------------------------------------\n",
      "8   bn_2_1                [None, 48, 48, 64]                 128\n",
      "----------------------------------------------------------------\n",
      "9   relu_2_1              [None, 48, 48, 64]                   0\n",
      "----------------------------------------------------------------\n",
      "10  conv2d_2_2            [None, 46, 46, 64]               36928\n",
      "----------------------------------------------------------------\n",
      "11  bn_2_2                [None, 46, 46, 64]                 128\n",
      "----------------------------------------------------------------\n",
      "12  relu_2_2              [None, 46, 46, 64]                   0\n",
      "----------------------------------------------------------------\n",
      "13  avg_pool2d_2          [None, 23, 23, 64]                   0\n",
      "----------------------------------------------------------------\n",
      "14  conv2d_3_1           [None, 21, 21, 128]               73856\n",
      "----------------------------------------------------------------\n",
      "15  bn_3_1               [None, 21, 21, 128]                 256\n",
      "----------------------------------------------------------------\n",
      "16  relu_3_1             [None, 21, 21, 128]                   0\n",
      "----------------------------------------------------------------\n",
      "17  conv2d_3_2           [None, 19, 19, 128]              147584\n",
      "----------------------------------------------------------------\n",
      "18  bn_3_2               [None, 19, 19, 128]                 256\n",
      "----------------------------------------------------------------\n",
      "19  relu_3_2             [None, 19, 19, 128]                   0\n",
      "----------------------------------------------------------------\n",
      "20  global_avg_pool2d            [None, 128]                   0\n",
      "----------------------------------------------------------------\n",
      "21  pred                         [None, 128]               16512\n",
      "----------------------------------------------------------------\n",
      "Parameters: 303840\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'model_1/conv2d_1_1_f:0' shape=(3, 3, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_1_1_b:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_1_1_scale:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_1_1_beta:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_1_2_f:0' shape=(3, 3, 32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_1_2_b:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_1_2_scale:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_1_2_beta:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_2_1_f:0' shape=(3, 3, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_2_1_b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_2_1_scale:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_2_1_beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_2_2_f:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_2_2_b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_2_2_scale:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_2_2_beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_3_1_f:0' shape=(3, 3, 64, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_3_1_b:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_3_1_scale:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_3_1_beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_3_2_f:0' shape=(3, 3, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/conv2d_3_2_b:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_3_2_scale:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/bn_3_2_beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/pred_w:0' shape=(128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'model_1/pred_b:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = vbranch.losses.triplet_omniglot(model.output, A, P, K, 'loss')\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  7/100 [=>............................] - ETA: 9:30 - loss: 58.2008"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(train_init_op)\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "        print(\"Epoch {}/{}\".format(e + 1, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(STEPS_PER_EPOCH)\n",
    "        \n",
    "        for i in range(STEPS_PER_EPOCH):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            progbar.update(i + 1, values=[(\"loss\", loss_value)])\n",
    "    \n",
    "    if save:\n",
    "        saver = tf.train.Saver()\n",
    "        path = os.path.join(model_path, 'ckpt')\n",
    "        saver.save(sess, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_sess(sess, model_path):\n",
    "    meta_path = os.path.join(model_path, 'ckpt.meta')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "\n",
    "    imported_graph = tf.train.import_meta_graph(meta_path)\n",
    "    imported_graph.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run(n_run):\n",
    "    all_runs = './omniglot/python/one-shot-classification/all_runs'\n",
    "    \n",
    "    run_path = os.path.join(all_runs,'run%02d'%n_run,'class_labels.txt')\n",
    "    with open(run_path) as f:\n",
    "        content = f.read().splitlines()\n",
    "\n",
    "    pairs = [line.split() for line in content]\n",
    "    test_files  = [pair[0] for pair in pairs]\n",
    "    train_files = [pair[1] for pair in pairs]\n",
    "\n",
    "    answers_files = copy.copy(train_files)\n",
    "    test_files.sort()\n",
    "    train_files.sort()\n",
    "    \n",
    "    def f_load(f):\n",
    "        path = os.path.join(all_runs, f)\n",
    "        return cv2.imread(path)[..., 0]\n",
    "\n",
    "    train_imgs = np.stack([f_load(f) for f in train_files]).\\\n",
    "                        astype('float32')[..., np.newaxis]\n",
    "    test_imgs  = np.stack([f_load(f) for f in test_files]).\\\n",
    "                        astype('float32')[..., np.newaxis]\n",
    "\n",
    "    return train_files, test_files, train_imgs, test_imgs, answers_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_one_shot_acc(test_pred, train_pred, train_files, answers_files):\n",
    "    n_test = len(test_pred)\n",
    "    n_train = len(train_pred)\n",
    "    \n",
    "    distM = np.zeros((n_test, n_train))\n",
    "    for i in range(n_test):\n",
    "        for c in range(n_train):\n",
    "            distM[i,c] = distance.euclidean(test_pred[i],train_pred[c])\n",
    "            \n",
    "    YHAT = np.argmin(distM,axis=1)\n",
    "    \n",
    "    # compute the error rate\n",
    "    correct = 0.0\n",
    "    for i in range(n_test):\n",
    "        if train_files[YHAT[i]] == answers_files[i]:\n",
    "            correct += 1.0\n",
    "        \n",
    "    return correct / n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model_1/ckpt\n"
     ]
    }
   ],
   "source": [
    "total_runs = 20\n",
    "train_pred_runs = [[] for _ in range(total_runs)]\n",
    "test_pred_runs = [[] for _ in range(total_runs)]\n",
    "\n",
    "run_data = [get_run(r+1) for r in range(total_runs)]\n",
    "\n",
    "num_models = 1\n",
    "graphs = [tf.Graph() for _ in range(num_models)]\n",
    "sessions = [tf.Session(graph=g) for g in graphs]\n",
    "\n",
    "for i in range(len(graphs)):\n",
    "    with graphs[i].as_default():\n",
    "        restore_sess(sessions[i], './models/model_' + str(i + 1))\n",
    "        \n",
    "        for r in range(total_runs):\n",
    "            train_files,test_files,train_imgs,test_imgs,answers_files = run_data[r]\n",
    "            \n",
    "            feed_dict = {'x:0':train_imgs, 'batch_size:0':len(train_imgs)}\n",
    "            sessions[i].run('test_init_op', feed_dict=feed_dict)\n",
    "            train_pred_runs[r].append(sessions[i].run('model_%d'%(i+1)+'/'+'pred:0'))\n",
    "    \n",
    "            feed_dict = {'x:0':test_imgs, 'batch_size:0':len(test_imgs)}\n",
    "            sessions[i].run('test_init_op', feed_dict=feed_dict)\n",
    "            test_pred_runs[r].append(sessions[i].run('model_%d'%(i+1)+'/'+'pred:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_runs = []\n",
    "for r in range(total_runs):\n",
    "    test_embed = np.mean(test_pred_runs[r], axis=0)\n",
    "    train_embed = np.mean(train_pred_runs[r], axis=0)\n",
    "    train_files = run_data[r][0]\n",
    "    answers_files = run_data[r][-1]\n",
    "    \n",
    "    acc = compute_one_shot_acc(test_embed, train_embed,train_files, answers_files)\n",
    "    acc_runs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3, 0.4, 0.35, 0.2, 0.45, 0.4, 0.35, 0.2, 0.25, 0.15, 0.5, 0.3, 0.1, 0.5, 0.45, 0.45, 0.25, 0.3, 0.2, 0.2]\n",
      "0.31500000000000006\n"
     ]
    }
   ],
   "source": [
    "print(acc_runs)\n",
    "print(np.mean(acc_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
