{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D, Dense\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, Concatenate\n",
    "from keras.models import Model\n",
    "# from keras.datasets import cifar10\n",
    "from keras import backend\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from vbranch.datasets.cifar10 import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "backend.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, blocks, name, growth_rate=32):\n",
    "    for i in range(blocks):\n",
    "        x = conv_block(x, growth_rate, name=name+'_block'+str(i+1))\n",
    "    return x\n",
    "\n",
    "def transition_block(x, reduction, name):\n",
    "    x = BatchNormalization(epsilon=1.001e-5, name=name + '_bn')(x)\n",
    "    x = Activation('relu', name=name + '_relu')(x)\n",
    "    x = Conv2D(int(backend.int_shape(x)[-1] * reduction), 1, use_bias=False,\n",
    "            name=name + '_conv')(x)\n",
    "    x = AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(x, growth_rate, name):\n",
    "    x1 = BatchNormalization(epsilon=1.001e-5, name=name + '_0_bn')(x)\n",
    "    x1 = Activation('relu', name=name + '_0_relu')(x1)\n",
    "    x1 = Conv2D(4 * growth_rate, 1, use_bias=False,\n",
    "            name=name + '_1_conv')(x1)\n",
    "    x1 = BatchNormalization(epsilon=1.001e-5, name=name + '_1_bn')(x1)\n",
    "    x1 = Activation('relu', name=name + '_1_relu')(x1)\n",
    "    x1 = Conv2D(growth_rate, 3, padding='same', use_bias=False,\n",
    "               name=name + '_2_conv')(x1)\n",
    "    x = Concatenate(name=name + '_concat')([x, x1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNet(depth, growth_rate, input_shape, classes=10):\n",
    "    # No BC -> divide by 3\n",
    "    # BC -> divide by 6\n",
    "    n_layers = (depth - 4) // 6\n",
    "\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1, 1))(img_input)\n",
    "    x = Conv2D(growth_rate*2, 3, use_bias=False, name='conv1/conv')(x)\n",
    "\n",
    "    x = dense_block(x, n_layers, 'conv2', growth_rate)\n",
    "    x = transition_block(x, 0.5, 'pool2')\n",
    "    x = dense_block(x, n_layers, 'conv3', growth_rate)\n",
    "    x = transition_block(x, 0.5, 'pool3')\n",
    "    x = dense_block(x, n_layers, 'conv4', growth_rate)\n",
    "\n",
    "    x = BatchNormalization(epsilon=1.001e-5, name='bn')(x)\n",
    "    x = Activation('relu', name='relu')(x)\n",
    "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x = Dense(classes, name='output', activation='softmax')(x)\n",
    "\n",
    "    return Model(img_input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data(preprocess=True, one_hot=True)\n",
    "# y_train = to_categorical(y_train, num_classes=10)\n",
    "# y_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(images):\n",
    "    demeaned = images.astype('float32') - np.mean(images, axis=(0,1,2))\n",
    "    normalized = demeaned / np.std(X_train, axis=(0,1,2))\n",
    "    return normalized\n",
    "\n",
    "def random_crop(images, labels, padding=2, multiple=3, imsize=32):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    pad_spec = ((0,0), (padding, padding), (padding, padding), (0,0))\n",
    "    padded_images = np.pad(images, pad_spec, 'constant', constant_values=0)\n",
    "\n",
    "    for im, lb in zip(padded_images, labels):\n",
    "        for _ in range(multiple):\n",
    "            crop_x = np.random.randint(2*padding)\n",
    "            crop_y = np.random.randint(2*padding)\n",
    "            cropped_im = im[crop_y:imsize+crop_y, crop_x:imsize+crop_x]\n",
    "            \n",
    "            if np.random.random() < 0.5:\n",
    "                cropped_im = np.flip(cropped_im, axis=1)\n",
    "                \n",
    "            augmented_images.append(cropped_im)\n",
    "            augmented_labels.append(lb)\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing\n",
    "# X_train_norm = normalize(X_train)\n",
    "# X_test_norm = normalize(X_test)\n",
    "# X_train_aug, y_train_aug = random_crop(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 0\n",
    "plt.imshow(X_train[n])\n",
    "plt.title(f'Class: {np.argmax(y_train[n])}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\19196\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DenseNet(depth=100, growth_rate=12, input_shape=(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 34, 34, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 32, 32, 24)   648         zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 32, 32, 24)   96          conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 32, 32, 24)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 32, 32, 48)   1152        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 32, 32, 48)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 32, 32, 12)   5184        conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 32, 32, 36)   0           conv1/conv[0][0]                 \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 32, 32, 36)   144         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 32, 32, 36)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 32, 32, 48)   1728        conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 32, 32, 48)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 32, 32, 12)   5184        conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 32, 32, 48)   0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 32, 32, 48)   0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 32, 32, 48)   2304        conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 32, 32, 48)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 12)   5184        conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 32, 32, 60)   0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 32, 32, 60)   240         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 32, 32, 60)   0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 32, 32, 48)   2880        conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 32, 32, 48)   0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 32, 32, 12)   5184        conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 32, 32, 72)   0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 32, 32, 72)   288         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 32, 32, 72)   0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 32, 32, 48)   3456        conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 32, 32, 48)   0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 32, 32, 12)   5184        conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 32, 32, 84)   0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 32, 32, 84)   336         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 32, 32, 84)   0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 32, 32, 48)   4032        conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 32, 32, 48)   0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 32, 32, 12)   5184        conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 32, 32, 96)   0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block7_0_bn (BatchNormali (None, 32, 32, 96)   384         conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block7_0_relu (Activation (None, 32, 32, 96)   0           conv2_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block7_1_conv (Conv2D)    (None, 32, 32, 48)   4608        conv2_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block7_1_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block7_1_relu (Activation (None, 32, 32, 48)   0           conv2_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block7_2_conv (Conv2D)    (None, 32, 32, 12)   5184        conv2_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block7_concat (Concatenat (None, 32, 32, 108)  0           conv2_block6_concat[0][0]        \n",
      "                                                                 conv2_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block8_0_bn (BatchNormali (None, 32, 32, 108)  432         conv2_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block8_0_relu (Activation (None, 32, 32, 108)  0           conv2_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block8_1_conv (Conv2D)    (None, 32, 32, 48)   5184        conv2_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block8_1_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block8_1_relu (Activation (None, 32, 32, 48)   0           conv2_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block8_2_conv (Conv2D)    (None, 32, 32, 12)   5184        conv2_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block8_concat (Concatenat (None, 32, 32, 120)  0           conv2_block7_concat[0][0]        \n",
      "                                                                 conv2_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block9_0_bn (BatchNormali (None, 32, 32, 120)  480         conv2_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block9_0_relu (Activation (None, 32, 32, 120)  0           conv2_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block9_1_conv (Conv2D)    (None, 32, 32, 48)   5760        conv2_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block9_1_bn (BatchNormali (None, 32, 32, 48)   192         conv2_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block9_1_relu (Activation (None, 32, 32, 48)   0           conv2_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block9_2_conv (Conv2D)    (None, 32, 32, 12)   5184        conv2_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block9_concat (Concatenat (None, 32, 32, 132)  0           conv2_block8_concat[0][0]        \n",
      "                                                                 conv2_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block10_0_bn (BatchNormal (None, 32, 32, 132)  528         conv2_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block10_0_relu (Activatio (None, 32, 32, 132)  0           conv2_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block10_1_conv (Conv2D)   (None, 32, 32, 48)   6336        conv2_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block10_1_bn (BatchNormal (None, 32, 32, 48)   192         conv2_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block10_1_relu (Activatio (None, 32, 32, 48)   0           conv2_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block10_2_conv (Conv2D)   (None, 32, 32, 12)   5184        conv2_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block10_concat (Concatena (None, 32, 32, 144)  0           conv2_block9_concat[0][0]        \n",
      "                                                                 conv2_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block11_0_bn (BatchNormal (None, 32, 32, 144)  576         conv2_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block11_0_relu (Activatio (None, 32, 32, 144)  0           conv2_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block11_1_conv (Conv2D)   (None, 32, 32, 48)   6912        conv2_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block11_1_bn (BatchNormal (None, 32, 32, 48)   192         conv2_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block11_1_relu (Activatio (None, 32, 32, 48)   0           conv2_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block11_2_conv (Conv2D)   (None, 32, 32, 12)   5184        conv2_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block11_concat (Concatena (None, 32, 32, 156)  0           conv2_block10_concat[0][0]       \n",
      "                                                                 conv2_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block12_0_bn (BatchNormal (None, 32, 32, 156)  624         conv2_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block12_0_relu (Activatio (None, 32, 32, 156)  0           conv2_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block12_1_conv (Conv2D)   (None, 32, 32, 48)   7488        conv2_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block12_1_bn (BatchNormal (None, 32, 32, 48)   192         conv2_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block12_1_relu (Activatio (None, 32, 32, 48)   0           conv2_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block12_2_conv (Conv2D)   (None, 32, 32, 12)   5184        conv2_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block12_concat (Concatena (None, 32, 32, 168)  0           conv2_block11_concat[0][0]       \n",
      "                                                                 conv2_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block13_0_bn (BatchNormal (None, 32, 32, 168)  672         conv2_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block13_0_relu (Activatio (None, 32, 32, 168)  0           conv2_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block13_1_conv (Conv2D)   (None, 32, 32, 48)   8064        conv2_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block13_1_bn (BatchNormal (None, 32, 32, 48)   192         conv2_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block13_1_relu (Activatio (None, 32, 32, 48)   0           conv2_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block13_2_conv (Conv2D)   (None, 32, 32, 12)   5184        conv2_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block13_concat (Concatena (None, 32, 32, 180)  0           conv2_block12_concat[0][0]       \n",
      "                                                                 conv2_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block14_0_bn (BatchNormal (None, 32, 32, 180)  720         conv2_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block14_0_relu (Activatio (None, 32, 32, 180)  0           conv2_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block14_1_conv (Conv2D)   (None, 32, 32, 48)   8640        conv2_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block14_1_bn (BatchNormal (None, 32, 32, 48)   192         conv2_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block14_1_relu (Activatio (None, 32, 32, 48)   0           conv2_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block14_2_conv (Conv2D)   (None, 32, 32, 12)   5184        conv2_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block14_concat (Concatena (None, 32, 32, 192)  0           conv2_block13_concat[0][0]       \n",
      "                                                                 conv2_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block15_0_bn (BatchNormal (None, 32, 32, 192)  768         conv2_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block15_0_relu (Activatio (None, 32, 32, 192)  0           conv2_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block15_1_conv (Conv2D)   (None, 32, 32, 48)   9216        conv2_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block15_1_bn (BatchNormal (None, 32, 32, 48)   192         conv2_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block15_1_relu (Activatio (None, 32, 32, 48)   0           conv2_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block15_2_conv (Conv2D)   (None, 32, 32, 12)   5184        conv2_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block15_concat (Concatena (None, 32, 32, 204)  0           conv2_block14_concat[0][0]       \n",
      "                                                                 conv2_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block16_0_bn (BatchNormal (None, 32, 32, 204)  816         conv2_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block16_0_relu (Activatio (None, 32, 32, 204)  0           conv2_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block16_1_conv (Conv2D)   (None, 32, 32, 48)   9792        conv2_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block16_1_bn (BatchNormal (None, 32, 32, 48)   192         conv2_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block16_1_relu (Activatio (None, 32, 32, 48)   0           conv2_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block16_2_conv (Conv2D)   (None, 32, 32, 12)   5184        conv2_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block16_concat (Concatena (None, 32, 32, 216)  0           conv2_block15_concat[0][0]       \n",
      "                                                                 conv2_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 32, 32, 216)  864         conv2_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 32, 32, 216)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 32, 32, 108)  23328       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 16, 16, 108)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 16, 16, 108)  432         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 16, 16, 108)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 16, 16, 48)   5184        conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 16, 16, 48)   192         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 16, 16, 48)   0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 16, 16, 12)   5184        conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 16, 16, 120)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 16, 16, 120)  480         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 16, 16, 120)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 16, 16, 48)   5760        conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 16, 16, 48)   192         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 16, 16, 48)   0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 16, 16, 12)   5184        conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 16, 16, 132)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 16, 16, 132)  528         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 16, 16, 132)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 16, 16, 48)   6336        conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 16, 16, 48)   192         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 16, 16, 48)   0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 16, 16, 12)   5184        conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 16, 16, 144)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 16, 16, 144)  576         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 16, 16, 144)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 16, 16, 48)   6912        conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 16, 16, 48)   192         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 16, 16, 48)   0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 12)   5184        conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 16, 16, 156)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 16, 16, 156)  624         conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 16, 16, 156)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 16, 16, 48)   7488        conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 16, 16, 48)   192         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 16, 16, 48)   0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 16, 16, 12)   5184        conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 16, 16, 168)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 16, 16, 168)  672         conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 16, 16, 168)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 16, 16, 48)   8064        conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 16, 16, 48)   192         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 16, 16, 48)   0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 16, 16, 12)   5184        conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 16, 16, 180)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 16, 16, 180)  720         conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 16, 16, 180)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 16, 16, 48)   8640        conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 16, 16, 48)   192         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 16, 16, 48)   0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 16, 16, 12)   5184        conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 16, 16, 192)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 16, 16, 192)  768         conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 16, 16, 192)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 16, 16, 48)   9216        conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 16, 16, 48)   192         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 16, 16, 48)   0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 16, 16, 12)   5184        conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 16, 16, 204)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 16, 16, 204)  816         conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 16, 16, 204)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 16, 16, 48)   9792        conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 16, 16, 48)   192         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 16, 16, 48)   0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 16, 16, 12)   5184        conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 16, 16, 216)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 16, 16, 216)  864         conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 16, 16, 216)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 16, 16, 48)   10368       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 16, 16, 48)   192         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 16, 16, 48)   0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 16, 16, 12)   5184        conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 16, 16, 228)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 16, 16, 228)  912         conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 16, 16, 228)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 16, 16, 48)   10944       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 16, 16, 48)   192         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 16, 16, 48)   0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 16, 16, 12)   5184        conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 16, 16, 240)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 16, 16, 240)  960         conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 16, 16, 240)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 16, 16, 48)   11520       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 16, 16, 48)   192         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 16, 16, 48)   0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 16, 16, 12)   5184        conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 16, 16, 252)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block13_0_bn (BatchNormal (None, 16, 16, 252)  1008        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block13_0_relu (Activatio (None, 16, 16, 252)  0           conv3_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block13_1_conv (Conv2D)   (None, 16, 16, 48)   12096       conv3_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block13_1_bn (BatchNormal (None, 16, 16, 48)   192         conv3_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block13_1_relu (Activatio (None, 16, 16, 48)   0           conv3_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block13_2_conv (Conv2D)   (None, 16, 16, 12)   5184        conv3_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block13_concat (Concatena (None, 16, 16, 264)  0           conv3_block12_concat[0][0]       \n",
      "                                                                 conv3_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block14_0_bn (BatchNormal (None, 16, 16, 264)  1056        conv3_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block14_0_relu (Activatio (None, 16, 16, 264)  0           conv3_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block14_1_conv (Conv2D)   (None, 16, 16, 48)   12672       conv3_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block14_1_bn (BatchNormal (None, 16, 16, 48)   192         conv3_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block14_1_relu (Activatio (None, 16, 16, 48)   0           conv3_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block14_2_conv (Conv2D)   (None, 16, 16, 12)   5184        conv3_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block14_concat (Concatena (None, 16, 16, 276)  0           conv3_block13_concat[0][0]       \n",
      "                                                                 conv3_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block15_0_bn (BatchNormal (None, 16, 16, 276)  1104        conv3_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block15_0_relu (Activatio (None, 16, 16, 276)  0           conv3_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block15_1_conv (Conv2D)   (None, 16, 16, 48)   13248       conv3_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block15_1_bn (BatchNormal (None, 16, 16, 48)   192         conv3_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block15_1_relu (Activatio (None, 16, 16, 48)   0           conv3_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block15_2_conv (Conv2D)   (None, 16, 16, 12)   5184        conv3_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block15_concat (Concatena (None, 16, 16, 288)  0           conv3_block14_concat[0][0]       \n",
      "                                                                 conv3_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block16_0_bn (BatchNormal (None, 16, 16, 288)  1152        conv3_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block16_0_relu (Activatio (None, 16, 16, 288)  0           conv3_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block16_1_conv (Conv2D)   (None, 16, 16, 48)   13824       conv3_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block16_1_bn (BatchNormal (None, 16, 16, 48)   192         conv3_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block16_1_relu (Activatio (None, 16, 16, 48)   0           conv3_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block16_2_conv (Conv2D)   (None, 16, 16, 12)   5184        conv3_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block16_concat (Concatena (None, 16, 16, 300)  0           conv3_block15_concat[0][0]       \n",
      "                                                                 conv3_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 16, 16, 300)  1200        conv3_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 16, 16, 300)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 16, 16, 150)  45000       pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 8, 8, 150)    0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 8, 8, 150)    600         pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 8, 8, 150)    0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 8, 8, 48)     7200        conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 8, 8, 48)     192         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 8, 8, 48)     0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 8, 8, 12)     5184        conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 8, 8, 162)    0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 8, 8, 162)    648         conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 8, 8, 162)    0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 8, 8, 48)     7776        conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 8, 8, 48)     192         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 8, 8, 48)     0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 8, 8, 12)     5184        conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 8, 8, 174)    0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 8, 8, 174)    696         conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 8, 8, 174)    0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 8, 8, 48)     8352        conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 8, 8, 48)     192         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 8, 8, 48)     0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 8, 8, 12)     5184        conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 8, 8, 186)    0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 8, 8, 186)    744         conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 8, 8, 186)    0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 8, 8, 48)     8928        conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 8, 8, 48)     192         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 8, 8, 48)     0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 8, 8, 12)     5184        conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 8, 8, 198)    0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 8, 8, 198)    792         conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 8, 8, 198)    0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 8, 8, 48)     9504        conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 8, 8, 48)     192         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 8, 8, 48)     0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 8, 8, 12)     5184        conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 8, 8, 210)    0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 8, 8, 210)    840         conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 8, 8, 210)    0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 8, 8, 48)     10080       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 8, 8, 48)     192         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 8, 8, 48)     0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 12)     5184        conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 8, 8, 222)    0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 8, 8, 222)    888         conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 8, 8, 222)    0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 8, 8, 48)     10656       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 8, 8, 48)     192         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 8, 8, 48)     0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 8, 8, 12)     5184        conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 8, 8, 234)    0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 8, 8, 234)    936         conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 8, 8, 234)    0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 8, 8, 48)     11232       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 8, 8, 48)     192         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 8, 8, 48)     0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 8, 8, 12)     5184        conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 8, 8, 246)    0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 8, 8, 246)    984         conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 8, 8, 246)    0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 8, 8, 48)     11808       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 8, 8, 48)     192         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 8, 8, 48)     0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 8, 8, 12)     5184        conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 8, 8, 258)    0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 8, 8, 258)    1032        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 8, 8, 258)    0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 8, 8, 48)     12384       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 8, 8, 48)     192         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 8, 8, 48)     0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 8, 8, 12)     5184        conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 8, 8, 270)    0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 8, 8, 270)    1080        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 8, 8, 270)    0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 8, 8, 48)     12960       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 8, 8, 48)     192         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 8, 8, 48)     0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 8, 8, 12)     5184        conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 8, 8, 282)    0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 8, 8, 282)    1128        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 8, 8, 282)    0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 8, 8, 48)     13536       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 8, 8, 48)     192         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 8, 8, 48)     0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 8, 8, 12)     5184        conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 8, 8, 294)    0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 8, 8, 294)    1176        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 8, 8, 294)    0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 8, 8, 48)     14112       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 8, 8, 48)     192         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 8, 8, 48)     0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 8, 8, 12)     5184        conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 8, 8, 306)    0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 8, 8, 306)    1224        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 8, 8, 306)    0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 8, 8, 48)     14688       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 8, 8, 48)     192         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 8, 8, 48)     0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 8, 8, 12)     5184        conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 8, 8, 318)    0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 8, 8, 318)    1272        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 8, 8, 318)    0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 8, 8, 48)     15264       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 8, 8, 48)     192         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 8, 8, 48)     0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 8, 8, 12)     5184        conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 8, 8, 330)    0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 8, 8, 330)    1320        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 8, 8, 330)    0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 8, 8, 48)     15840       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 8, 8, 48)     192         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 8, 8, 48)     0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 8, 8, 12)     5184        conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 8, 8, 342)    0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 8, 8, 342)    1368        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 8, 8, 342)    0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 342)          0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 10)           3430        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 793,150\n",
      "Trainable params: 769,162\n",
      "Non-trainable params: 23,988\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_learning_rate(epoch):\n",
    "    if epoch < 100:\n",
    "        return 0.001\n",
    "    elif epoch < 150:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\19196\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 373s 7ms/step - loss: 1.1966 - accuracy: 0.5668 - val_loss: 1.2046 - val_accuracy: 0.5769\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 351s 7ms/step - loss: 0.7837 - accuracy: 0.7247 - val_loss: 2.2098 - val_accuracy: 0.4624\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 355s 7ms/step - loss: 0.5978 - accuracy: 0.7919 - val_loss: 0.8675 - val_accuracy: 0.7014\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 329s 7ms/step - loss: 0.4862 - accuracy: 0.8318 - val_loss: 0.7847 - val_accuracy: 0.7287\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 354s 7ms/step - loss: 0.4137 - accuracy: 0.8580 - val_loss: 0.6403 - val_accuracy: 0.7926\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 353s 7ms/step - loss: 0.3465 - accuracy: 0.8801 - val_loss: 0.5756 - val_accuracy: 0.8122\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 354s 7ms/step - loss: 0.3001 - accuracy: 0.8946 - val_loss: 0.5950 - val_accuracy: 0.8130\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 355s 7ms/step - loss: 0.2538 - accuracy: 0.9105 - val_loss: 0.5743 - val_accuracy: 0.8242\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 353s 7ms/step - loss: 0.2157 - accuracy: 0.9247 - val_loss: 0.6596 - val_accuracy: 0.7995\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 354s 7ms/step - loss: 0.1870 - accuracy: 0.9334 - val_loss: 0.5724 - val_accuracy: 0.8189\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 296s 6ms/step - loss: 0.1640 - accuracy: 0.9420 - val_loss: 0.5633 - val_accuracy: 0.8330\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 288s 6ms/step - loss: 0.1418 - accuracy: 0.9501 - val_loss: 0.5795 - val_accuracy: 0.8275\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 288s 6ms/step - loss: 0.1203 - accuracy: 0.9578 - val_loss: 0.6679 - val_accuracy: 0.8165\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 290s 6ms/step - loss: 0.1105 - accuracy: 0.9611 - val_loss: 0.5693 - val_accuracy: 0.8401\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 290s 6ms/step - loss: 0.0979 - accuracy: 0.9653 - val_loss: 0.7516 - val_accuracy: 0.8077\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 291s 6ms/step - loss: 0.0916 - accuracy: 0.9671 - val_loss: 0.7188 - val_accuracy: 0.8323\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 286s 6ms/step - loss: 0.0801 - accuracy: 0.9720 - val_loss: 0.6555 - val_accuracy: 0.8382\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 292s 6ms/step - loss: 0.0792 - accuracy: 0.9719 - val_loss: 0.6127 - val_accuracy: 0.8534\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 295s 6ms/step - loss: 0.0737 - accuracy: 0.9740 - val_loss: 0.6185 - val_accuracy: 0.8525\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 288s 6ms/step - loss: 0.0618 - accuracy: 0.9786 - val_loss: 0.7281 - val_accuracy: 0.8336\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 292s 6ms/step - loss: 0.0629 - accuracy: 0.9779 - val_loss: 0.6515 - val_accuracy: 0.8473\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 289s 6ms/step - loss: 0.0593 - accuracy: 0.9782 - val_loss: 0.7048 - val_accuracy: 0.8418\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 285s 6ms/step - loss: 0.0592 - accuracy: 0.9789 - val_loss: 0.8194 - val_accuracy: 0.8280\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 294s 6ms/step - loss: 0.0500 - accuracy: 0.9822 - val_loss: 0.6175 - val_accuracy: 0.8494\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 292s 6ms/step - loss: 0.0556 - accuracy: 0.9814 - val_loss: 0.6841 - val_accuracy: 0.8475\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 291s 6ms/step - loss: 0.0456 - accuracy: 0.9840 - val_loss: 0.7226 - val_accuracy: 0.8459\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 289s 6ms/step - loss: 0.0499 - accuracy: 0.9824 - val_loss: 0.6372 - val_accuracy: 0.8484\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 289s 6ms/step - loss: 0.0421 - accuracy: 0.9849 - val_loss: 1.0240 - val_accuracy: 0.8165\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 284s 6ms/step - loss: 0.0397 - accuracy: 0.9862 - val_loss: 0.7868 - val_accuracy: 0.8417\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 288s 6ms/step - loss: 0.0445 - accuracy: 0.9849 - val_loss: 0.6957 - val_accuracy: 0.8524\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 290s 6ms/step - loss: 0.0367 - accuracy: 0.9871 - val_loss: 0.9200 - val_accuracy: 0.8346\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 285s 6ms/step - loss: 0.0424 - accuracy: 0.9853 - val_loss: 0.7089 - val_accuracy: 0.8610\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 288s 6ms/step - loss: 0.0363 - accuracy: 0.9875 - val_loss: 0.6603 - val_accuracy: 0.8544\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 284s 6ms/step - loss: 0.0339 - accuracy: 0.9885 - val_loss: 0.8352 - val_accuracy: 0.8329\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 288s 6ms/step - loss: 0.0347 - accuracy: 0.9881 - val_loss: 0.7771 - val_accuracy: 0.8349\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 287s 6ms/step - loss: 0.0351 - accuracy: 0.9879 - val_loss: 0.9730 - val_accuracy: 0.8215\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 283s 6ms/step - loss: 0.0310 - accuracy: 0.9900 - val_loss: 0.7313 - val_accuracy: 0.8606\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 287s 6ms/step - loss: 0.0344 - accuracy: 0.9881 - val_loss: 0.7675 - val_accuracy: 0.8477\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 287s 6ms/step - loss: 0.0333 - accuracy: 0.9886 - val_loss: 0.6450 - val_accuracy: 0.8652\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 283s 6ms/step - loss: 0.0281 - accuracy: 0.9900 - val_loss: 0.6236 - val_accuracy: 0.8676\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 287s 6ms/step - loss: 0.0292 - accuracy: 0.9897 - val_loss: 0.8467 - val_accuracy: 0.8490\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 286s 6ms/step - loss: 0.0307 - accuracy: 0.9896 - val_loss: 0.7248 - val_accuracy: 0.8566\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 284s 6ms/step - loss: 0.0277 - accuracy: 0.9905 - val_loss: 0.9369 - val_accuracy: 0.8371\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 286s 6ms/step - loss: 0.0276 - accuracy: 0.9904 - val_loss: 0.8378 - val_accuracy: 0.8442\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 288s 6ms/step - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.7193 - val_accuracy: 0.8574\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 283s 6ms/step - loss: 0.0348 - accuracy: 0.9886 - val_loss: 0.7672 - val_accuracy: 0.8527\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 288s 6ms/step - loss: 0.0203 - accuracy: 0.9930 - val_loss: 0.7496 - val_accuracy: 0.8494\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 282s 6ms/step - loss: 0.0286 - accuracy: 0.9902 - val_loss: 0.7698 - val_accuracy: 0.8550\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 286s 6ms/step - loss: 0.0252 - accuracy: 0.9909 - val_loss: 0.7914 - val_accuracy: 0.8634\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 287s 6ms/step - loss: 0.0223 - accuracy: 0.9923 - val_loss: 0.6271 - val_accuracy: 0.8773\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 282s 6ms/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 0.8346 - val_accuracy: 0.8449\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 286s 6ms/step - loss: 0.0282 - accuracy: 0.9900 - val_loss: 0.7019 - val_accuracy: 0.8673\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 2597s 52ms/step - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.9620 - val_accuracy: 0.8326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 273s 5ms/step - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.8876 - val_accuracy: 0.8524\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 273s 5ms/step - loss: 0.0221 - accuracy: 0.9920 - val_loss: 0.7972 - val_accuracy: 0.8512\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0186 - accuracy: 0.9938 - val_loss: 0.8244 - val_accuracy: 0.8538\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 266s 5ms/step - loss: 0.0190 - accuracy: 0.9937 - val_loss: 0.8208 - val_accuracy: 0.8557\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0244 - accuracy: 0.9915 - val_loss: 0.6525 - val_accuracy: 0.8731\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 267s 5ms/step - loss: 0.0201 - accuracy: 0.9928 - val_loss: 0.6480 - val_accuracy: 0.8773\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 0.8381 - val_accuracy: 0.8567\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 270s 5ms/step - loss: 0.0211 - accuracy: 0.9930 - val_loss: 0.7878 - val_accuracy: 0.8661\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 267s 5ms/step - loss: 0.0164 - accuracy: 0.9943 - val_loss: 0.6924 - val_accuracy: 0.8698\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.7645 - val_accuracy: 0.8664\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0188 - accuracy: 0.9934 - val_loss: 0.6679 - val_accuracy: 0.8671\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 267s 5ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.8824 - val_accuracy: 0.8565\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 270s 5ms/step - loss: 0.0190 - accuracy: 0.9933 - val_loss: 0.7525 - val_accuracy: 0.8629\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 267s 5ms/step - loss: 0.0205 - accuracy: 0.9928 - val_loss: 0.7221 - val_accuracy: 0.8627\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 269s 5ms/step - loss: 0.0127 - accuracy: 0.9956 - val_loss: 0.8978 - val_accuracy: 0.8520\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 270s 5ms/step - loss: 0.0238 - accuracy: 0.9916 - val_loss: 0.7163 - val_accuracy: 0.8608\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 266s 5ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.6725 - val_accuracy: 0.8738\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 270s 5ms/step - loss: 0.0204 - accuracy: 0.9935 - val_loss: 0.8399 - val_accuracy: 0.8495\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 270s 5ms/step - loss: 0.0165 - accuracy: 0.9942 - val_loss: 0.7583 - val_accuracy: 0.8670\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 266s 5ms/step - loss: 0.0172 - accuracy: 0.9942 - val_loss: 0.7047 - val_accuracy: 0.8763\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 269s 5ms/step - loss: 0.0169 - accuracy: 0.9945 - val_loss: 0.7700 - val_accuracy: 0.8653\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 266s 5ms/step - loss: 0.0171 - accuracy: 0.9943 - val_loss: 0.9354 - val_accuracy: 0.8522\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 270s 5ms/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.8020 - val_accuracy: 0.8620\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 270s 5ms/step - loss: 0.0162 - accuracy: 0.9944 - val_loss: 0.9875 - val_accuracy: 0.8316\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 265s 5ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.7807 - val_accuracy: 0.8686\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.7600 - val_accuracy: 0.8640\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 0.6916 - val_accuracy: 0.8793\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 269s 5ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.8614 - val_accuracy: 0.8597\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.8016 - val_accuracy: 0.8627\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 273s 5ms/step - loss: 0.0149 - accuracy: 0.9950 - val_loss: 0.7374 - val_accuracy: 0.8691\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 0.0146 - accuracy: 0.9952 - val_loss: 0.7049 - val_accuracy: 0.8723\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0144 - accuracy: 0.9950 - val_loss: 0.9948 - val_accuracy: 0.8521\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 0.0127 - accuracy: 0.9953 - val_loss: 0.6491 - val_accuracy: 0.8819\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0167 - accuracy: 0.9944 - val_loss: 0.6980 - val_accuracy: 0.8715\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.8115 - val_accuracy: 0.8617\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 0.0118 - accuracy: 0.9959 - val_loss: 0.6839 - val_accuracy: 0.8807\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.9960 - val_accuracy: 0.8446\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 0.7601 - val_accuracy: 0.8716\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 0.0140 - accuracy: 0.9951 - val_loss: 0.6249 - val_accuracy: 0.8837\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.6634 - val_accuracy: 0.8813\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 0.0085 - accuracy: 0.9971 - val_loss: 0.9326 - val_accuracy: 0.8554\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 274s 5ms/step - loss: 0.0168 - accuracy: 0.9940 - val_loss: 0.6940 - val_accuracy: 0.8804\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0105 - accuracy: 0.9963 - val_loss: 0.8452 - val_accuracy: 0.8615\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 0.0147 - accuracy: 0.9944 - val_loss: 0.7698 - val_accuracy: 0.8712\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.9220 - val_accuracy: 0.8565\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.6983 - val_accuracy: 0.8762\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.7719 - val_accuracy: 0.8667\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.5969 - val_accuracy: 0.8937\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.5879 - val_accuracy: 0.8953\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.5909 - val_accuracy: 0.8950\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 5.9958e-04 - accuracy: 0.9999 - val_loss: 0.5822 - val_accuracy: 0.8964\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 4.5414e-04 - accuracy: 0.9999 - val_loss: 0.5819 - val_accuracy: 0.8974\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 3.7395e-04 - accuracy: 1.0000 - val_loss: 0.5868 - val_accuracy: 0.8961\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 3.2846e-04 - accuracy: 1.0000 - val_loss: 0.5844 - val_accuracy: 0.8968\n",
      "Epoch 108/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 268s 5ms/step - loss: 2.4083e-04 - accuracy: 1.0000 - val_loss: 0.5856 - val_accuracy: 0.8975\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 1.7459e-04 - accuracy: 1.0000 - val_loss: 0.5877 - val_accuracy: 0.8977\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 2.2649e-04 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 0.8963\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 1.8188e-04 - accuracy: 1.0000 - val_loss: 0.5998 - val_accuracy: 0.8986\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 1.8008e-04 - accuracy: 1.0000 - val_loss: 0.5959 - val_accuracy: 0.8980\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 2.1374e-04 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 0.8957\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 1.8003e-04 - accuracy: 1.0000 - val_loss: 0.6085 - val_accuracy: 0.8996\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 1.5660e-04 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 0.9002\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 9.3886e-05 - accuracy: 1.0000 - val_loss: 0.5998 - val_accuracy: 0.9001\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 1.1774e-04 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 0.8990\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 1.5753e-04 - accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 0.8987\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 8.7241e-05 - accuracy: 1.0000 - val_loss: 0.6207 - val_accuracy: 0.8986\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 6.4171e-05 - accuracy: 1.0000 - val_loss: 0.6165 - val_accuracy: 0.9006\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 6.8434e-05 - accuracy: 1.0000 - val_loss: 0.6151 - val_accuracy: 0.9014\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 7.7102e-05 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 0.8989\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 8.4944e-05 - accuracy: 1.0000 - val_loss: 0.6358 - val_accuracy: 0.8979\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 1.0622e-04 - accuracy: 1.0000 - val_loss: 0.6427 - val_accuracy: 0.8967\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 8.2969e-05 - accuracy: 1.0000 - val_loss: 0.6455 - val_accuracy: 0.8969\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 1.0035e-04 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.9002\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 5.4985e-05 - accuracy: 1.0000 - val_loss: 0.6417 - val_accuracy: 0.8989\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 4.2304e-05 - accuracy: 1.0000 - val_loss: 0.6412 - val_accuracy: 0.8999\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 4.3403e-05 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 0.8997\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 3.6871e-05 - accuracy: 1.0000 - val_loss: 0.6417 - val_accuracy: 0.8998\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 2.5298e-05 - accuracy: 1.0000 - val_loss: 0.6449 - val_accuracy: 0.9008\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 273s 5ms/step - loss: 5.6113e-05 - accuracy: 1.0000 - val_loss: 0.6569 - val_accuracy: 0.8992\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 3.2546e-05 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 0.8999\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 4.0493e-05 - accuracy: 1.0000 - val_loss: 0.6587 - val_accuracy: 0.8974\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 4.8124e-05 - accuracy: 1.0000 - val_loss: 0.6629 - val_accuracy: 0.8983\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 4.0322e-05 - accuracy: 1.0000 - val_loss: 0.6628 - val_accuracy: 0.8970\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 5.9106e-05 - accuracy: 1.0000 - val_loss: 0.6766 - val_accuracy: 0.8955\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 6.2523e-05 - accuracy: 1.0000 - val_loss: 0.6796 - val_accuracy: 0.8954\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 4.3557e-05 - accuracy: 1.0000 - val_loss: 0.6696 - val_accuracy: 0.8964\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 7.7061e-05 - accuracy: 1.0000 - val_loss: 0.6962 - val_accuracy: 0.8944\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 7.2891e-05 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.8957\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 5.0938e-05 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 0.8964\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 267s 5ms/step - loss: 5.9452e-05 - accuracy: 1.0000 - val_loss: 0.6910 - val_accuracy: 0.8949\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 1.0756e-04 - accuracy: 1.0000 - val_loss: 0.6944 - val_accuracy: 0.8943\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 5.3075e-05 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.8952\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 267s 5ms/step - loss: 4.9545e-05 - accuracy: 1.0000 - val_loss: 0.6795 - val_accuracy: 0.8966\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 2.9275e-05 - accuracy: 1.0000 - val_loss: 0.6793 - val_accuracy: 0.8963\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 2.5230e-05 - accuracy: 1.0000 - val_loss: 0.6826 - val_accuracy: 0.8968\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 6.0529e-05 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.8956\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 272s 5ms/step - loss: 8.7165e-05 - accuracy: 1.0000 - val_loss: 0.7004 - val_accuracy: 0.8960\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 273s 5ms/step - loss: 5.5692e-05 - accuracy: 1.0000 - val_loss: 0.6960 - val_accuracy: 0.8964\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 7.9785e-05 - accuracy: 1.0000 - val_loss: 0.6956 - val_accuracy: 0.8966\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 126s 3ms/step - loss: 5.3179e-05 - accuracy: 1.0000 - val_loss: 0.6940 - val_accuracy: 0.8963\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 4.4213e-05 - accuracy: 1.0000 - val_loss: 0.6958 - val_accuracy: 0.8971\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 2.6483e-05 - accuracy: 1.0000 - val_loss: 0.6966 - val_accuracy: 0.8965\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 2.8707e-05 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.8971\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 6.0697e-05 - accuracy: 1.0000 - val_loss: 0.6915 - val_accuracy: 0.8966\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 2.5152e-05 - accuracy: 1.0000 - val_loss: 0.6926 - val_accuracy: 0.8970\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 2.6729e-05 - accuracy: 1.0000 - val_loss: 0.6916 - val_accuracy: 0.8966\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 3.8036e-05 - accuracy: 1.0000 - val_loss: 0.6896 - val_accuracy: 0.8967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.8081e-05 - accuracy: 1.0000 - val_loss: 0.6926 - val_accuracy: 0.8973\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.8377e-05 - accuracy: 1.0000 - val_loss: 0.6929 - val_accuracy: 0.8976\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 3.3423e-05 - accuracy: 1.0000 - val_loss: 0.6899 - val_accuracy: 0.8973\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.8677e-05 - accuracy: 1.0000 - val_loss: 0.6907 - val_accuracy: 0.8976\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 2.8704e-05 - accuracy: 1.0000 - val_loss: 0.6887 - val_accuracy: 0.8978\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 2.2249e-05 - accuracy: 1.0000 - val_loss: 0.6894 - val_accuracy: 0.8980\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.9785e-05 - accuracy: 1.0000 - val_loss: 0.6915 - val_accuracy: 0.8982\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 1.3744e-05 - accuracy: 1.0000 - val_loss: 0.6914 - val_accuracy: 0.8977\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 130s 3ms/step - loss: 2.1437e-05 - accuracy: 1.0000 - val_loss: 0.6894 - val_accuracy: 0.8982\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 130s 3ms/step - loss: 1.6241e-05 - accuracy: 1.0000 - val_loss: 0.6909 - val_accuracy: 0.8973\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 129s 3ms/step - loss: 3.7213e-05 - accuracy: 1.0000 - val_loss: 0.6887 - val_accuracy: 0.8979\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 129s 3ms/step - loss: 1.5544e-05 - accuracy: 1.0000 - val_loss: 0.6888 - val_accuracy: 0.8981\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 129s 3ms/step - loss: 1.3061e-05 - accuracy: 1.0000 - val_loss: 0.6896 - val_accuracy: 0.8979\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 1.5002e-05 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.8976\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 2.7897e-05 - accuracy: 1.0000 - val_loss: 0.6893 - val_accuracy: 0.8979\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 1.3069e-05 - accuracy: 1.0000 - val_loss: 0.6923 - val_accuracy: 0.8980\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 1.5806e-05 - accuracy: 1.0000 - val_loss: 0.6890 - val_accuracy: 0.8980\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 18473s 369ms/step - loss: 1.9102e-05 - accuracy: 1.0000 - val_loss: 0.6902 - val_accuracy: 0.8983\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.3733e-05 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.8979\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.2960e-05 - accuracy: 1.0000 - val_loss: 0.6915 - val_accuracy: 0.8983\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.2304e-05 - accuracy: 1.0000 - val_loss: 0.6896 - val_accuracy: 0.8982\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.6686e-05 - accuracy: 1.0000 - val_loss: 0.6895 - val_accuracy: 0.8976\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 1.8398e-05 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 0.8985\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 1.6157e-05 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.8982\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 129s 3ms/step - loss: 2.8708e-05 - accuracy: 1.0000 - val_loss: 0.6918 - val_accuracy: 0.8978\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 2.6456e-05 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.8978\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 1.1074e-05 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.8975\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 129s 3ms/step - loss: 2.1421e-05 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 0.8989\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.1026e-05 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.8982\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.1024e-05 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.8976\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 1.0643e-05 - accuracy: 1.0000 - val_loss: 0.6887 - val_accuracy: 0.8989\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 1.0210e-05 - accuracy: 1.0000 - val_loss: 0.6887 - val_accuracy: 0.8995\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 9.8271e-06 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 0.8995\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 128s 3ms/step - loss: 9.0624e-06 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 0.8996\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.0276e-05 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 0.8991\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.2820e-05 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.8995\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 8.9416e-06 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.9001\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.5463e-05 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 0.8999\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.0717e-05 - accuracy: 1.0000 - val_loss: 0.6893 - val_accuracy: 0.8998\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 1.2331e-05 - accuracy: 1.0000 - val_loss: 0.6864 - val_accuracy: 0.8990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x18ac1523a08>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          epochs=200, batch_size=64, shuffle=True,\n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[LearningRateScheduler(step_learning_rate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('densenet-cifar10+-adam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
